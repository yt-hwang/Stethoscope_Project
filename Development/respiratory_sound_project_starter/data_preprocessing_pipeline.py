#!/usr/bin/env python3
"""
í˜¸í¡ìŒ ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
- ì˜¤ë””ì˜¤ ë¡œë”©, ì •ê·œí™”, ë…¸ì´ì¦ˆ ì œê±°
- íŠ¹ì§• ì¶”ì¶œ ë° ë¼ë²¨ë§
- ë°ì´í„°ì…‹ ìƒì„±
"""

import numpy as np
import pandas as pd
import librosa
import soundfile as sf
from pathlib import Path
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

# í”„ë¡œì íŠ¸ ì„¤ì •
from src.config import SR, N_FFT, HOP_LEN, N_MELS, FMIN, FMAX
from src.audio_io import load_audio, pre_emphasis
from src.features import stft_mag_db, logmel, mfcc, wheeze_indicators

class RespiratoryDataProcessor:
    """
    í˜¸í¡ìŒ ë°ì´í„° ì „ì²˜ë¦¬ í´ë˜ìŠ¤
    """
    
    def __init__(self, sample_rate=SR, frame_length=25, hop_length=10):
        self.sample_rate = sample_rate
        self.frame_length = frame_length  # ms
        self.hop_length = hop_length      # ms
        self.scaler = StandardScaler()
        self.label_encoder = LabelEncoder()
        
    def load_and_preprocess_audio(self, audio_path, apply_preemphasis=True):
        """
        ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë”© ë° ê¸°ë³¸ ì „ì²˜ë¦¬
        """
        print(f"Loading: {audio_path}")
        
        # 1. ì˜¤ë””ì˜¤ ë¡œë”©
        y = load_audio(audio_path, sr=self.sample_rate)
        
        # 2. Pre-emphasis (ê³ ì£¼íŒŒìˆ˜ ê°•ì¡°)
        if apply_preemphasis:
            y = pre_emphasis(y)
        
        # 3. ì •ê·œí™” (RMS ê¸°ë°˜)
        rms = np.sqrt(np.mean(y**2))
        if rms > 0:
            y = y / rms * 0.1  # RMSë¥¼ 0.1ë¡œ ì •ê·œí™”
        
        # 4. ì§§ì€ ë¬´ìŒ êµ¬ê°„ ì œê±° (ì—ë„ˆì§€ ê¸°ë°˜)
        y = self._remove_silence(y)
        
        return y
    
    def _remove_silence(self, y, frame_length=1024, hop_length=512, threshold=0.01):
        """
        ë¬´ìŒ êµ¬ê°„ ì œê±°
        """
        # ì—ë„ˆì§€ ê³„ì‚°
        energy = []
        for i in range(0, len(y) - frame_length, hop_length):
            frame = y[i:i + frame_length]
            energy.append(np.mean(frame**2))
        
        energy = np.array(energy)
        
        # ì„ê³„ê°’ ì´í•˜ êµ¬ê°„ ì°¾ê¸°
        silent_frames = energy < threshold
        
        if not np.any(silent_frames):
            return y
        
        # ì—°ì†ëœ ë¬´ìŒ êµ¬ê°„ì˜ ì‹œì‘/ë ì°¾ê¸°
        silent_regions = []
        start = None
        for i, is_silent in enumerate(silent_frames):
            if is_silent and start is None:
                start = i
            elif not is_silent and start is not None:
                silent_regions.append((start * hop_length, i * hop_length))
                start = None
        
        # ë§ˆì§€ë§‰ ë¬´ìŒ êµ¬ê°„ ì²˜ë¦¬
        if start is not None:
            silent_regions.append((start * hop_length, len(y)))
        
        # ë¬´ìŒ êµ¬ê°„ ì œê±°
        if silent_regions:
            # ë¬´ìŒì´ ì•„ë‹Œ êµ¬ê°„ë“¤ë§Œ ì¶”ì¶œ
            non_silent_parts = []
            last_end = 0
            
            for start, end in silent_regions:
                if start > last_end:
                    non_silent_parts.append(y[last_end:start])
                last_end = end
            
            if last_end < len(y):
                non_silent_parts.append(y[last_end:])
            
            if non_silent_parts:
                y = np.concatenate(non_silent_parts)
        
        return y
    
    def extract_features(self, y, feature_type='all'):
        """
        ë‹¤ì–‘í•œ íŠ¹ì§• ì¶”ì¶œ
        """
        features = {}
        
        if feature_type in ['all', 'spectral']:
            # STFT ê¸°ë°˜ íŠ¹ì§•
            S_db = stft_mag_db(y)
            features['stft_db'] = S_db
            
            # Mel-spectrogram
            M_db = logmel(y)
            features['mel_db'] = M_db
            
            # MFCC
            mfcc_features = mfcc(y, n_mfcc=20)
            features['mfcc'] = mfcc_features
            
            # Wheezing ì§€í‘œ
            indicators = wheeze_indicators(S_db)
            features.update(indicators)
        
        if feature_type in ['all', 'temporal']:
            # ì‹œê°„ ë„ë©”ì¸ íŠ¹ì§•
            features['rms'] = librosa.feature.rms(y=y, hop_length=HOP_LEN)[0]
            features['zero_crossing_rate'] = librosa.feature.zero_crossing_rate(y, hop_length=HOP_LEN)[0]
            
            # ìŠ¤í™íŠ¸ëŸ¼ íŠ¹ì§•
            spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=self.sample_rate, hop_length=HOP_LEN)[0]
            spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=self.sample_rate, hop_length=HOP_LEN)[0]
            spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=self.sample_rate, hop_length=HOP_LEN)[0]
            
            features['spectral_centroid'] = spectral_centroids
            features['spectral_rolloff'] = spectral_rolloff
            features['spectral_bandwidth'] = spectral_bandwidth
        
        return features
    
    def create_segments(self, y, segment_length_sec=2.0, overlap_ratio=0.5):
        """
        ì˜¤ë””ì˜¤ë¥¼ ê³ ì • ê¸¸ì´ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ë¶„í• 
        """
        segment_length = int(segment_length_sec * self.sample_rate)
        hop_length = int(segment_length * (1 - overlap_ratio))
        
        segments = []
        for start in range(0, len(y) - segment_length, hop_length):
            segment = y[start:start + segment_length]
            segments.append(segment)
        
        return segments
    
    def create_dataset_from_metadata(self, metadata_path, audio_base_path):
        """
        ë©”íƒ€ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„°ì…‹ ìƒì„±
        """
        print("Creating dataset from metadata...")
        
        # ë©”íƒ€ë°ì´í„° ë¡œë”©
        metadata = pd.read_csv(metadata_path)
        
        dataset = []
        labels = []
        
        for idx, row in metadata.iterrows():
            audio_file = row['Audio File']
            diagnosis = row['Diagnosis']
            
            # NaN ê°’ ì²´í¬
            if pd.isna(audio_file):
                continue
                
            # ë¼ë²¨ ê²°ì • (ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜)
            if pd.isna(diagnosis) or diagnosis == 'No pull diagnosis':
                label = 'normal'
            elif 'Asthma' in str(diagnosis):
                label = 'asthma'
            else:
                label = 'unknown'
            
            # ì‹¤ì œ íŒŒì¼ êµ¬ì¡°ì— ë§ê²Œ ê²½ë¡œ ìˆ˜ì •
            # ì˜ˆ: WEBSS-002-01.wav -> WEBSS002_3/WEBSS-002 TP 2_60sec.wav
            patient_id = str(audio_file).split('-')[0] + str(audio_file).split('-')[1]
            
            # í™˜ìë³„ í´ë” ì°¾ê¸°
            patient_folders = {
                'WEBSS002': 'WEBSS002_3',
                'WEBSS003': 'WEBSS003_6', 
                'WEBSS004': 'WEBSS004_5',
                'WEBSS005': 'WEBSS005_6',
                'WEBSS006': 'WEBSS006_12',
                'WEBSS007': 'WEBSS007_7'
            }
            
            if patient_id in patient_folders:
                folder_name = patient_folders[patient_id]
                # ì‹¤ì œ íŒŒì¼ëª… íŒ¨í„´ì— ë§ê²Œ ë³€í™˜
                file_num = str(audio_file).split('-')[2].split('.')[0]
                actual_filename = f"WEBSS-{patient_id[4:]} TP{file_num}_60sec.wav"
                audio_path = Path(audio_base_path) / folder_name / actual_filename
            else:
                continue
            
            if audio_path.exists():
                try:
                    # ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬
                    y = self.load_and_preprocess_audio(str(audio_path))
                    
                    # ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±
                    segments = self.create_segments(y, segment_length_sec=2.0)
                    
                    for segment in segments:
                        # íŠ¹ì§• ì¶”ì¶œ
                        features = self.extract_features(segment)
                        
                        # íŠ¹ì§• ë²¡í„°í™”
                        feature_vector = self._vectorize_features(features)
                        
                        dataset.append(feature_vector)
                        labels.append(label)
                        
                except Exception as e:
                    print(f"Error processing {audio_file}: {e}")
                    continue
        
        return np.array(dataset), np.array(labels)
    
    def _vectorize_features(self, features):
        """
        íŠ¹ì§•ë“¤ì„ í•˜ë‚˜ì˜ ë²¡í„°ë¡œ ë³€í™˜
        """
        vector = []
        
        # MFCC (20ê°œ ê³„ìˆ˜)
        if 'mfcc' in features:
            mfcc_mean = np.mean(features['mfcc'], axis=1)
            mfcc_std = np.std(features['mfcc'], axis=1)
            vector.extend(mfcc_mean)
            vector.extend(mfcc_std)
        
        # Wheezing ì§€í‘œë“¤
        for key in ['flatness', 'centroid', 'e_ratio_100_1k__1k_2_5k']:
            if key in features:
                vector.append(np.mean(features[key]))
                vector.append(np.std(features[key]))
        
        # ì‹œê°„ ë„ë©”ì¸ íŠ¹ì§•ë“¤
        for key in ['rms', 'zero_crossing_rate', 'spectral_centroid', 'spectral_rolloff', 'spectral_bandwidth']:
            if key in features:
                vector.append(np.mean(features[key]))
                vector.append(np.std(features[key]))
        
        return np.array(vector)
    
    def prepare_training_data(self, X, y, test_size=0.2, random_state=42):
        """
        í›ˆë ¨ ë°ì´í„° ì¤€ë¹„
        """
        # ë¼ë²¨ ì¸ì½”ë”©
        y_encoded = self.label_encoder.fit_transform(y)
        
        # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_encoded, test_size=test_size, random_state=random_state, stratify=y_encoded
        )
        
        # íŠ¹ì§• ì •ê·œí™”
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        return X_train_scaled, X_test_scaled, y_train, y_test

def analyze_dataset_statistics(X, y, feature_names=None):
    """
    ë°ì´í„°ì…‹ í†µê³„ ë¶„ì„
    """
    print("\nğŸ“Š Dataset Statistics")
    print("=" * 50)
    
    # í´ë˜ìŠ¤ ë¶„í¬
    unique, counts = np.unique(y, return_counts=True)
    print(f"Classes: {unique}")
    print(f"Counts: {counts}")
    print(f"Total samples: {len(y)}")
    
    # íŠ¹ì§• í†µê³„
    print(f"\nFeature matrix shape: {X.shape}")
    print(f"Mean values: {np.mean(X, axis=0)[:5]}...")  # ì²˜ìŒ 5ê°œë§Œ
    print(f"Std values: {np.std(X, axis=0)[:5]}...")
    
    # í´ë˜ìŠ¤ë³„ íŠ¹ì§• ë¶„í¬
    for class_label in unique:
        class_mask = y == class_label
        class_features = X[class_mask]
        print(f"\nClass {class_label}:")
        print(f"  Samples: {np.sum(class_mask)}")
        print(f"  Mean feature values: {np.mean(class_features, axis=0)[:3]}...")

def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    """
    print("ğŸš€ í˜¸í¡ìŒ ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    
    # ë°ì´í„° í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
    processor = RespiratoryDataProcessor()
    
    # ë©”íƒ€ë°ì´í„° ê²½ë¡œ
    metadata_path = "../../Audio shared/Sheet 1-Tabular_asthma_data.csv"
    audio_base_path = "../../Audio shared/Hospital sound"
    
    # ë°ì´í„°ì…‹ ìƒì„±
    X, y = processor.create_dataset_from_metadata(metadata_path, audio_base_path)
    
    if len(X) > 0:
        # ë°ì´í„°ì…‹ í†µê³„
        analyze_dataset_statistics(X, y)
        
        # í›ˆë ¨ ë°ì´í„° ì¤€ë¹„
        X_train, X_test, y_train, y_test = processor.prepare_training_data(X, y)
        
        print(f"\nâœ… ì „ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"Training set: {X_train.shape}")
        print(f"Test set: {X_test.shape}")
        print(f"Classes: {processor.label_encoder.classes_}")
        
        # ê²°ê³¼ ì €ì¥
        np.savez('preprocessed_dataset.npz',
                X_train=X_train, X_test=X_test,
                y_train=y_train, y_test=y_test,
                feature_names=None)  # TODO: ì‹¤ì œ íŠ¹ì§• ì´ë¦„ë“¤ ì¶”ê°€
        
        print("ğŸ’¾ ì „ì²˜ë¦¬ëœ ë°ì´í„°ì…‹ ì €ì¥: preprocessed_dataset.npz")
        
    else:
        print("âŒ ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨")

if __name__ == "__main__":
    main()
