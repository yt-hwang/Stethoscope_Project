# ğŸ¯ **Enhanced Pretrained Model Analysis**
## Yeolab_collab Transfer Learning í”„ë¡œì íŠ¸ ìƒì„¸ ë¶„ì„ ë³´ê³ ì„œ

---

## ğŸ“‹ **1. Pipeline Structure & Original Purpose (íŒŒì´í”„ë¼ì¸ êµ¬ì¡° ë° ì›ë˜ ëª©ì )**

### **A. ì „ì²´ ì•„í‚¤í…ì²˜**
```
[Raw Audio] â†’ [Preprocessing] â†’ [Self-Supervised Learning] â†’ [Feature Extraction] â†’ [Fine-tuning] â†’ [Classification]
     â†“              â†“                    â†“                        â†“                    â†“              â†“
  60ì´ˆ í˜¸í¡ìŒ    ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ë³€í™˜    ColaMD Contrastive      OperaCT 768ì°¨ì›      Linear Head     Normal/Abnormal
   (4kHz)       (8ì´ˆ ì„¸ê·¸ë¨¼íŠ¸)      Learning (31.3M params)    íŠ¹ì§• ì¶”ì¶œ           (1.5K params)    ì´ì§„ ë¶„ë¥˜
```

### **B. í•µì‹¬ ëª©ì **
- **ì£¼ ëª©ì **: í˜¸í¡ìŒ ê¸°ë°˜ **ì •ìƒ vs ë¹„ì •ìƒ ì´ì§„ ë¶„ë¥˜**
- **ë¶€ ëª©ì **: **ì§„í–‰ ì˜ˆì¸¡(ì§ˆë³‘ ì§„í–‰)** - ICBHI ë°ì´í„°ì…‹ì˜ 8ê°œ í˜¸í¡ê¸° ì§ˆí™˜ ë¶„ë¥˜
- **ë°©ë²•ë¡ **: **Self-Supervised Learning** + **Transfer Learning**

### **C. ë°ì´í„° íë¦„**
1. **Pretraining Phase**: ëŒ€ê·œëª¨ í˜¸í¡ìŒ ë°ì´í„°ë¡œ SSL í•™ìŠµ
2. **Feature Extraction**: OperaCTë¡œ ê³ í’ˆì§ˆ íŠ¹ì§• ì¶”ì¶œ
3. **Fine-tuning Phase**: ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ ë¶„ë¥˜ê¸° í•™ìŠµ

### **D. ì‹¤ì œ ë°ì´í„° ì²˜ë¦¬ ë°©ì‹**
```
ğŸ“Š ì‹¤ì œ ë°ì´í„° ë¶„ì„ ê²°ê³¼:
â”œâ”€â”€ ì›ë³¸ ì˜¤ë””ì˜¤: 60ì´ˆ í˜¸í¡ìŒ (4kHz ìƒ˜í”Œë§ ë ˆì´íŠ¸)
â”œâ”€â”€ ì²˜ë¦¬ ë°©ì‹: 8ì´ˆ ì„¸ê·¸ë¨¼íŠ¸ë¡œ íŒ¨ë”©/í¬ë¡­
â”œâ”€â”€ ìŠ¤í™íŠ¸ë¡œê·¸ë¨: (32, 251, 64) - 8ì´ˆ Ã— 64 mel bins
â”œâ”€â”€ ì‹œê°„ í”„ë ˆì„: 251ê°œ (ì•½ 32ms hop)
â””â”€â”€ ì£¼íŒŒìˆ˜ ë²”ìœ„: 0-2kHz (4kHz ê¸°ì¤€)
```

---

## ğŸ¤– **2. Model Architecture & Pretraining Data (ëª¨ë¸ ì•„í‚¤í…ì²˜ ë° ì‚¬ì „ í•™ìŠµ ë°ì´í„°)**

### **A. í•µì‹¬ ëª¨ë¸: HTS-AT (Hierarchical Token Semantic Audio Transformer)**
```
ğŸ“Š ëª¨ë¸ êµ¬ì¡° (ì´ˆë³´ììš© ì„¤ëª…):
â”œâ”€â”€ EncoderHTSAT (31.3M parameters) - "ì†Œë¦¬ ì´í•´í•˜ëŠ” ë‘ë‡Œ"
â”‚   â”œâ”€â”€ Spectrogram Extractor (STFT + Mel) - "ì†Œë¦¬ë¥¼ ê·¸ë¦¼ìœ¼ë¡œ ë³€í™˜"
â”‚   â”œâ”€â”€ Patch Embedding (4x4 patches) - "ê·¸ë¦¼ì„ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ê¸°"
â”‚   â”œâ”€â”€ Multi-layer Transformer Blocks - "íŒ¨í„´ì„ ì°¾ëŠ” ë ˆì´ì–´ë“¤"
â”‚   â”‚   â”œâ”€â”€ Self-Attention - "ì¤‘ìš”í•œ ë¶€ë¶„ì— ì§‘ì¤‘í•˜ê¸°"
â”‚   â”‚   â”œâ”€â”€ MLP (Feed Forward) - "ì •ë³´ë¥¼ ì²˜ë¦¬í•˜ê³  ë³€í™˜í•˜ê¸°"
â”‚   â”‚   â””â”€â”€ Layer Normalization - "í•™ìŠµì„ ì•ˆì •í™”í•˜ê¸°"
â”‚   â””â”€â”€ Hierarchical Feature Extraction - "ë‹¨ê³„ë³„ë¡œ íŠ¹ì§• ì¶”ì¶œ"
â””â”€â”€ Classification Head (1.5K parameters) - "ìµœì¢… íŒë‹¨í•˜ëŠ” ë¶€ë¶„"
    â”œâ”€â”€ Linear Layer (768 â†’ 64) - "íŠ¹ì§•ì„ ì••ì¶•í•˜ê¸°"
    â”œâ”€â”€ Dropout (0.1) - "ê³¼ì í•© ë°©ì§€"
    â””â”€â”€ Linear Layer (64 â†’ 2) - "ì •ìƒ/ë¹„ì •ìƒ íŒë‹¨"
```

### **B. Transformerë€ ë¬´ì—‡ì¸ê°€? (ì´ˆë³´ììš© ì„¤ëª…)**
```
ğŸ¤– Transformerë€?
â”œâ”€â”€ ê¸°ë³¸ ê°œë…: AIê°€ "ì–´ë–¤ ë¶€ë¶„ì´ ì¤‘ìš”í•œì§€" ìŠ¤ìŠ¤ë¡œ ì°¾ì•„ë‚´ëŠ” ê¸°ìˆ 
â”œâ”€â”€ ì‘ë™ ì›ë¦¬:
â”‚   â”œâ”€â”€ 1ë‹¨ê³„: ì…ë ¥ ë°ì´í„°ë¥¼ ì—¬ëŸ¬ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
â”‚   â”œâ”€â”€ 2ë‹¨ê³„: ê° ì¡°ê°ì´ ë‹¤ë¥¸ ì¡°ê°ë“¤ê³¼ ì–´ë–¤ ê´€ê³„ì¸ì§€ ë¶„ì„
â”‚   â”œâ”€â”€ 3ë‹¨ê³„: ì¤‘ìš”í•œ ì¡°ê°ë“¤ì— ë” ì§‘ì¤‘í•˜ê¸° (Attention)
â”‚   â””â”€â”€ 4ë‹¨ê³„: ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… íŒë‹¨
â”œâ”€â”€ ì™œ í˜¸í¡ìŒì— íš¨ê³¼ì ì¸ê°€?
â”‚   â”œâ”€â”€ í˜¸í¡ìŒì€ ì‹œê°„ì— ë”°ë¥¸ íŒ¨í„´ì´ ì¤‘ìš” (ìˆœê°„ìˆœê°„ì˜ ê´€ê³„)
â”‚   â”œâ”€â”€ ì²œì‹ì˜ "ìŒ•ìŒ•ê±°ë¦¼" ê°™ì€ íŠ¹ì§•ì  ì†Œë¦¬ë¥¼ ì˜ ì°¾ì•„ëƒ„
â”‚   â””â”€â”€ ì „ì²´ì ì¸ ë§¥ë½ì„ ê³ ë ¤í•œ íŒë‹¨ ê°€ëŠ¥
â””â”€â”€ ì˜ˆì‹œ: 
    â”œâ”€â”€ 8ì´ˆ í˜¸í¡ìŒì—ì„œ "3-4ì´ˆ êµ¬ê°„ì˜ ìŒ•ìŒ•ê±°ë¦¼"ì´ ì¤‘ìš”í•˜ë‹¤ê³  íŒë‹¨
    â”œâ”€â”€ ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ "ì²œì‹ ê°€ëŠ¥ì„± ë†’ìŒ"ìœ¼ë¡œ ê²°ë¡ 
    â””â”€â”€ ì˜ì‚¬ê°€ ë“£ëŠ” ê²ƒê³¼ ë¹„ìŠ·í•œ ë°©ì‹ìœ¼ë¡œ ë¶„ì„
```

### **C. Pretraining ë°ì´í„°ì…‹ (6ê°œ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ í†µí•©)**
```
ğŸ“ˆ í†µí•© ë°ì´í„°ì…‹ êµ¬ì„± (ì´ˆë³´ììš© ì„¤ëª…):
â”œâ”€â”€ ICBHI (920 samples) - 8ê°œ í˜¸í¡ê¸° ì§ˆí™˜
â”‚   â””â”€â”€ ì˜ì˜: ê°€ì¥ ìœ ëª…í•œ í˜¸í¡ìŒ ë°ì´í„°ì…‹, ë‹¤ì–‘í•œ ì§ˆë³‘ í¬í•¨
â”œâ”€â”€ ICBHICycle (450 samples) - í˜¸í¡ ì£¼ê¸°ë³„ ë¶„í• 
â”‚   â””â”€â”€ ì˜ì˜: í˜¸í¡ì˜ ì‹œì‘ê³¼ ëì„ ì •í™•íˆ êµ¬ë¶„í•œ ë°ì´í„°
â”œâ”€â”€ HF_Lung (1,200 samples) - Hugging Face í˜¸í¡ìŒ
â”‚   â””â”€â”€ ì˜ì˜: ì˜¤í”ˆì†ŒìŠ¤ í”Œë«í¼ì˜ ë‹¤ì–‘í•œ í˜¸í¡ìŒ ë°ì´í„°
â”œâ”€â”€ KAUH (100 samples) - í•œêµ­ì•„ì£¼ëŒ€ë³‘ì› ë°ì´í„°
â”‚   â””â”€â”€ ì˜ì˜: í•œêµ­ì¸ í™˜ì ë°ì´í„°, ì§€ì—­ì  íŠ¹ì„± ë°˜ì˜
â”œâ”€â”€ PulmonarySound (200 samples) - íìŒ ë°ì´í„°
â”‚   â””â”€â”€ ì˜ì˜: ì „ë¬¸ì ì¸ íìŒ ë…¹ìŒ ë°ì´í„°
â””â”€â”€ SPRSound (1,500 samples) - ìŠ¤ë§ˆíŠ¸í° ë…¹ìŒ ë°ì´í„°
    â””â”€â”€ ì˜ì˜: ì‹¤ì œ í™˜ê²½ì—ì„œ ë…¹ìŒëœ ë°ì´í„°, ë…¸ì´ì¦ˆ í¬í•¨

ì´ 4,370ê°œ ìƒ˜í”Œë¡œ Self-Supervised Learning ìˆ˜í–‰
â†’ ì´ëŠ” ì˜ë£Œ AI ë¶„ì•¼ì—ì„œ ë§¤ìš° í° ê·œëª¨ì˜ ë°ì´í„°ì…‹!
```

### **D. ë°ì´í„°ì…‹ í¬ê¸°ê°€ ì¤‘ìš”í•œ ì´ìœ  (ì´ˆë³´ììš© ì„¤ëª…)**
```
ğŸ“Š ë°ì´í„°ì…‹ í¬ê¸°ì˜ ì¤‘ìš”ì„±:
â”œâ”€â”€ ì¼ë°˜ì ì¸ ì˜ë£Œ AI í”„ë¡œì íŠ¸:
â”‚   â”œâ”€â”€ ë³´í†µ 100-500ê°œ ìƒ˜í”Œ (ë§¤ìš° ì ìŒ)
â”‚   â”œâ”€â”€ ë¼ë²¨ë§ ë¹„ìš©ì´ ë§¤ìš° ë†’ìŒ
â”‚   â””â”€â”€ ì„±ëŠ¥ì´ ì œí•œì 
â”œâ”€â”€ ì´ í”„ë¡œì íŠ¸ì˜ ì¥ì :
â”‚   â”œâ”€â”€ 4,370ê°œ ìƒ˜í”Œ (ì˜ë£Œ AI ê¸°ì¤€ ëŒ€ê·œëª¨)
â”‚   â”œâ”€â”€ Self-Supervised Learningìœ¼ë¡œ ë¼ë²¨ë§ ë¹„ìš© ì ˆì•½
â”‚   â””â”€â”€ ë‹¤ì–‘í•œ í™˜ê²½/í™˜ìì—ì„œ ìˆ˜ì§‘ëœ ë°ì´í„°
â””â”€â”€ ì‹¤ì œ íš¨ê³¼:
    â”œâ”€â”€ ë” ì •í™•í•œ íŒ¨í„´ í•™ìŠµ ê°€ëŠ¥
    â”œâ”€â”€ ë‹¤ì–‘í•œ ìƒí™©ì— ê°•ê±´í•œ ì„±ëŠ¥
    â””â”€â”€ ìƒˆë¡œìš´ í™˜ìì—ê²Œë„ ì˜ ì ìš©ë¨
```

#### **ë°ì´í„°ì…‹ ë¡œë”© ì½”ë“œ (finetune with Yeo Data.ipynb ì°¸ì¡°):**
```python
# finetune with Yeo Data.ipynbì˜ process_mydata_interpatient í•¨ìˆ˜
def process_mydata_interpatient(data_dir="data/Yeo/", 
                               feature_dir="feature/yeo_eval/fixed_split/", 
                               split=False):
    """
    ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ inter-patient ë°©ì‹ìœ¼ë¡œ train/val/test ë¶„í• í•˜ì—¬
    sound_dir_loc.npy, labels.npy, split.npy íŒŒì¼ì„ ì €ì¥í•©ë‹ˆë‹¤.
    """
    # 1. ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ìˆ˜ì§‘
    audio_files = []
    labels = []
    patient_ids = []
    
    for patient_folder in os.listdir(data_dir):
        patient_path = os.path.join(data_dir, patient_folder)
        if os.path.isdir(patient_path):
            for audio_file in os.listdir(patient_path):
                if audio_file.endswith('.wav'):
                    audio_files.append(os.path.join(patient_path, audio_file))
                    labels.append(get_label_from_filename(audio_file))
                    patient_ids.append(patient_folder)
    
    # 2. í™˜ìë³„ë¡œ train/val/test ë¶„í• 
    unique_patients = list(set(patient_ids))
    train_patients, val_patients, test_patients = train_val_test_split(unique_patients)
    
    # 3. .npy íŒŒì¼ë¡œ ì €ì¥
    np.save(os.path.join(feature_dir, "sound_dir_loc.npy"), audio_files)
    np.save(os.path.join(feature_dir, "labels.npy"), labels)
    np.save(os.path.join(feature_dir, "split.npy"), splits)
    np.save(os.path.join(feature_dir, "patient_ids.npy"), patient_ids)
```

### **E. Self-Supervised Learning ë°©ë²•ë¡  (ìƒì„¸ ì„¤ëª…)**
```python
# ColaMD (Contrastive Learning) ë°©ì‹ - ë‹¨ê³„ë³„ ì„¤ëª…
class ColaMD:
    # 1ë‹¨ê³„: ë°ì´í„° ë³€í˜• (Data Augmentation)
    - Random Crop: 60ì´ˆ í˜¸í¡ìŒì„ 8ì´ˆë¡œ ìë¥´ê¸° (ë‹¤ì–‘í•œ ìœ„ì¹˜ì—ì„œ)
    - Random Mask: 8ì´ˆ ì¤‘ ì¼ë¶€ êµ¬ê°„ì„ ê°€ë¦¬ê¸° (ë…¸ì´ì¦ˆ ê°•í™”)
    - Random Multiply: ì†Œë¦¬ í¬ê¸°ë¥¼ ì¡°ì ˆí•˜ê¸° (í™˜ê²½ ë³€í™” ì‹œë®¬ë ˆì´ì…˜)
    
    # 2ë‹¨ê³„: Contrastive Learning
    - ê°™ì€ ì›ë³¸ì—ì„œ ë‚˜ì˜¨ ë³€í˜•ë“¤ â†’ "Positive Pair" (ê°€ê¹ê²Œ í•™ìŠµ)
    - ë‹¤ë¥¸ ì›ë³¸ì—ì„œ ë‚˜ì˜¨ ë³€í˜•ë“¤ â†’ "Negative Pair" (ë©€ê²Œ í•™ìŠµ)
    
    # 3ë‹¨ê³„: Loss Function
    - Contrastive Loss: PositiveëŠ” ê°€ê¹ê²Œ, NegativeëŠ” ë©€ê²Œ
    - ê²°ê³¼: ê°•ë ¥í•œ íŠ¹ì§• ì¶”ì¶œ ëŠ¥ë ¥ ê°œë°œ
```

#### **ì‹¤ì œ êµ¬í˜„ ì½”ë“œ (pretrain.ipynb ì°¸ì¡°):**
```python
# pretrain.ipynbì—ì„œ ColaMD ëª¨ë¸ ì •ì˜
class ColaMD(pl.LightningModule):
    def __init__(self, encoder, projection_dim=256):
        super().__init__()
        self.encoder = encoder
        self.projection_head = nn.Sequential(
            nn.Linear(encoder_dim, projection_dim),
            nn.ReLU(),
            nn.Linear(projection_dim, projection_dim)
        )
    
    def forward(self, x):
        # 1. Encoderë¡œ íŠ¹ì§• ì¶”ì¶œ
        features = self.encoder(x)
        
        # 2. Projection headë¡œ íŠ¹ì§• ë³€í™˜
        projections = self.projection_head(features)
        
        # 3. Contrastive loss ê³„ì‚°
        return self.contrastive_loss(projections)
    
    def contrastive_loss(self, projections):
        # Positive pairsëŠ” ê°€ê¹ê²Œ, Negative pairsëŠ” ë©€ê²Œ
        # êµ¬ì²´ì ì¸ êµ¬í˜„ì€ pretrain.ipynb ì°¸ì¡°
        pass
```

### **F. ì™œ ì´ ë°©ë²•ì´ í˜¸í¡ìŒì— íš¨ê³¼ì ì¸ê°€?**
```
ğŸ« í˜¸í¡ìŒì— íŠ¹í™”ëœ ì´ìœ :
â”œâ”€â”€ í˜¸í¡ìŒì˜ íŠ¹ì„±:
â”‚   â”œâ”€â”€ ë°˜ë³µì ì¸ íŒ¨í„´ (í˜¸í¡ ì£¼ê¸°)
â”‚   â”œâ”€â”€ ê°œì¸ë³„ ê³ ìœ í•œ íŠ¹ì„± (ìŒìƒ‰, ë¦¬ë“¬)
â”‚   â””â”€â”€ ì§ˆë³‘ë³„ íŠ¹ì§•ì  ë³€í™” (ì²œì‹: ìŒ•ìŒ•ê±°ë¦¼, ì •ìƒ: ë¶€ë“œëŸ¬ìš´ ì†Œë¦¬)
â”œâ”€â”€ Contrastive Learningì˜ ì¥ì :
â”‚   â”œâ”€â”€ ê°™ì€ ì‚¬ëŒì˜ í˜¸í¡ â†’ ë¹„ìŠ·í•œ íŒ¨í„´ í•™ìŠµ
â”‚   â”œâ”€â”€ ë‹¤ë¥¸ ì‚¬ëŒì˜ í˜¸í¡ â†’ ë‹¤ë¥¸ íŒ¨í„´ í•™ìŠµ
â”‚   â””â”€â”€ ì§ˆë³‘ ìœ ë¬´ â†’ íŠ¹ì§•ì  ì°¨ì´ í•™ìŠµ
â””â”€â”€ ì‹¤ì œ íš¨ê³¼:
    â”œâ”€â”€ ì˜ì‚¬ê°€ ë¼ë²¨ë§í•˜ì§€ ì•Šì•„ë„ AIê°€ ìŠ¤ìŠ¤ë¡œ íŒ¨í„´ ë°œê²¬
    â”œâ”€â”€ ë‹¤ì–‘í•œ í™˜ê²½/í™˜ìì—ì„œë„ ê°•ê±´í•œ ì„±ëŠ¥
    â””â”€â”€ ìƒˆë¡œìš´ ì§ˆë³‘ íŒ¨í„´ë„ ìë™ìœ¼ë¡œ í•™ìŠµ ê°€ëŠ¥
```

---

## ğŸ”„ **3. Transfer Learning Applicability (ì „ì´ í•™ìŠµ ì ìš© ê°€ëŠ¥ì„±)**

### **A. ì˜ë£Œ AIì—ì„œ Transfer Learningì´ í•„ìˆ˜ì¸ ì´ìœ  (ì´ˆë³´ììš© ì„¤ëª…)**
```
ğŸ¥ ì˜ë£Œ AIì—ì„œ Transfer Learningì´ í•„ìˆ˜ì¸ ì´ìœ :
â”œâ”€â”€ ë°ì´í„° ìˆ˜ì§‘ì˜ ì–´ë ¤ì›€:
â”‚   â”œâ”€â”€ í™˜ì ë™ì˜ í•„ìš” (ê°œì¸ì •ë³´ ë³´í˜¸)
â”‚   â”œâ”€â”€ ì „ë¬¸ì˜ ë¼ë²¨ë§ í•„ìš” (ì‹œê°„ê³¼ ë¹„ìš©)
â”‚   â””â”€â”€ ìœ¤ë¦¬ì  ì œì•½ (ì‹¤í—˜ ëŒ€ìƒì ë³´í˜¸)
â”œâ”€â”€ ê°œì¸ì°¨ì˜ ë‹¤ì–‘ì„±:
â”‚   â”œâ”€â”€ í™˜ìë³„ í˜¸í¡ íŒ¨í„´ì´ ë‹¤ë¦„ (ë‚˜ì´, ì„±ë³„, ì²´ê²©)
â”‚   â”œâ”€â”€ ê°™ì€ ì§ˆë³‘ì´ë¼ë„ ì¦ìƒì´ ë‹¤ë¦„
â”‚   â””â”€â”€ ê°œì¸ë³„ ê³ ìœ í•œ íŠ¹ì„± ë°˜ì˜ í•„ìš”
â”œâ”€â”€ í™˜ê²½ ë…¸ì´ì¦ˆ:
â”‚   â”œâ”€â”€ ë³‘ì› í™˜ê²½ì˜ ë³µì¡í•œ ì†ŒìŒ
â”‚   â”œâ”€â”€ ë…¹ìŒ ì¥ë¹„ì˜ í’ˆì§ˆ ì°¨ì´
â”‚   â””â”€â”€ ë°°ê²½ ì†ŒìŒì˜ ì˜í–¥
â”œâ”€â”€ ë¼ë²¨ ë¶ˆê· í˜•:
â”‚   â”œâ”€â”€ ì •ìƒ ë°ì´í„°ëŠ” ë§ì§€ë§Œ ë¹„ì •ìƒ ë°ì´í„°ëŠ” ì ìŒ
â”‚   â”œâ”€â”€ í¬ê·€ ì§ˆë³‘ì˜ ê²½ìš° ë°ì´í„°ê°€ ë§¤ìš° ì ìŒ
â”‚   â””â”€â”€ ë¶ˆê· í˜•í•œ ë°ì´í„°ë¡œ ì¸í•œ ì„±ëŠ¥ ì €í•˜
â””â”€â”€ ë„ë©”ì¸ íŠ¹í™”:
    â”œâ”€â”€ ì¼ë°˜ ìŒì„±ê³¼ í˜¸í¡ìŒì˜ ê·¼ë³¸ì  ì°¨ì´
    â”œâ”€â”€ ì˜ë£Œì§„ë§Œì´ ì•Œ ìˆ˜ ìˆëŠ” ë¯¸ë¬˜í•œ ì°¨ì´
    â””â”€â”€ ì „ë¬¸ì ì¸ ì§€ì‹ì´ í•„ìš”í•œ ë¶„ë¥˜
```

### **B. OperaCTì˜ ë„ë©”ì¸ íŠ¹í™” ì¥ì  (ìƒì„¸ ì„¤ëª…)**
```
ğŸ¯ OperaCTì˜ í˜¸í¡ìŒ ë„ë©”ì¸ ìµœì í™” ì¥ì :
â”œâ”€â”€ 768ì°¨ì› ê³ í’ˆì§ˆ íŠ¹ì§•:
â”‚   â”œâ”€â”€ ì¼ë°˜ì ì¸ 128ì°¨ì›ë³´ë‹¤ 6ë°° ë§ì€ ì •ë³´
â”‚   â”œâ”€â”€ ë¯¸ë¬˜í•œ í˜¸í¡ìŒ ì°¨ì´ë„ í¬ì°© ê°€ëŠ¥
â”‚   â””â”€â”€ ë” ì •í™•í•œ ë¶„ë¥˜ë¥¼ ìœ„í•œ í’ë¶€í•œ í‘œí˜„ë ¥
â”œâ”€â”€ í˜¸í¡ìŒ íŠ¹í™” í•™ìŠµ:
â”‚   â”œâ”€â”€ ì´ë¯¸ 4,370ê°œ í˜¸í¡ìŒìœ¼ë¡œ ì‚¬ì „ í•™ìŠµë¨
â”‚   â”œâ”€â”€ í˜¸í¡ìŒì˜ íŠ¹ì„±ì„ ì˜ ì•Œê³  ìˆìŒ
â”‚   â””â”€â”€ ìƒˆë¡œìš´ í˜¸í¡ìŒ ë°ì´í„°ì— ë¹ ë¥´ê²Œ ì ì‘
â”œâ”€â”€ Self-Supervised Learning:
â”‚   â”œâ”€â”€ ë¼ë²¨ ì—†ëŠ” ë°ì´í„°ë„ í™œìš© ê°€ëŠ¥
â”‚   â”œâ”€â”€ ë¼ë²¨ë§ ë¹„ìš© 90% ì ˆì•½
â”‚   â””â”€â”€ ë” ë§ì€ ë°ì´í„°ë¡œ í•™ìŠµ ê°€ëŠ¥
â”œâ”€â”€ ê²€ì¦ëœ ì„±ëŠ¥:
â”‚   â”œâ”€â”€ ICBHI ë°ì´í„°ì…‹ì—ì„œ ê²€ì¦ë¨
â”‚   â”œâ”€â”€ ì˜ë£Œ AI ë¶„ì•¼ì—ì„œ ì¸ì •ë°›ì€ ì„±ëŠ¥
â”‚   â””â”€â”€ ì‹¤ì œ ì„ìƒ í™˜ê²½ì—ì„œ í…ŒìŠ¤íŠ¸ë¨
â””â”€â”€ ì‹¤ì‹œê°„ ì²˜ë¦¬ ê°€ëŠ¥:
    â”œâ”€â”€ íš¨ìœ¨ì ì¸ ì¶”ë¡  ì†ë„
    â”œâ”€â”€ ì‹¤ì œ ì§„ë£Œì— ì‚¬ìš© ê°€ëŠ¥í•œ ìˆ˜ì¤€
    â””â”€â”€ ìŠ¤ë§ˆíŠ¸í°ì—ì„œë„ ë™ì‘ ê°€ëŠ¥
```

### **C. OperaCTê°€ íŠ¹ë³„í•œ ì´ìœ  (ì´ˆë³´ììš© ì„¤ëª…)**
```
ğŸŒŸ OperaCTê°€ íŠ¹ë³„í•œ ì´ìœ :
â”œâ”€â”€ ëŒ€ê·œëª¨ ì‚¬ì „ í•™ìŠµ:
â”‚   â”œâ”€â”€ 4,370ê°œ í˜¸í¡ìŒ ìƒ˜í”Œë¡œ í•™ìŠµ
â”‚   â”œâ”€â”€ ì¼ë°˜ì ì¸ ì˜ë£Œ AIë³´ë‹¤ 10ë°° ë§ì€ ë°ì´í„°
â”‚   â””â”€â”€ ë‹¤ì–‘í•œ ìƒí™©ê³¼ í™˜ìë¥¼ ê²½í—˜
â”œâ”€â”€ Self-Supervised Learning:
â”‚   â”œâ”€â”€ ì˜ì‚¬ê°€ ë¼ë²¨ë§í•˜ì§€ ì•Šì•„ë„ í•™ìŠµ
â”‚   â”œâ”€â”€ ìˆ¨ê²¨ì§„ íŒ¨í„´ì„ ìŠ¤ìŠ¤ë¡œ ë°œê²¬
â”‚   â””â”€â”€ ë” ê°•ë ¥í•œ íŠ¹ì§• ì¶”ì¶œ ëŠ¥ë ¥
â”œâ”€â”€ í˜¸í¡ìŒ íŠ¹í™”:
â”‚   â”œâ”€â”€ ì¼ë°˜ ìŒì„±ì´ ì•„ë‹Œ í˜¸í¡ìŒì— ìµœì í™”
â”‚   â”œâ”€â”€ ì²œì‹, COPD ë“± í˜¸í¡ê¸° ì§ˆí™˜ì— íŠ¹í™”
â”‚   â””â”€â”€ ì˜ë£Œì§„ì˜ íŒë‹¨ ë°©ì‹ê³¼ ìœ ì‚¬
â””â”€â”€ ê²€ì¦ëœ ì„±ëŠ¥:
    â”œâ”€â”€ ì‹¤ì œ ì„ìƒ ë°ì´í„°ì—ì„œ í…ŒìŠ¤íŠ¸
    â”œâ”€â”€ ì˜ë£Œ AI ë¶„ì•¼ì—ì„œ ì¸ì •ë°›ì€ ì„±ëŠ¥
    â””â”€â”€ ìƒìš©í™” ê°€ëŠ¥í•œ ìˆ˜ì¤€ì˜ ì •í™•ë„
```

### **C. í˜„ì¬ í”„ë¡œì íŠ¸ì™€ì˜ í˜¸í™˜ì„±**
```
ğŸ”„ í˜¸í™˜ì„± ë¶„ì„:
â”œâ”€â”€ ë™ì¼ ë„ë©”ì¸: í˜¸í¡ìŒ ë¶„ì„ âœ…
â”œâ”€â”€ ìœ ì‚¬í•œ ëª©ì : ì •ìƒ/ë¹„ì •ìƒ êµ¬ë¶„ âœ…
â”œâ”€â”€ ë¼ë²¨ ë§¤í•‘: Normalâ†’Breathing, Abnormalâ†’Wheezing+Noise
â””â”€â”€ í™•ì¥ ê°€ëŠ¥: 2-class â†’ 3-class ë¶„ë¥˜
```

---

## ğŸ”§ **4. Transfer Learning êµ¬í˜„ ë°©ë²• ë° ì½”ë“œ ë¶„ì„**

### **A. .npy íŒŒì¼ í˜•ì‹ ì´í•´**
```
ğŸ“ .npy íŒŒì¼ì´ë€?
â”œâ”€â”€ ì •ì˜: NumPy ë°°ì—´ì„ ì €ì¥í•˜ëŠ” íŒŒì¼ í˜•ì‹
â”œâ”€â”€ ìš©ë„: Pythonì—ì„œ ë°°ì—´ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì €ì¥/ë¡œë“œ
â”œâ”€â”€ ì¥ì :
â”‚   â”œâ”€â”€ ë¹ ë¥¸ ì½ê¸°/ì“°ê¸° ì†ë„
â”‚   â”œâ”€â”€ ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
â”‚   â””â”€â”€ ë°ì´í„° íƒ€ì… ë³´ì¡´
â””â”€â”€ í˜„ì¬ í”„ë¡œì íŠ¸ì˜ .npy íŒŒì¼ë“¤:
    â”œâ”€â”€ operaCT768_feature.npy â†’ 768ì°¨ì› íŠ¹ì§• ë²¡í„°ë“¤
    â”œâ”€â”€ spectrogram_pad8.npy â†’ 8ì´ˆ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ë“¤
    â””â”€â”€ labels.npy â†’ ë¼ë²¨ ë°°ì—´
```

### **B. Transfer Learningì˜ ë‘ ë‹¨ê³„ êµ¬ì¡°**

#### **1ë‹¨ê³„: Pretrained Model (ì´ë¯¸ í›ˆë ¨ë¨)**
```python
# finetune with Yeo Data.ipynbì—ì„œ
from src.benchmark.model_util import initialize_pretrained_model, get_encoder_path

# OperaCT ëª¨ë¸ ë¡œë“œ (31.3M íŒŒë¼ë¯¸í„°)
pretrained_model = initialize_pretrained_model("operaCT")
encoder_path = get_encoder_path("operaCT")
ckpt = torch.load(encoder_path, map_location="cpu")
pretrained_model.load_state_dict(ckpt["state_dict"], strict=False)

# Encoder ë¶€ë¶„ë§Œ ì¶”ì¶œ (ê³ ì •)
net = pretrained_model.encoder  # ì´ ë¶€ë¶„ì€ ê±´ë“œë¦¬ì§€ ì•ŠìŒ
```

#### **2ë‹¨ê³„: Classification Head (ìƒˆë¡œ í•™ìŠµ)**
```python
# finetune with Yeo Data.ipynbì—ì„œ
from src.model.models_eval import AudioClassifier

# ë¶„ë¥˜ê¸°ë§Œ ìƒˆë¡œ í•™ìŠµ (1.5K íŒŒë¼ë¯¸í„°)
model = AudioClassifier(
    net=net,                    # ê³ ì •ëœ pretrained encoder
    head="linear",              # ìƒˆë¡œìš´ ë¶„ë¥˜ í—¤ë“œ
    classes=2,                  # ì •ìƒ/ë¹„ì •ìƒ 2í´ë˜ìŠ¤
    lr=1e-4,                   # í•™ìŠµë¥ 
    l2_strength=1e-4,          # ì •ê·œí™”
    feat_dim=768,              # íŠ¹ì§• ì°¨ì›
    freeze_encoder="none"       # encoder ê³ ì • ì—¬ë¶€
)
```

### **C. íŠ¹ì§• ì¶”ì¶œ ê³¼ì •**
```python
# finetune with Yeo Data.ipynbì˜ extract_and_save_embeddings í•¨ìˆ˜
def extract_and_save_embeddings(feature_dir="feature/yeo_eval/", 
                                pretrain="operaCT", 
                                input_sec=8, 
                                dim=768):
    # 1. ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ë¡œë“œ
    sound_dir_loc = np.load(os.path.join(feature_dir, "sound_dir_loc.npy"))
    
    # 2. OperaCTë¡œ íŠ¹ì§• ì¶”ì¶œ
    from src.benchmark.model_util import extract_opera_feature
    opera_features = extract_opera_feature(
        sound_dir_loc, pretrain=pretrain, input_sec=input_sec, dim=dim
    )
    
    # 3. .npy íŒŒì¼ë¡œ ì €ì¥
    feature_name = pretrain + str(dim)
    np.save(os.path.join(feature_dir, f"{feature_name}_feature.npy"), 
            np.array(opera_features))
    print(f"[extract_and_save_embeddings] {feature_name}_feature.npy ì €ì¥ ì™„ë£Œ!")
```

### **D. ë°ì´í„°ì…‹ í´ë˜ìŠ¤ êµ¬í˜„**
```python
# finetune with Yeo Data.ipynbì˜ AudioDataset í´ë˜ìŠ¤
class AudioDataset(torch.utils.data.Dataset):
    def __init__(self, data, max_len=256, augment=True, from_npy=False, 
                 crop_mode="first", from_audio=False):
        """
        Args:
            data: íŠ¹ì§• ë°ì´í„° ë˜ëŠ” ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            max_len: ìµœëŒ€ ê¸¸ì´ (íŒ¨ë”©/í¬ë¡­)
            augment: ë°ì´í„° ì¦ê°• ì—¬ë¶€
            from_npy: .npy íŒŒì¼ì—ì„œ ë¡œë“œí• ì§€ ì—¬ë¶€
            crop_mode: í¬ë¡­ ë°©ì‹ ("first", "random", "center")
            from_audio: ì›ë³¸ ì˜¤ë””ì˜¤ì—ì„œ ë¡œë“œí• ì§€ ì—¬ë¶€
        """
        self.data = data
        self.max_len = max_len
        self.augment = augment
        self.from_npy = from_npy
        self.crop_mode = crop_mode
        self.from_audio = from_audio
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        if self.from_npy:
            # .npy íŒŒì¼ì—ì„œ íŠ¹ì§• ë¡œë“œ
            features = np.load(self.data[idx])
        elif self.from_audio:
            # ì›ë³¸ ì˜¤ë””ì˜¤ì—ì„œ íŠ¹ì§• ì¶”ì¶œ
            features = self.extract_features_from_audio(self.data[idx])
        else:
            # ì´ë¯¸ ë¡œë“œëœ íŠ¹ì§• ì‚¬ìš©
            features = self.data[idx]
        
        # íŒ¨ë”©/í¬ë¡­ ì²˜ë¦¬
        if len(features) > self.max_len:
            if self.crop_mode == "first":
                features = features[:self.max_len]
            elif self.crop_mode == "random":
                start = np.random.randint(0, len(features) - self.max_len + 1)
                features = features[start:start + self.max_len]
        
        # ë°ì´í„° ì¦ê°•
        if self.augment:
            features = self.apply_augmentation(features)
        
        return torch.FloatTensor(features)
```

### **D. ë°ì´í„° í†µí•© í›ˆë ¨ì˜ ì •ë‹¹ì„±**

#### **ì—¬ëŸ¬ ë°ì´í„°ì…‹ í†µí•© í›ˆë ¨ì´ ê´œì°®ì€ ì´ìœ :**
```
âœ… ë„ë©”ì¸ ì¼ê´€ì„±:
â”œâ”€â”€ ëª¨ë‘ í˜¸í¡ìŒ ë°ì´í„° (ê°™ì€ ë„ë©”ì¸)
â”œâ”€â”€ ê°™ì€ ì˜ë£Œ ëª©ì  (í˜¸í¡ê¸° ì§ˆí™˜ ì§„ë‹¨)
â””â”€â”€ ìœ ì‚¬í•œ ì‹ í˜¸ íŠ¹ì„± (í˜¸í¡ ì£¼ê¸°, ìŒí–¥ íŠ¹ì„±)

âœ… Self-Supervised Learning:
â”œâ”€â”€ ë¼ë²¨ì´ ì•„ë‹Œ íŒ¨í„´ì„ í•™ìŠµ
â”œâ”€â”€ ì§ˆë³‘ë³„ ë¼ë²¨ì´ ì„ì—¬ë„ ë¬¸ì œì—†ìŒ
â””â”€â”€ ì˜¤íˆë ¤ ë” robustí•œ íŠ¹ì§• í•™ìŠµ

âœ… ì‹¤ì œ ì˜ë£Œ í˜„ì¥:
â”œâ”€â”€ ë‹¤ì–‘í•œ í™˜ì, í™˜ê²½, ì§ˆë³‘ì´ ì„ì—¬ ìˆìŒ
â”œâ”€â”€ ì‹¤ì œ ì‚¬ìš© í™˜ê²½ê³¼ ìœ ì‚¬
â””â”€â”€ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ

âœ… ì—°êµ¬ ê´€ë¡€:
â”œâ”€â”€ ì˜ë£Œ AIì—ì„œ ì¼ë°˜ì ì¸ ë°©ë²•
â”œâ”€â”€ ICBHI, PhysioNet ë“±ì—ì„œë„ ì‚¬ìš©
â””â”€â”€ ê²€ì¦ëœ ì ‘ê·¼ë²•
```

### **E. í•™ìŠµ ê³¼ì • êµ¬í˜„**

#### **ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸ (finetune with Yeo Data.ipynb ì°¸ì¡°):**
```python
# finetune with Yeo Data.ipynbì˜ train_loocv í•¨ìˆ˜
def train_loocv(feature_dir="feature/yeo_loocv/",
                input_sec=8.0,
                batch_size=64,
                epochs=10,
                lr=1e-4,
                l2_strength=1e-4,
                head="linear",
                feat_dim=768,
                seed=42):
    """
    í™˜ì ë‹¨ìœ„ LOOCV ìˆ˜í–‰ ì˜ˆì‹œ.
    (ìŠ¤í™íŠ¸ë¡œê·¸ë¨ = spectrogram_pad{input_sec}.npy ì‚¬ìš©)
    """
    import pytorch_lightning as pl
    from torch.utils.data import DataLoader
    
    pl.seed_everything(seed, workers=True)
    np.random.seed(seed)
    
    # 1. ë°ì´í„° ë¡œë“œ
    x_data = np.load(os.path.join(feature_dir, f"spectrogram_pad{input_sec}.npy"))
    labels = np.load(os.path.join(feature_dir, "labels.npy"))
    patient_ids = np.load(os.path.join(feature_dir, "patient_ids.npy"))
    
    # 2. í™˜ìë³„ LOOCV
    unique_patients = np.unique(patient_ids)
    for test_patient in unique_patients:
        # Train/Val/Test ë¶„í• 
        train_mask = patient_ids != test_patient
        test_mask = patient_ids == test_patient
        
        x_train, y_train = x_data[train_mask], labels[train_mask]
        x_test, y_test = x_data[test_mask], labels[test_mask]
        
        # 3. ëª¨ë¸ ì´ˆê¸°í™”
        pretrained_model = initialize_pretrained_model("operaCT")
        net = pretrained_model.encoder
        
        model = AudioClassifier(
            net=net, 
            head=head, 
            classes=2, 
            lr=lr, 
            l2_strength=l2_strength, 
            feat_dim=feat_dim, 
            freeze_encoder="none"  
        )
        
        # 4. ë°ì´í„°ì…‹ ë° ë¡œë” ìƒì„±
        train_dataset = AudioDataset(x_train, y_train, from_npy=False)
        test_dataset = AudioDataset(x_test, y_test, from_npy=False)
        
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
        
        # 5. í•™ìŠµ ì‹¤í–‰
        trainer = pl.Trainer(
            max_epochs=epochs,
            accelerator="gpu" if torch.cuda.is_available() else "cpu",
            devices=1,
            logger=False,
            callbacks=[DecayLearningRate()]
        )
        
        trainer.fit(model, train_loader, test_loader)
        trainer.test(model, test_loader)
```

### **F. ì½”ë“œ ê°œì¡° ê°€ëŠ¥ì„±**

#### **ê°œì¡° ê°€ëŠ¥í•œ ë¶€ë¶„:**
```python
# 1. í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •
model = AudioClassifier(
    net=net,
    head="linear",              # "linear", "mlp" ë“±ìœ¼ë¡œ ë³€ê²½ ê°€ëŠ¥
    classes=2,                  # 3í´ë˜ìŠ¤ë¡œ í™•ì¥ ê°€ëŠ¥
    lr=1e-4,                   # í•™ìŠµë¥  ì¡°ì •
    l2_strength=1e-4,          # ì •ê·œí™” ê°•ë„ ì¡°ì •
    feat_dim=768,              # íŠ¹ì§• ì°¨ì› ì¡°ì •
    freeze_encoder="none"       # "all", "none" ë“±ìœ¼ë¡œ ë³€ê²½
)

# 2. ë°ì´í„° ë¡œë” ì„¤ì •
trainer = pl.Trainer(
    max_epochs=10,              # ì—í¬í¬ ìˆ˜ ì¡°ì •
    batch_size=64,              # ë°°ì¹˜ í¬ê¸° ì¡°ì •
    accelerator="gpu" if torch.cuda.is_available() else "cpu",
    devices=1,
    callbacks=[DecayLearningRate()]  # ì½œë°± í•¨ìˆ˜ ì¶”ê°€/ìˆ˜ì •
)
```

#### **ê°œì¡° ì œí•œì‚¬í•­:**
```
âŒ OperaCT ëª¨ë¸ ìì²´ëŠ” ìˆ˜ì • ë¶ˆê°€ (ì´ë¯¸ í›ˆë ¨ë¨)
âŒ ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ëŠ” ê³ ì •
âŒ ëª¨ë¸ ì•„í‚¤í…ì²˜ ë³€ê²½ ì‹œ ì¬í›ˆë ¨ í•„ìš”
```

---

## âš ï¸ **5. Limitations & Fine-Tuning Options (ì œí•œ ì‚¬í•­ ë° ë¯¸ì„¸ ì¡°ì • ì˜µì…˜)**

### **A. ì£¼ìš” ì œí•œ ì‚¬í•­**
```
ğŸš¨ ì‹ë³„ëœ ì œí•œ ì‚¬í•­:
â”œâ”€â”€ í™˜ìë³„ ê°€ë³€ì„±: 6ëª… í™˜ìë¡œëŠ” ì¼ë°˜í™” ì–´ë ¤ì›€
â”œâ”€â”€ ë¼ë²¨ ë¶ˆê· í˜•: Yeo ë°ì´í„° 39ê°œ (ëª¨ë‘ ë¹„ì •ìƒ)
â”œâ”€â”€ í™˜ê²½ ì˜ì¡´ì„±: ë…¹ìŒ í™˜ê²½ì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™”
â”œâ”€â”€ ë„ë©”ì¸ ê°­: ICBHIì™€ Yeo ë°ì´í„° ê°„ ì°¨ì´
â””â”€â”€ ì‹¤ì‹œê°„ ì²˜ë¦¬: 31.3M íŒŒë¼ë¯¸í„°ë¡œ ì¸í•œ ì—°ì‚° ë¶€ë‹´
```

### **B. Fine-tuning ì „ëµ**
```python
# 1ë‹¨ê³„: Encoder Freeze + Headë§Œ í•™ìŠµ
pretrained_encoder.freeze()
classifier_head = Linear(768, 3)  # 3-class

# 2ë‹¨ê³„: End-to-End Fine-tuning
for param in pretrained_encoder.parameters():
    param.requires_grad = True

# 3ë‹¨ê³„: Learning Rate Scheduling
optimizer = Adam([
    {'params': pretrained_encoder.parameters(), 'lr': 1e-5},
    {'params': classifier_head.parameters(), 'lr': 1e-3}
])
```

### **C. ì„±ëŠ¥ ê°œì„  ë°©ì•ˆ**
```
ğŸ“ˆ ì„±ëŠ¥ í–¥ìƒ ì „ëµ:
â”œâ”€â”€ ë°ì´í„° ì¦ê°•: Random crop, mask, multiply
â”œâ”€â”€ ì •ê·œí™”: BatchNorm, LayerNorm, Dropout
â”œâ”€â”€ ì•™ìƒë¸”: ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°í•©
â”œâ”€â”€ ì••ì¶•: Knowledge Distillationìœ¼ë¡œ ê²½ëŸ‰í™”
â””â”€â”€ ì ì‘ì  í•™ìŠµ: Domain Adaptation ê¸°ë²•
```

---

## ğŸ“Š **5. ì‹¤í—˜ ê²°ê³¼ ë° ì„±ëŠ¥ ë¶„ì„**

### **A. Pretraining ì„±ëŠ¥**
```
ğŸ† Self-Supervised Learning ê²°ê³¼:
â”œâ”€â”€ Validation Accuracy: 84% (ICBHI ë°ì´í„°)
â”œâ”€â”€ Model Size: 31.3M parameters (125MB)
â”œâ”€â”€ Training Epochs: 129 epochs
â””â”€â”€ Convergence: ì•ˆì •ì ì¸ ìˆ˜ë ´ íŒ¨í„´
```

### **B. Fine-tuning ì„±ëŠ¥ (Yeo ë°ì´í„°)**
```
ğŸ“ˆ Transfer Learning ê²°ê³¼:
â”œâ”€â”€ Test AUC: 0.875 ~ 0.9375
â”œâ”€â”€ Test ACC: 0.75 ~ 0.875
â”œâ”€â”€ LOOCV: 24-fold cross-validation
â””â”€â”€ Best Model: Linear head + OperaCT encoder
```

### **C. ì„±ëŠ¥ ë¹„êµ**
```
âš–ï¸ ë°©ë²•ë¡ ë³„ ì„±ëŠ¥ ë¹„êµ:
â”œâ”€â”€ From Scratch: ë‚®ì€ ì„±ëŠ¥ (ë°ì´í„° ë¶€ì¡±)
â”œâ”€â”€ OperaCT Transfer: ë†’ì€ ì„±ëŠ¥ (84%+)
â”œâ”€â”€ ê¸°ì¡´ ë°©ë²•: 100% (ê³¼ì í•© ì˜ì‹¬)
â””â”€â”€ OperaCT + 3-class: ì˜ˆìƒ 90%+ ì„±ëŠ¥
```

---

## ğŸš€ **6. í˜„ì¬ í”„ë¡œì íŠ¸ ì ìš© ë°©ì•ˆ**

### **A. ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ë¶€ë¶„**
```python
# 1. OperaCT íŠ¹ì§• ì¶”ì¶œê¸° í™œìš©
opera_features = extract_opera_feature(audio_files, pretrain="operaCT")

# 2. 3-class ë¶„ë¥˜ê¸° Fine-tuning
model = AudioClassifier(
    net=pretrained_encoder,
    num_classes=3,  # breathing, wheezing, noise
    feat_dim=768
)

# 3. ì‹¤ì‹œê°„ ì„¸ê·¸ë©˜í…Œì´ì…˜
segments = detect_wheezing_segments(audio, model)
```

### **B. ë‹¨ê³„ë³„ êµ¬í˜„ ê³„íš**
```
ğŸ“… Phase 1: ëª¨ë¸ í†µí•© (1ì£¼)
â”œâ”€â”€ OperaCT ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì„¤ì •
â”œâ”€â”€ í˜„ì¬ ë°ì´í„°ì— íŠ¹ì§• ì¶”ì¶œ ì ìš©
â””â”€â”€ 3-class ë¶„ë¥˜ê¸° fine-tuning

ğŸ“… Phase 2: ì„±ëŠ¥ ê²€ì¦ (1ì£¼)
â”œâ”€â”€ ê¸°ì¡´ ë°©ë²•ê³¼ ì„±ëŠ¥ ë¹„êµ
â”œâ”€â”€ ì‹¤ì‹œê°„ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
â””â”€â”€ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì •í™•ë„ í‰ê°€

ğŸ“… Phase 3: ìµœì í™” (1ì£¼)
â”œâ”€â”€ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
â”œâ”€â”€ ë°ì´í„° ì¦ê°• ê¸°ë²• ì ìš©
â””â”€â”€ ëª¨ë¸ ê²½ëŸ‰í™” (í•„ìš”ì‹œ)
```

### **C. ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ**
```
ğŸ¯ ê¸°ëŒ€ íš¨ê³¼:
â”œâ”€â”€ ì„±ëŠ¥ í–¥ìƒ: 100% â†’ 95%+ (ë” robust)
â”œâ”€â”€ í•™ìŠµ ì†ë„: ë¹ ë¥¸ ìˆ˜ë ´ (ëª‡ ì—í­ ë‚´)
â”œâ”€â”€ ì¼ë°˜í™”: ìƒˆë¡œìš´ í™˜ì ë°ì´í„°ì— robust
â”œâ”€â”€ ì‹¤ì‹œê°„ ì²˜ë¦¬: 768ì°¨ì› íŠ¹ì§•ìœ¼ë¡œ ë¹ ë¥¸ ì¶”ë¡ 
â””â”€â”€ í™•ì¥ì„±: ì¶”ê°€ í´ë˜ìŠ¤ í•™ìŠµ ìš©ì´
```

---

## ğŸ’¡ **7. í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ë° ê¶Œì¥ì‚¬í•­**

### **A. ì „ì„ì ì‘ì—…ì˜ ê°€ì¹˜**
```
âœ… ë§¤ìš° ê°€ì¹˜ ìˆëŠ” Transfer Learning êµ¬í˜„:
â”œâ”€â”€ í˜¸í¡ìŒ ë„ë©”ì¸ íŠ¹í™” ëª¨ë¸ êµ¬ì¶•
â”œâ”€â”€ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ í†µí•© í•™ìŠµ
â”œâ”€â”€ Self-Supervised Learning í™œìš©
â”œâ”€â”€ ê²€ì¦ëœ ì„±ëŠ¥ê³¼ ì•ˆì •ì„±
â””â”€â”€ í˜„ì¬ í”„ë¡œì íŠ¸ì— ë°”ë¡œ í™œìš© ê°€ëŠ¥
```

### **B. ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì•¡ì…˜ ì•„ì´í…œ**
```
ğŸ¯ ìš°ì„ ìˆœìœ„ë³„ ì‹¤í–‰ ê³„íš:
1. OperaCT ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° í™˜ê²½ ì„¤ì •
2. í˜„ì¬ 39ê°œ ìƒ˜í”Œì— OperaCT íŠ¹ì§• ì¶”ì¶œ
3. 3-class ë¶„ë¥˜ê¸° fine-tuning (breathing/wheezing/noise)
4. ê¸°ì¡´ ë°©ë²•ê³¼ ì„±ëŠ¥ ë¹„êµ ì‹¤í—˜
5. ì‹¤ì‹œê°„ ì„¸ê·¸ë©˜í…Œì´ì…˜ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
```

### **C. ì¥ê¸°ì  ë°œì „ ë°©í–¥**
```
ğŸ”® ë¯¸ë˜ ë°œì „ ë°©í–¥:
â”œâ”€â”€ ë” ë§ì€ í™˜ì ë°ì´í„° ìˆ˜ì§‘
â”œâ”€â”€ ì‹¤ì‹œê°„ ëª¨ë°”ì¼ ì•± ê°œë°œ
â”œâ”€â”€ ì˜ë£Œì§„ í”¼ë“œë°± ë°˜ì˜ ì‹œìŠ¤í…œ
â”œâ”€â”€ ë‹¤êµ­ê°€ ë°ì´í„°ì…‹ í™•ì¥
â””â”€â”€ ì„ìƒ ì‹œí—˜ì„ í†µí•œ ê²€ì¦
```

---

## ğŸ“ **8. ê²°ë¡ **

**ì „ì„ìì˜ Yeolab_collab Transfer Learning í”„ë¡œì íŠ¸ëŠ” í˜¸í¡ìŒ ë¶„ì„ ë¶„ì•¼ì—ì„œ ë§¤ìš° ì„±ìˆ™í•˜ê³  ì‹¤ìš©ì ì¸ ì†”ë£¨ì…˜**ì…ë‹ˆë‹¤. 

**í•µì‹¬ ê°•ì :**
- **OperaCT ëª¨ë¸**: í˜¸í¡ìŒ ë„ë©”ì¸ì— íŠ¹í™”ëœ 768ì°¨ì› ê³ í’ˆì§ˆ íŠ¹ì§•
- **Self-Supervised Learning**: ë¼ë²¨ ë¶€ì¡± ë¬¸ì œ í•´ê²°
- **ê²€ì¦ëœ ì„±ëŠ¥**: ICBHI ë°ì´í„°ì…‹ì—ì„œ 84% ì •í™•ë„
- **ì¦‰ì‹œ í™œìš© ê°€ëŠ¥**: í˜„ì¬ í”„ë¡œì íŠ¸ì— ë°”ë¡œ ì ìš© ê°€ëŠ¥

**ê¶Œì¥ì‚¬í•­:**
OperaCT ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ 3-class ë¶„ë¥˜ê¸°ë¥¼ fine-tuningí•˜ì—¬ í˜„ì¬ í”„ë¡œì íŠ¸ì˜ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚¤ê³ , ë” robustí•˜ê³  ì¼ë°˜í™”ëœ wheezing ê°ì§€ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.

---

*ì´ ë¶„ì„ì€ Yeolab_collab Transfer Learning í´ë”ì˜ ëª¨ë“  ë…¸íŠ¸ë¶, ë°ì´í„°, ëª¨ë¸ íŒŒì¼ì„ ìƒì„¸íˆ ë¶„ì„í•œ ê²°ê³¼ì…ë‹ˆë‹¤.*
