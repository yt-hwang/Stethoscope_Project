#!/usr/bin/env python3
"""
ê°„ë‹¨í•œ í˜¸í¡ìŒ ë°ì´í„° ì²˜ë¦¬ê¸° - ì‹¤ì œ íŒŒì¼ êµ¬ì¡°ì— ë§ì¶° ìˆ˜ì •
"""

import numpy as np
import pandas as pd
import librosa
from pathlib import Path
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

# í”„ë¡œì íŠ¸ ì„¤ì •
from src.config import SR, N_FFT, HOP_LEN, N_MELS, FMIN, FMAX
from src.audio_io import load_audio, pre_emphasis
from src.features import stft_mag_db, logmel, mfcc, wheeze_indicators

def process_audio_file(audio_path, label='unknown'):
    """
    ë‹¨ì¼ ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬
    """
    print(f"Processing: {audio_path}")
    
    try:
        # 1. ì˜¤ë””ì˜¤ ë¡œë”© ë° ì „ì²˜ë¦¬
        y = load_audio(str(audio_path))
        y = pre_emphasis(y)
        
        # 2. ì •ê·œí™”
        rms = np.sqrt(np.mean(y**2))
        if rms > 0:
            y = y / rms * 0.1
        
        # 3. íŠ¹ì§• ì¶”ì¶œ
        features = {}
        
        # STFT ë° Mel-spectrogram
        S_db = stft_mag_db(y)
        M_db = logmel(y)
        mfcc_features = mfcc(y, n_mfcc=20)
        
        # Wheezing ì§€í‘œ
        indicators = wheeze_indicators(S_db)
        
        # ì‹œê°„ ë„ë©”ì¸ íŠ¹ì§•
        rms_feature = librosa.feature.rms(y=y, hop_length=HOP_LEN)[0]
        zcr = librosa.feature.zero_crossing_rate(y, hop_length=HOP_LEN)[0]
        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=SR, hop_length=HOP_LEN)[0]
        
        # íŠ¹ì§• ë²¡í„° ìƒì„±
        feature_vector = []
        
        # MFCC í†µê³„ (í‰ê· , í‘œì¤€í¸ì°¨)
        feature_vector.extend(np.mean(mfcc_features, axis=1))  # 20ê°œ
        feature_vector.extend(np.std(mfcc_features, axis=1))   # 20ê°œ
        
        # Wheezing ì§€í‘œ í†µê³„
        feature_vector.append(np.mean(indicators['flatness']))
        feature_vector.append(np.std(indicators['flatness']))
        feature_vector.append(np.mean(indicators['centroid']))
        feature_vector.append(np.std(indicators['centroid']))
        feature_vector.append(np.mean(indicators['e_ratio_100_1k__1k_2_5k']))
        feature_vector.append(np.std(indicators['e_ratio_100_1k__1k_2_5k']))
        
        # ì‹œê°„ ë„ë©”ì¸ íŠ¹ì§• í†µê³„
        feature_vector.append(np.mean(rms_feature))
        feature_vector.append(np.std(rms_feature))
        feature_vector.append(np.mean(zcr))
        feature_vector.append(np.std(zcr))
        feature_vector.append(np.mean(spectral_centroid))
        feature_vector.append(np.std(spectral_centroid))
        
        return np.array(feature_vector), label
        
    except Exception as e:
        print(f"Error processing {audio_path}: {e}")
        return None, None

def create_simple_dataset():
    """
    ì‹¤ì œ íŒŒì¼ êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°„ë‹¨í•œ ë°ì´í„°ì…‹ ìƒì„±
    """
    print("ğŸš€ ê°„ë‹¨í•œ í˜¸í¡ìŒ ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘")
    
    # ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
    audio_base_path = Path("../../Audio shared/Hospital sound")
    
    # í™˜ìë³„ ë¼ë²¨ë§ (ë©”íƒ€ë°ì´í„° ê¸°ë°˜)
    patient_labels = {
        'WEBSS002': 'asthma',  # ì²œì‹
        'WEBSS003': 'asthma',  # ì²œì‹
        'WEBSS004': 'asthma',  # ì²œì‹
        'WEBSS005': 'normal',  # No pull diagnosis
        'WEBSS006': 'asthma',  # ì²œì‹
        'WEBSS007': 'asthma'   # ì²œì‹
    }
    
    dataset = []
    labels = []
    
    # ê° í™˜ì í´ë” ì²˜ë¦¬
    for folder_name in audio_base_path.iterdir():
        if folder_name.is_dir():
            patient_id = folder_name.name.split('_')[0]  # WEBSS002_3 -> WEBSS002
            label = patient_labels.get(patient_id, 'unknown')
            
            print(f"\nğŸ“ Processing {folder_name.name} (Label: {label})")
            
            # í´ë” ë‚´ ëª¨ë“  wav íŒŒì¼ ì²˜ë¦¬
            for audio_file in folder_name.glob("*.wav"):
                feature_vector, file_label = process_audio_file(audio_file, label)
                
                if feature_vector is not None:
                    dataset.append(feature_vector)
                    labels.append(file_label)
                    print(f"  âœ… {audio_file.name}: {len(feature_vector)} features")
                else:
                    print(f"  âŒ Failed: {audio_file.name}")
    
    if dataset:
        X = np.array(dataset)
        y = np.array(labels)
        
        print(f"\nğŸ“Š Dataset Statistics:")
        print(f"Total samples: {len(X)}")
        print(f"Feature dimension: {X.shape[1]}")
        
        # í´ë˜ìŠ¤ ë¶„í¬
        unique, counts = np.unique(y, return_counts=True)
        for cls, count in zip(unique, counts):
            print(f"  {cls}: {count} samples")
        
        return X, y
    else:
        print("âŒ No data processed successfully")
        return None, None

def analyze_features(X, y):
    """
    íŠ¹ì§• ë¶„ì„ ë° ì‹œê°í™”
    """
    print("\nğŸ” Feature Analysis")
    
    # íŠ¹ì§• ì´ë¦„ë“¤
    feature_names = [
        'mfcc_mean_0', 'mfcc_mean_1', 'mfcc_mean_2', 'mfcc_mean_3', 'mfcc_mean_4',
        'mfcc_mean_5', 'mfcc_mean_6', 'mfcc_mean_7', 'mfcc_mean_8', 'mfcc_mean_9',
        'mfcc_mean_10', 'mfcc_mean_11', 'mfcc_mean_12', 'mfcc_mean_13', 'mfcc_mean_14',
        'mfcc_mean_15', 'mfcc_mean_16', 'mfcc_mean_17', 'mfcc_mean_18', 'mfcc_mean_19',
        'mfcc_std_0', 'mfcc_std_1', 'mfcc_std_2', 'mfcc_std_3', 'mfcc_std_4',
        'mfcc_std_5', 'mfcc_std_6', 'mfcc_std_7', 'mfcc_std_8', 'mfcc_std_9',
        'mfcc_std_10', 'mfcc_std_11', 'mfcc_std_12', 'mfcc_std_13', 'mfcc_std_14',
        'mfcc_std_15', 'mfcc_std_16', 'mfcc_std_17', 'mfcc_std_18', 'mfcc_std_19',
        'flatness_mean', 'flatness_std', 'centroid_mean', 'centroid_std',
        'energy_ratio_mean', 'energy_ratio_std', 'rms_mean', 'rms_std',
        'zcr_mean', 'zcr_std', 'spectral_centroid_mean', 'spectral_centroid_std'
    ]
    
    # í´ë˜ìŠ¤ë³„ íŠ¹ì§• ë¶„í¬ ì‹œê°í™”
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('Feature Distribution by Class', fontsize=16)
    
    # ì¤‘ìš”í•œ íŠ¹ì§•ë“¤ ì„ íƒ
    important_features = [
        ('flatness_mean', 40),  # Wheezing ì§€í‘œ
        ('centroid_mean', 42),  # ì£¼íŒŒìˆ˜ ì¤‘ì‹¬
        ('energy_ratio_mean', 44),  # ì—ë„ˆì§€ ë¹„ìœ¨
        ('spectral_centroid_mean', 50)  # ìŠ¤í™íŠ¸ëŸ¼ ì¤‘ì‹¬
    ]
    
    for idx, (feature_name, feature_idx) in enumerate(important_features):
        ax = axes[idx // 2, idx % 2]
        
        # í´ë˜ìŠ¤ë³„ ë°•ìŠ¤í”Œë¡¯
        class_data = []
        class_labels = []
        
        for cls in np.unique(y):
            class_mask = y == cls
            class_data.append(X[class_mask, feature_idx])
            class_labels.append(cls)
        
        ax.boxplot(class_data, labels=class_labels)
        ax.set_title(f'{feature_name}')
        ax.set_ylabel('Value')
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('feature_analysis.png', dpi=300, bbox_inches='tight')
    print("ğŸ’¾ Feature analysis saved: feature_analysis.png")

def prepare_training_data(X, y, test_size=0.2, random_state=42):
    """
    í›ˆë ¨ ë°ì´í„° ì¤€ë¹„
    """
    # ë¼ë²¨ ì¸ì½”ë”©
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y)
    
    # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_encoded, test_size=test_size, random_state=random_state, stratify=y_encoded
    )
    
    # íŠ¹ì§• ì •ê·œí™”
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    print(f"\nâœ… Training data prepared:")
    print(f"Training set: {X_train_scaled.shape}")
    print(f"Test set: {X_test_scaled.shape}")
    print(f"Classes: {label_encoder.classes_}")
    
    return X_train_scaled, X_test_scaled, y_train, y_test, scaler, label_encoder

def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    """
    # ë°ì´í„°ì…‹ ìƒì„±
    X, y = create_simple_dataset()
    
    if X is not None and y is not None:
        # íŠ¹ì§• ë¶„ì„
        analyze_features(X, y)
        
        # í›ˆë ¨ ë°ì´í„° ì¤€ë¹„
        X_train, X_test, y_train, y_test, scaler, label_encoder = prepare_training_data(X, y)
        
        # ê²°ê³¼ ì €ì¥
        np.savez('simple_dataset.npz',
                X_train=X_train, X_test=X_test,
                y_train=y_train, y_test=y_test)
        
        print("ğŸ’¾ Dataset saved: simple_dataset.npz")
        
        # ê°„ë‹¨í•œ ë¶„ë¥˜ê¸° í…ŒìŠ¤íŠ¸
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.metrics import classification_report, confusion_matrix
        
        print("\nğŸ¤– Testing with Random Forest...")
        rf = RandomForestClassifier(n_estimators=100, random_state=42)
        rf.fit(X_train, y_train)
        
        y_pred = rf.predict(X_test)
        
        print("\nğŸ“ˆ Classification Report:")
        print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))
        
        print("\nğŸ“Š Confusion Matrix:")
        print(confusion_matrix(y_test, y_pred))
        
    else:
        print("âŒ Dataset creation failed")

if __name__ == "__main__":
    main()
