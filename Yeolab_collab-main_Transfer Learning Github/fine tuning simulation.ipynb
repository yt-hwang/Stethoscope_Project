{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3af9ffc-8dbc-4fa0-81d1-089d8cb7dbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /workspace\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# 원하는 디렉토리로 이동\n",
    "os.chdir(\"/workspace\")\n",
    "# 현재 작업 디렉토리 확인\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7268d92-d39a-4434-8fe2-6649cd66f44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 : Import Libraries\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob as gb\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from lightning.pytorch.utilities import CombinedLoader\n",
    "\n",
    "# (src.model.models_cola) 안의 Cola, ColaMD 모델\n",
    "from src.model.models_cola import Cola, ColaMD\n",
    "\n",
    "\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch import seed_everything\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from src.util import random_crop, random_mask, random_multiply, crop_first\n",
    "from src.model.models_eval import AudioClassifier, AudioClassifierCLAP, AudioClassifierAudioMAE\n",
    "from src.benchmark.model_util import get_encoder_path, initialize_pretrained_model\n",
    "from src.util import train_test_split_from_list\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ddde74a-63ed-4c24-9b4d-1c8c7cba01e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_data = pd.read_csv('data/icbhi/ICBHI_Challenge_diagnosis.txt',\n",
    "                          dtype=str, sep='\\t', names=['userID', 'class'])\n",
    "splits_data = pd.read_csv('data/icbhi/ICBHI_challenge_train_test.txt',\n",
    "                          dtype=str, sep='\\t', names=['fileID', 'group'])\n",
    "demographics_data = pd.read_csv('data/icbhi/ICBHI_Challenge_demographic_information.txt',\n",
    "                                dtype=str, sep='\\t', names=['userId', 'Age', 'Sex', 'Adult_BMI', 'Child Weight', 'Child Height'])\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "SR = 16000\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "data_dir = \"data/icbhi/\"\n",
    "feature_dir = \"feature/icbhidisease_eval/\"\n",
    "\n",
    "audio_dir = data_dir + \"ICBHI_final_database/\"\n",
    "\n",
    "\n",
    "def process_disease():\n",
    "    sound_dir_loc = np.array(\n",
    "        gb.glob(\"data/icbhi/ICBHI_final_database/*.wav\"))\n",
    "    labels = []\n",
    "    filenames = []\n",
    "    split = []\n",
    "\n",
    "    for i in tqdm(range(sound_dir_loc.shape[0])):\n",
    "        filename = sound_dir_loc[i].strip().split('.')[0]\n",
    "        fileID = filename.split('/')[-1].split('.')[0]\n",
    "        userID = filename.split('/')[-1].split('_')[0]\n",
    "        disease_label = labels_data[\"class\"][labels_data.userID ==\n",
    "                                             userID].values[0]\n",
    "        file_split = splits_data[\"group\"][splits_data.fileID ==\n",
    "                                          fileID].values[0]\n",
    "\n",
    "        filenames.append(sound_dir_loc[i])\n",
    "        labels.append(disease_label)\n",
    "        split.append(file_split)\n",
    "\n",
    "    np.save(feature_dir + \"labels.npy\", labels)\n",
    "    np.save(feature_dir + \"sound_dir_loc.npy\", filenames)\n",
    "    np.save(feature_dir + \"split.npy\", split)\n",
    "\n",
    "\n",
    "def extract_and_save_embeddings(feature=\"operaCE\", input_sec=8, dim=1280):\n",
    "    sound_dir_loc = np.load(feature_dir + \"sound_dir_loc.npy\")\n",
    "\n",
    "    from src.benchmark.model_util import extract_opera_feature\n",
    "    opera_features = extract_opera_feature(\n",
    "        sound_dir_loc, pretrain=feature, input_sec=input_sec, dim=dim)\n",
    "    feature += str(dim)\n",
    "    np.save(feature_dir + feature + \"_feature.npy\", np.array(opera_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98804b09-7741-4b5b-aa73-9082cdc3cc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 920/920 [00:00<00:00, 2523.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting feature from operaCT model with input_sec 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/audio/lib/python3.10/site-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "100%|███████████████████████████████████████| 920/920 [2:00:30<00:00,  7.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(920, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pretrain = \"operaCT\"\n",
    "dim = 768\n",
    "min_len_htsat = 8\n",
    "\n",
    "process_disease()\n",
    "\n",
    "extract_and_save_embeddings(\n",
    "    feature = \"operaCT\", input_sec=min_len_htsat, dim=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b92c6de4-edca-4d7e-9825-9c1b3d1dfd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, max_len=256, augment=True, from_npy=False, crop_mode=\"first\", from_audio=False):\n",
    "        self.data = data[0]\n",
    "        self.label = data[1]\n",
    "        self.max_len = max_len\n",
    "        self.augment = augment\n",
    "        self.from_npy = from_npy\n",
    "        self.crop_mode = crop_mode\n",
    "        self.from_audio = from_audio\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.from_npy:\n",
    "            npy_path = self.data[idx]\n",
    "            x = np.load(npy_path + \".npy\")\n",
    "        else:\n",
    "            x = self.data[idx]\n",
    "\n",
    "        label = self.label[idx]\n",
    "\n",
    "        if self.from_audio:\n",
    "            return x, label\n",
    "\n",
    "        if self.max_len:\n",
    "            if self.crop_mode == \"random\":\n",
    "                x = random_crop(x, crop_size=self.max_len)\n",
    "            else:\n",
    "                x = crop_first(x, crop_size=self.max_len)\n",
    "\n",
    "        if self.augment:\n",
    "            x = random_mask(x)\n",
    "            x = random_multiply(x)\n",
    "\n",
    "        x = torch.tensor(x, dtype=torch.float)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return x, label\n",
    "\n",
    "\n",
    "class DecayLearningRate(pl.Callback):\n",
    "    def __init__(self):\n",
    "        self.old_lrs = []\n",
    "\n",
    "    def on_train_start(self, trainer, pl_module):\n",
    "        # track the initial learning rates\n",
    "        for opt_idx, optimizer in enumerate(trainer.optimizers):\n",
    "            group = []\n",
    "            for param_group in optimizer.param_groups:\n",
    "                group.append(param_group[\"lr\"])\n",
    "            self.old_lrs.append(group)\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        for opt_idx, optimizer in enumerate(trainer.optimizers):\n",
    "            old_lr_group = self.old_lrs[opt_idx]\n",
    "            new_lr_group = []\n",
    "            for p_idx, param_group in enumerate(optimizer.param_groups):\n",
    "                old_lr = old_lr_group[p_idx]\n",
    "                new_lr = old_lr * 0.99\n",
    "                new_lr_group.append(new_lr)\n",
    "                param_group[\"lr\"] = new_lr\n",
    "            self.old_lrs[opt_idx] = new_lr_group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdc51d83-1157-47f5-89f5-d38bb1c3b726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_icbhidisease(n_cls=2, pretrain=\"operaCT\", l2_strength=1e-4, epochs=64, batch_size=64, lr=1e-4, head=\"linear\", feat_dim=1280):\n",
    "    print(\"*\" * 48)\n",
    "    print(\"training dataset ICBHI disease from model pretrained on\", pretrain, \"with l2_strength\", l2_strength, \"lr\", lr, \"head\", head)\n",
    "\n",
    "    feature_dir = \"feature/icbhidisease_eval/\"\n",
    "\n",
    "    from_audio = False\n",
    "    if pretrain == 'operaCT':\n",
    "        if not os.path.exists(feature_dir + \"spectrogram_pad8.npy\"):\n",
    "            from src.util import get_split_signal_librosa\n",
    "            sound_dir_loc = np.load(feature_dir + \"sound_dir_loc.npy\")\n",
    "            x_data = []\n",
    "            for audio_file in sound_dir_loc:\n",
    "                #print(audio_file[:-4])\n",
    "                #print(audio_file)\n",
    "                data = get_split_signal_librosa(\"\", audio_file[:-4], spectrogram=True, input_sec=8.18, trim_tail=False)[0]\n",
    "                # print(data.shape)\n",
    "                x_data.append(data)\n",
    "            x_data = np.array(x_data)\n",
    "            np.save(feature_dir + \"spectrogram_pad8.npy\", x_data)\n",
    "\n",
    "        x_data = np.load(feature_dir + \"spectrogram_pad8.npy\")\n",
    "        pretrained_model = initialize_pretrained_model(pretrain)\n",
    "        if pretrain == \"null\":\n",
    "            lr = 1e-4\n",
    "            epochs = 64\n",
    "            print(\"-\" * 20 + \"training from scratch\")\n",
    "        else:\n",
    "            encoder_path = get_encoder_path(pretrain)\n",
    "            print(\"loading weights from\", encoder_path)\n",
    "            ckpt = torch.load(encoder_path)\n",
    "            pretrained_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n",
    "        \n",
    "        freeze_encoder = \"early\" if pretrain == \"operaCE\" else \"none\"\n",
    "        net = pretrained_model.encoder\n",
    "        #dim_out = pretrained_model.encoder.out_dim\n",
    "        model = AudioClassifier(net=net, head=head, classes=n_cls, lr=lr, l2_strength=l2_strength, feat_dim=feat_dim, freeze_encoder=freeze_encoder)\n",
    "\n",
    "\n",
    "    y_set = np.load(feature_dir + \"split.npy\")\n",
    "    y_label = np.load(feature_dir + \"labels.npy\")\n",
    "    print(collections.Counter(y_label))\n",
    "\n",
    "    mask = (y_label == \"Healthy\") | (y_label == \"COPD\")\n",
    "    y_label = y_label[mask]\n",
    "    y_set = y_set[mask]\n",
    "    x_data = x_data[mask]   \n",
    "\n",
    "    label_dict = {\"Healthy\": 0, \"COPD\": 1}\n",
    "    y_label = np.array([label_dict[y] for y in y_label])\n",
    "\n",
    "    x_data_train, x_data_test, y_label_train, y_label_test = train_test_split_from_list(x_data, y_label, y_set)\n",
    "\n",
    "    x_data_train, x_data_vad, y_label_train, y_label_vad = train_test_split(\n",
    "        x_data_train, y_label_train, test_size=0.2, random_state=1337, stratify=y_label_train\n",
    "    )\n",
    "\n",
    "    print(collections.Counter(y_label_train))\n",
    "    print(collections.Counter(y_label_vad))\n",
    "    print(collections.Counter(y_label_test))\n",
    "\n",
    "    train_data = AudioDataset((x_data_train, y_label_train), augment=False, max_len=False, from_audio=from_audio)\n",
    "    test_data = AudioDataset((x_data_test, y_label_test), augment=False, max_len=False, from_audio=from_audio)\n",
    "    val_data = AudioDataset((x_data_vad, y_label_vad), augment=False, max_len=False, from_audio=from_audio)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_data, batch_size=batch_size, num_workers=2, shuffle=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_data, batch_size=batch_size, num_workers=2, shuffle=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_data, batch_size=batch_size, shuffle=True, num_workers=2\n",
    "    )\n",
    "\n",
    "    logger = CSVLogger(\n",
    "        save_dir=\"cks/logs/finetune\", name=\"icbhi\", \n",
    "        version=\"_\".join([head, pretrain, str(batch_size), str(lr), str(epochs), str(l2_strength)])\n",
    "    )\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"valid_auc\", mode=\"max\", dirpath=\"cks/finetune/icbhi/\", \n",
    "        filename=\"_\".join([\"finetuning\", head, pretrain, str(batch_size), str(lr), str(epochs), str(l2_strength)]) + \"-{epoch:02d}-{valid_auc:.2f}\",\n",
    "        every_n_epochs=3,\n",
    "    )\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=epochs,\n",
    "        accelerator=\"gpu\",\n",
    "        devices=1,\n",
    "        # logger=logger,\n",
    "        logger=False,\n",
    "        callbacks=[DecayLearningRate(), checkpoint_callback],\n",
    "        gradient_clip_val=1.0,\n",
    "        log_every_n_steps=1,\n",
    "        enable_progress_bar=False\n",
    "    )\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    trainer.test(dataloaders=train_loader)\n",
    "    trainer.test(dataloaders=val_loader)\n",
    "    test_res = trainer.test(dataloaders=test_loader)\n",
    "    auc = test_res[0][\"test_auc\"]\n",
    "    print(\"finished training dataset icbhi disease from model pretrained on\", pretrain,  \"with l2_strength\", l2_strength, \"lr\", lr, \"head\", head)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89be6739-1bf3-4503-bdb4-bf6e6afd30dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d0a577-5681-4107-9e25-fa2c75b80091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "966a49df-8ac1-450c-be3d-54cdd8c459b5",
   "metadata": {},
   "source": [
    "### with Coswara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fbb8bfd2-5fd6-4ab5-9332-1647e72a2e99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************\n",
      "training dataset ICBHI disease from model pretrained on operaCT with l2_strength 0.0001 lr 0.0001 head linear\n",
      "loading weights from cks/model/combined/icbhi_icbhicycle_hf_lung_Coswara_kauh_PulmonarySound_SPRSound/encoder-test_experiment_transformer-epoch=49--valid_acc=0.83-valid_loss=0.7015.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768 2\n",
      "Counter({'COPD': 793, 'Pneumonia': 37, 'Healthy': 35, 'URTI': 23, 'Bronchiectasis': 16, 'Bronchiolitis': 13, 'LRTI': 2, 'Asthma': 1})\n",
      "(828, 256, 64) 828 828\n",
      "Counter({1: 355, 0: 14})\n",
      "Counter({1: 89, 0: 4})\n",
      "Counter({1: 349, 0: 17})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name | Type             | Params\n",
      "------------------------------------------\n",
      "0 | net  | EncoderHTSAT     | 31.3 M\n",
      "1 | head | Sequential       | 1.5 K \n",
      "2 | loss | CrossEntropyLoss | 0     \n",
      "------------------------------------------\n",
      "30.2 M    Trainable params\n",
      "1.1 M     Non-trainable params\n",
      "31.3 M    Total params\n",
      "125.307   Total estimated model params size (MB)\n",
      "/opt/conda/envs/audio/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "`Trainer.fit` stopped: `max_epochs=64` reached.\n",
      "/opt/conda/envs/audio/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:145: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "Restoring states from the checkpoint path at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=11-valid_auc=1.00.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n",
      "Loaded model weights from the checkpoint at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=11-valid_auc=1.00.ckpt\n",
      "/opt/conda/envs/audio/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:492: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_auc tensor(1.)\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc                    1.0\n",
      "        test_auc                    1.0\n",
      "        test_loss         0.00025190235464833677\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=11-valid_auc=1.00.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n",
      "Loaded model weights from the checkpoint at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=11-valid_auc=1.00.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_auc tensor(1.)\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc                    1.0\n",
      "        test_auc                    1.0\n",
      "        test_loss           0.01184949092566967\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=11-valid_auc=1.00.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n",
      "Loaded model weights from the checkpoint at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=11-valid_auc=1.00.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_auc tensor(0.9493)\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.9644808743169399\n",
      "        test_auc             0.949266791343689\n",
      "        test_loss           0.18203821778297424\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training dataset icbhi disease from model pretrained on operaCT with l2_strength 0.0001 lr 0.0001 head linear\n",
      "************************************************\n",
      "training dataset ICBHI disease from model pretrained on operaCT with l2_strength 0.0001 lr 0.0001 head linear\n",
      "loading weights from cks/model/combined/icbhi_icbhicycle_hf_lung_Coswara_kauh_PulmonarySound_SPRSound/encoder-test_experiment_transformer-epoch=49--valid_acc=0.83-valid_loss=0.7015.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/audio/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:652: Checkpoint directory /workspace/cks/finetune/icbhi exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768 2\n",
      "Counter({'COPD': 793, 'Pneumonia': 37, 'Healthy': 35, 'URTI': 23, 'Bronchiectasis': 16, 'Bronchiolitis': 13, 'LRTI': 2, 'Asthma': 1})\n",
      "(828, 256, 64) 828 828\n",
      "Counter({1: 355, 0: 14})\n",
      "Counter({1: 89, 0: 4})\n",
      "Counter({1: 349, 0: 17})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name | Type             | Params\n",
      "------------------------------------------\n",
      "0 | net  | EncoderHTSAT     | 31.3 M\n",
      "1 | head | Sequential       | 1.5 K \n",
      "2 | loss | CrossEntropyLoss | 0     \n",
      "------------------------------------------\n",
      "30.2 M    Trainable params\n",
      "1.1 M     Non-trainable params\n",
      "31.3 M    Total params\n",
      "125.307   Total estimated model params size (MB)\n",
      "/opt/conda/envs/audio/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "`Trainer.fit` stopped: `max_epochs=64` reached.\n",
      "/opt/conda/envs/audio/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:145: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "Restoring states from the checkpoint path at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=14-valid_auc=1.00.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n",
      "Loaded model weights from the checkpoint at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=14-valid_auc=1.00.ckpt\n",
      "/opt/conda/envs/audio/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:492: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_auc tensor(1.)\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc                    1.0\n",
      "        test_auc                    1.0\n",
      "        test_loss         0.00018816653755493462\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=14-valid_auc=1.00.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n",
      "Loaded model weights from the checkpoint at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=14-valid_auc=1.00.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_auc tensor(1.)\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.9892473118279571\n",
      "        test_auc                    1.0\n",
      "        test_loss          0.055642370134592056\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=14-valid_auc=1.00.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n",
      "Loaded model weights from the checkpoint at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=14-valid_auc=1.00.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_auc tensor(0.9575)\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.9644808743169399\n",
      "        test_auc            0.9575256705284119\n",
      "        test_loss           0.19894832372665405\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training dataset icbhi disease from model pretrained on operaCT with l2_strength 0.0001 lr 0.0001 head linear\n",
      "************************************************\n",
      "training dataset ICBHI disease from model pretrained on operaCT with l2_strength 0.0001 lr 0.0001 head linear\n",
      "loading weights from cks/model/combined/icbhi_icbhicycle_hf_lung_Coswara_kauh_PulmonarySound_SPRSound/encoder-test_experiment_transformer-epoch=49--valid_acc=0.83-valid_loss=0.7015.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/audio/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:652: Checkpoint directory /workspace/cks/finetune/icbhi exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768 2\n",
      "Counter({'COPD': 793, 'Pneumonia': 37, 'Healthy': 35, 'URTI': 23, 'Bronchiectasis': 16, 'Bronchiolitis': 13, 'LRTI': 2, 'Asthma': 1})\n",
      "(828, 256, 64) 828 828\n",
      "Counter({1: 355, 0: 14})\n",
      "Counter({1: 89, 0: 4})\n",
      "Counter({1: 349, 0: 17})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name | Type             | Params\n",
      "------------------------------------------\n",
      "0 | net  | EncoderHTSAT     | 31.3 M\n",
      "1 | head | Sequential       | 1.5 K \n",
      "2 | loss | CrossEntropyLoss | 0     \n",
      "------------------------------------------\n",
      "30.2 M    Trainable params\n",
      "1.1 M     Non-trainable params\n",
      "31.3 M    Total params\n",
      "125.307   Total estimated model params size (MB)\n",
      "/opt/conda/envs/audio/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "`Trainer.fit` stopped: `max_epochs=64` reached.\n",
      "/opt/conda/envs/audio/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:145: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "Restoring states from the checkpoint path at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=05-valid_auc=1.00.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n",
      "Loaded model weights from the checkpoint at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=05-valid_auc=1.00.ckpt\n",
      "/opt/conda/envs/audio/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:492: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_auc tensor(0.9996)\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc             0.994579945799458\n",
      "        test_auc            0.9995976090431213\n",
      "        test_loss          0.023775946348905563\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=05-valid_auc=1.00.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n",
      "Loaded model weights from the checkpoint at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=05-valid_auc=1.00.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_auc tensor(1.)\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.9784946236559141\n",
      "        test_auc                    1.0\n",
      "        test_loss          0.043009158223867416\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=05-valid_auc=1.00.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n",
      "Loaded model weights from the checkpoint at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=05-valid_auc=1.00.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_auc tensor(0.9449)\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc             0.953551912568306\n",
      "        test_auc            0.9448845386505127\n",
      "        test_loss           0.19640712440013885\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training dataset icbhi disease from model pretrained on operaCT with l2_strength 0.0001 lr 0.0001 head linear\n",
      "************************************************\n",
      "training dataset ICBHI disease from model pretrained on operaCT with l2_strength 0.0001 lr 0.0001 head linear\n",
      "loading weights from cks/model/combined/icbhi_icbhicycle_hf_lung_Coswara_kauh_PulmonarySound_SPRSound/encoder-test_experiment_transformer-epoch=49--valid_acc=0.83-valid_loss=0.7015.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/audio/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:652: Checkpoint directory /workspace/cks/finetune/icbhi exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768 2\n",
      "Counter({'COPD': 793, 'Pneumonia': 37, 'Healthy': 35, 'URTI': 23, 'Bronchiectasis': 16, 'Bronchiolitis': 13, 'LRTI': 2, 'Asthma': 1})\n",
      "(828, 256, 64) 828 828\n",
      "Counter({1: 355, 0: 14})\n",
      "Counter({1: 89, 0: 4})\n",
      "Counter({1: 349, 0: 17})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name | Type             | Params\n",
      "------------------------------------------\n",
      "0 | net  | EncoderHTSAT     | 31.3 M\n",
      "1 | head | Sequential       | 1.5 K \n",
      "2 | loss | CrossEntropyLoss | 0     \n",
      "------------------------------------------\n",
      "30.2 M    Trainable params\n",
      "1.1 M     Non-trainable params\n",
      "31.3 M    Total params\n",
      "125.307   Total estimated model params size (MB)\n",
      "/opt/conda/envs/audio/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "`Trainer.fit` stopped: `max_epochs=64` reached.\n",
      "/opt/conda/envs/audio/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:145: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "Restoring states from the checkpoint path at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=08-valid_auc=1.00.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n",
      "Loaded model weights from the checkpoint at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=08-valid_auc=1.00.ckpt\n",
      "/opt/conda/envs/audio/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:492: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_auc tensor(1.)\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc                    1.0\n",
      "        test_auc                    1.0\n",
      "        test_loss          0.0004672133072745055\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=08-valid_auc=1.00.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n",
      "Loaded model weights from the checkpoint at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=08-valid_auc=1.00.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_auc tensor(1.)\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.9784946236559141\n",
      "        test_auc                    1.0\n",
      "        test_loss           0.03342290595173836\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=08-valid_auc=1.00.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n",
      "Loaded model weights from the checkpoint at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=08-valid_auc=1.00.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_auc tensor(0.9626)\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.9699453551912568\n",
      "        test_auc            0.9625822305679321\n",
      "        test_loss           0.13506416976451874\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training dataset icbhi disease from model pretrained on operaCT with l2_strength 0.0001 lr 0.0001 head linear\n",
      "************************************************\n",
      "training dataset ICBHI disease from model pretrained on operaCT with l2_strength 0.0001 lr 0.0001 head linear\n",
      "loading weights from cks/model/combined/icbhi_icbhicycle_hf_lung_Coswara_kauh_PulmonarySound_SPRSound/encoder-test_experiment_transformer-epoch=49--valid_acc=0.83-valid_loss=0.7015.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/audio/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:652: Checkpoint directory /workspace/cks/finetune/icbhi exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768 2\n",
      "Counter({'COPD': 793, 'Pneumonia': 37, 'Healthy': 35, 'URTI': 23, 'Bronchiectasis': 16, 'Bronchiolitis': 13, 'LRTI': 2, 'Asthma': 1})\n",
      "(828, 256, 64) 828 828\n",
      "Counter({1: 355, 0: 14})\n",
      "Counter({1: 89, 0: 4})\n",
      "Counter({1: 349, 0: 17})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name | Type             | Params\n",
      "------------------------------------------\n",
      "0 | net  | EncoderHTSAT     | 31.3 M\n",
      "1 | head | Sequential       | 1.5 K \n",
      "2 | loss | CrossEntropyLoss | 0     \n",
      "------------------------------------------\n",
      "30.2 M    Trainable params\n",
      "1.1 M     Non-trainable params\n",
      "31.3 M    Total params\n",
      "125.307   Total estimated model params size (MB)\n",
      "/opt/conda/envs/audio/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "`Trainer.fit` stopped: `max_epochs=64` reached.\n",
      "/opt/conda/envs/audio/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:145: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "Restoring states from the checkpoint path at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=11-valid_auc=1.00-v1.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n",
      "Loaded model weights from the checkpoint at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=11-valid_auc=1.00-v1.ckpt\n",
      "/opt/conda/envs/audio/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:492: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_auc tensor(1.)\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc                    1.0\n",
      "        test_auc                    1.0\n",
      "        test_loss          0.0002993276866618544\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=11-valid_auc=1.00-v1.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n",
      "Loaded model weights from the checkpoint at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=11-valid_auc=1.00-v1.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_auc tensor(1.)\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.9892473118279571\n",
      "        test_auc                    1.0\n",
      "        test_loss          0.061083901673555374\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=11-valid_auc=1.00-v1.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n",
      "Loaded model weights from the checkpoint at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=11-valid_auc=1.00-v1.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_auc tensor(0.9553)\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.9617486338797814\n",
      "        test_auc            0.9553345441818237\n",
      "        test_loss           0.17229753732681274\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "finished training dataset icbhi disease from model pretrained on operaCT with l2_strength 0.0001 lr 0.0001 head linear\n",
      "================================================\n",
      "[0.949266791343689, 0.9575256705284119, 0.9448845386505127, 0.9625822305679321, 0.9553345441818237]\n",
      "Five times mean task icbhidisease finetuning from operaCT results: auc mean 0.954 ± 0.006\n",
      "================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "task = \"icbhidisease\"\n",
    "pretrain = \"operaCT\"\n",
    "gridsearch = False\n",
    "lr = 1e-4\n",
    "l2_strength = 1e-5\n",
    "head = 'linear'\n",
    "modality = 'cough'\n",
    "mapgoogle = False\n",
    "dim =1280\n",
    "n_run = 5\n",
    "label = 'smoker'\n",
    "LOOCV = False\n",
    "avgprob = False\n",
    "\n",
    "if not LOOCV:\n",
    "    auc_scores = []\n",
    "    for seed in range(n_run):\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        seed_everything(seed, workers=True)\n",
    "        if task == \"icbhidisease\":\n",
    "            auc = finetune_icbhidisease(pretrain= pretrain, epochs=64, l2_strength=1e-4, feat_dim = 768)\n",
    "        auc_scores.append(auc)\n",
    "    print(\"=\" * 48)\n",
    "    print(auc_scores)\n",
    "    print(\"Five times mean task {} finetuning from {} results: auc mean {:.3f} ± {:.3f}\".format(task, pretrain, np.mean(auc_scores), np.std(auc_scores)) )\n",
    "    print(\"=\" * 48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a6c311-52ef-4768-847e-e8a89abc8061",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder-experiment_wo Coswara_kauh_Pul_20_SPR_20-epoch=129--valid_acc=0.77-valid_loss=0.8953"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a6a554ac-815c-4a9e-a06e-08e3301ee96a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = torch.load(\"cks/model/combined/icbhi_icbhicycle_hf_lung_Coswara_kauh_PulmonarySound_SPRSound/encoder-test_experiment_transformer-epoch=49--valid_acc=0.83-valid_loss=0.7015.ckpt\")\n",
    "pretrained_model = initialize_pretrained_model(pretrain)\n",
    "pretrained_model.load_state_dict(ckpt[\"state_dict\"], strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5c081c62-1226-48b3-95bd-89d6e0d8bb60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cola(\n",
      "  (do): Dropout(p=0.1, inplace=False)\n",
      "  (encoder): EncoderHTSAT(\n",
      "    (encoder): HTSATWrapper(\n",
      "      (htsat): HTSAT_Swin_Transformer(\n",
      "        (spectrogram_extractor): Spectrogram(\n",
      "          (stft): STFT(\n",
      "            (conv_real): Conv1d(1, 513, kernel_size=(1024,), stride=(320,), bias=False)\n",
      "            (conv_imag): Conv1d(1, 513, kernel_size=(1024,), stride=(320,), bias=False)\n",
      "          )\n",
      "        )\n",
      "        (logmel_extractor): LogmelFilterBank()\n",
      "        (spec_augmenter): SpecAugmentation(\n",
      "          (time_dropper): DropStripes()\n",
      "          (freq_dropper): DropStripes()\n",
      "        )\n",
      "        (bn0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (patch_embed): PatchEmbed(\n",
      "          (proj): Conv2d(1, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      "          (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "        (layers): ModuleList(\n",
      "          (0): BasicLayer(\n",
      "            dim=96, input_resolution=(64, 64), depth=2\n",
      "            (blocks): ModuleList(\n",
      "              (0): SwinTransformerBlock(\n",
      "                dim=96, input_resolution=(64, 64), num_heads=4, window_size=8, shift_size=0, mlp_ratio=4.0\n",
      "                (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=96, window_size=(8, 8), num_heads=4\n",
      "                  (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): SwinTransformerBlock(\n",
      "                dim=96, input_resolution=(64, 64), num_heads=4, window_size=8, shift_size=4, mlp_ratio=4.0\n",
      "                (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=96, window_size=(8, 8), num_heads=4\n",
      "                  (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath()\n",
      "                (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (downsample): PatchMerging(\n",
      "              input_resolution=(64, 64), dim=96\n",
      "              (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
      "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (1): BasicLayer(\n",
      "            dim=192, input_resolution=(32, 32), depth=2\n",
      "            (blocks): ModuleList(\n",
      "              (0): SwinTransformerBlock(\n",
      "                dim=192, input_resolution=(32, 32), num_heads=8, window_size=8, shift_size=0, mlp_ratio=4.0\n",
      "                (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=192, window_size=(8, 8), num_heads=8\n",
      "                  (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath()\n",
      "                (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): SwinTransformerBlock(\n",
      "                dim=192, input_resolution=(32, 32), num_heads=8, window_size=8, shift_size=4, mlp_ratio=4.0\n",
      "                (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=192, window_size=(8, 8), num_heads=8\n",
      "                  (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath()\n",
      "                (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (downsample): PatchMerging(\n",
      "              input_resolution=(32, 32), dim=192\n",
      "              (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
      "              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (2): BasicLayer(\n",
      "            dim=384, input_resolution=(16, 16), depth=6\n",
      "            (blocks): ModuleList(\n",
      "              (0): SwinTransformerBlock(\n",
      "                dim=384, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=0, mlp_ratio=4.0\n",
      "                (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=384, window_size=(8, 8), num_heads=16\n",
      "                  (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath()\n",
      "                (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): SwinTransformerBlock(\n",
      "                dim=384, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=4, mlp_ratio=4.0\n",
      "                (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=384, window_size=(8, 8), num_heads=16\n",
      "                  (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath()\n",
      "                (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (2): SwinTransformerBlock(\n",
      "                dim=384, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=0, mlp_ratio=4.0\n",
      "                (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=384, window_size=(8, 8), num_heads=16\n",
      "                  (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath()\n",
      "                (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (3): SwinTransformerBlock(\n",
      "                dim=384, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=4, mlp_ratio=4.0\n",
      "                (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=384, window_size=(8, 8), num_heads=16\n",
      "                  (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath()\n",
      "                (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (4): SwinTransformerBlock(\n",
      "                dim=384, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=0, mlp_ratio=4.0\n",
      "                (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=384, window_size=(8, 8), num_heads=16\n",
      "                  (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath()\n",
      "                (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (5): SwinTransformerBlock(\n",
      "                dim=384, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=4, mlp_ratio=4.0\n",
      "                (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=384, window_size=(8, 8), num_heads=16\n",
      "                  (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath()\n",
      "                (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (downsample): PatchMerging(\n",
      "              input_resolution=(16, 16), dim=384\n",
      "              (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
      "              (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (3): BasicLayer(\n",
      "            dim=768, input_resolution=(8, 8), depth=2\n",
      "            (blocks): ModuleList(\n",
      "              (0-1): 2 x SwinTransformerBlock(\n",
      "                dim=768, input_resolution=(8, 8), num_heads=32, window_size=8, shift_size=0, mlp_ratio=4.0\n",
      "                (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=768, window_size=(8, 8), num_heads=32\n",
      "                  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath()\n",
      "                (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
      "        (maxpool): AdaptiveMaxPool1d(output_size=1)\n",
      "        (tscam_conv): Conv2d(768, 527, kernel_size=(2, 3), stride=(1, 1), padding=(0, 1))\n",
      "        (head): Linear(in_features=527, out_features=527, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (g): Linear(in_features=768, out_features=512, bias=True)\n",
      "  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528b211b-6dfe-4b8f-96ef-661dc0e95b55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0b1a8d0-2879-4325-bdde-72f8656c87cc",
   "metadata": {},
   "source": [
    "### without Coswara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "817ebbb6-1ecb-45e2-a1f1-c1712ffa6d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************\n",
      "training dataset ICBHI disease from model pretrained on operaCT with l2_strength 0.0001 lr 0.0001 head linear\n",
      "loading weights from cks/model/combined/icbhi_icbhicycle_hf_lung_kauh_PulmonarySound_SPRSound/encoder-experiment_wo Coswara_kauh_Pul_20_SPR_20-epoch=129--valid_acc=0.77-valid_loss=0.8953.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/audio/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:652: Checkpoint directory /workspace/cks/finetune/icbhi exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n",
      "\n",
      "  | Name | Type             | Params\n",
      "------------------------------------------\n",
      "0 | net  | EncoderHTSAT     | 31.3 M\n",
      "1 | head | Sequential       | 1.5 K \n",
      "2 | loss | CrossEntropyLoss | 0     \n",
      "------------------------------------------\n",
      "30.2 M    Trainable params\n",
      "1.1 M     Non-trainable params\n",
      "31.3 M    Total params\n",
      "125.307   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768 2\n",
      "Counter({'COPD': 793, 'Pneumonia': 37, 'Healthy': 35, 'URTI': 23, 'Bronchiectasis': 16, 'Bronchiolitis': 13, 'LRTI': 2, 'Asthma': 1})\n",
      "(828, 256, 64) 828 828\n",
      "Counter({1: 355, 0: 14})\n",
      "Counter({1: 89, 0: 4})\n",
      "Counter({1: 349, 0: 17})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/audio/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "`Trainer.fit` stopped: `max_epochs=64` reached.\n",
      "/opt/conda/envs/audio/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:145: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "Restoring states from the checkpoint path at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=05-valid_auc=1.00-v6.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n",
      "Loaded model weights from the checkpoint at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=05-valid_auc=1.00-v6.ckpt\n",
      "/opt/conda/envs/audio/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:492: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_auc tensor(1.)\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc                    1.0\n",
      "        test_auc                    1.0\n",
      "        test_loss          0.0007978558423928916\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=05-valid_auc=1.00-v6.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n",
      "Loaded model weights from the checkpoint at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=05-valid_auc=1.00-v6.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_auc tensor(1.)\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.9892473118279571\n",
      "        test_auc                    1.0\n",
      "        test_loss           0.01798976957798004\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=05-valid_auc=1.00-v6.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n",
      "Loaded model weights from the checkpoint at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=05-valid_auc=1.00-v6.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_auc tensor(0.9831)\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.9726775956284153\n",
      "        test_auc            0.9831451177597046\n",
      "        test_loss           0.1572171300649643\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "finished training dataset icbhi disease from model pretrained on operaCT with l2_strength 0.0001 lr 0.0001 head linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************\n",
      "training dataset ICBHI disease from model pretrained on operaCT with l2_strength 0.0001 lr 0.0001 head linear\n",
      "loading weights from cks/model/combined/icbhi_icbhicycle_hf_lung_kauh_PulmonarySound_SPRSound/encoder-experiment_wo Coswara_kauh_Pul_20_SPR_20-epoch=129--valid_acc=0.77-valid_loss=0.8953.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/audio/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:652: Checkpoint directory /workspace/cks/finetune/icbhi exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n",
      "\n",
      "  | Name | Type             | Params\n",
      "------------------------------------------\n",
      "0 | net  | EncoderHTSAT     | 31.3 M\n",
      "1 | head | Sequential       | 1.5 K \n",
      "2 | loss | CrossEntropyLoss | 0     \n",
      "------------------------------------------\n",
      "30.2 M    Trainable params\n",
      "1.1 M     Non-trainable params\n",
      "31.3 M    Total params\n",
      "125.307   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768 2\n",
      "Counter({'COPD': 793, 'Pneumonia': 37, 'Healthy': 35, 'URTI': 23, 'Bronchiectasis': 16, 'Bronchiolitis': 13, 'LRTI': 2, 'Asthma': 1})\n",
      "(828, 256, 64) 828 828\n",
      "Counter({1: 355, 0: 14})\n",
      "Counter({1: 89, 0: 4})\n",
      "Counter({1: 349, 0: 17})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/audio/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "`Trainer.fit` stopped: `max_epochs=64` reached.\n",
      "/opt/conda/envs/audio/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:145: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "Restoring states from the checkpoint path at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=02-valid_auc=1.00-v7.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n",
      "Loaded model weights from the checkpoint at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=02-valid_auc=1.00-v7.ckpt\n",
      "/opt/conda/envs/audio/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:492: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_auc tensor(0.9920)\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc             0.983739837398374\n",
      "        test_auc            0.9919517040252686\n",
      "        test_loss           0.04907897487282753\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=02-valid_auc=1.00-v7.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n",
      "Loaded model weights from the checkpoint at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=02-valid_auc=1.00-v7.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_auc tensor(1.)\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.9784946236559141\n",
      "        test_auc                    1.0\n",
      "        test_loss           0.04282383620738983\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=02-valid_auc=1.00-v7.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n",
      "Loaded model weights from the checkpoint at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=02-valid_auc=1.00-v7.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_auc tensor(0.9572)\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.9562841530054644\n",
      "        test_auc             0.957188606262207\n",
      "        test_loss           0.1632528454065323\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "finished training dataset icbhi disease from model pretrained on operaCT with l2_strength 0.0001 lr 0.0001 head linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************\n",
      "training dataset ICBHI disease from model pretrained on operaCT with l2_strength 0.0001 lr 0.0001 head linear\n",
      "loading weights from cks/model/combined/icbhi_icbhicycle_hf_lung_kauh_PulmonarySound_SPRSound/encoder-experiment_wo Coswara_kauh_Pul_20_SPR_20-epoch=129--valid_acc=0.77-valid_loss=0.8953.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/audio/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:652: Checkpoint directory /workspace/cks/finetune/icbhi exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n",
      "\n",
      "  | Name | Type             | Params\n",
      "------------------------------------------\n",
      "0 | net  | EncoderHTSAT     | 31.3 M\n",
      "1 | head | Sequential       | 1.5 K \n",
      "2 | loss | CrossEntropyLoss | 0     \n",
      "------------------------------------------\n",
      "30.2 M    Trainable params\n",
      "1.1 M     Non-trainable params\n",
      "31.3 M    Total params\n",
      "125.307   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768 2\n",
      "Counter({'COPD': 793, 'Pneumonia': 37, 'Healthy': 35, 'URTI': 23, 'Bronchiectasis': 16, 'Bronchiolitis': 13, 'LRTI': 2, 'Asthma': 1})\n",
      "(828, 256, 64) 828 828\n",
      "Counter({1: 355, 0: 14})\n",
      "Counter({1: 89, 0: 4})\n",
      "Counter({1: 349, 0: 17})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/audio/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "`Trainer.fit` stopped: `max_epochs=64` reached.\n",
      "/opt/conda/envs/audio/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:145: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "Restoring states from the checkpoint path at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=02-valid_auc=1.00-v8.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n",
      "Loaded model weights from the checkpoint at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=02-valid_auc=1.00-v8.ckpt\n",
      "/opt/conda/envs/audio/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:492: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_auc tensor(0.9976)\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc             0.978319783197832\n",
      "        test_auc            0.9975855350494385\n",
      "        test_loss          0.042598746716976166\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=02-valid_auc=1.00-v8.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n",
      "Loaded model weights from the checkpoint at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=02-valid_auc=1.00-v8.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_auc tensor(1.)\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc             0.967741935483871\n",
      "        test_auc                    1.0\n",
      "        test_loss           0.05869511142373085\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=02-valid_auc=1.00-v8.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n",
      "Loaded model weights from the checkpoint at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=02-valid_auc=1.00-v8.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_auc tensor(0.9826)\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.9590163934426229\n",
      "        test_auc             0.982639491558075\n",
      "        test_loss           0.13415822386741638\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training dataset icbhi disease from model pretrained on operaCT with l2_strength 0.0001 lr 0.0001 head linear\n",
      "************************************************\n",
      "training dataset ICBHI disease from model pretrained on operaCT with l2_strength 0.0001 lr 0.0001 head linear\n",
      "loading weights from cks/model/combined/icbhi_icbhicycle_hf_lung_kauh_PulmonarySound_SPRSound/encoder-experiment_wo Coswara_kauh_Pul_20_SPR_20-epoch=129--valid_acc=0.77-valid_loss=0.8953.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/audio/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:652: Checkpoint directory /workspace/cks/finetune/icbhi exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n",
      "\n",
      "  | Name | Type             | Params\n",
      "------------------------------------------\n",
      "0 | net  | EncoderHTSAT     | 31.3 M\n",
      "1 | head | Sequential       | 1.5 K \n",
      "2 | loss | CrossEntropyLoss | 0     \n",
      "------------------------------------------\n",
      "30.2 M    Trainable params\n",
      "1.1 M     Non-trainable params\n",
      "31.3 M    Total params\n",
      "125.307   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768 2\n",
      "Counter({'COPD': 793, 'Pneumonia': 37, 'Healthy': 35, 'URTI': 23, 'Bronchiectasis': 16, 'Bronchiolitis': 13, 'LRTI': 2, 'Asthma': 1})\n",
      "(828, 256, 64) 828 828\n",
      "Counter({1: 355, 0: 14})\n",
      "Counter({1: 89, 0: 4})\n",
      "Counter({1: 349, 0: 17})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/audio/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "`Trainer.fit` stopped: `max_epochs=64` reached.\n",
      "/opt/conda/envs/audio/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:145: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "Restoring states from the checkpoint path at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=02-valid_auc=1.00-v9.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n",
      "Loaded model weights from the checkpoint at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=02-valid_auc=1.00-v9.ckpt\n",
      "/opt/conda/envs/audio/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:492: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_auc tensor(0.9958)\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc             0.986449864498645\n",
      "        test_auc            0.9957746267318726\n",
      "        test_loss           0.03553667664527893\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=02-valid_auc=1.00-v9.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n",
      "Loaded model weights from the checkpoint at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=02-valid_auc=1.00-v9.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_auc tensor(1.)\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.9784946236559141\n",
      "        test_auc                    1.0\n",
      "        test_loss          0.039514366537332535\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=02-valid_auc=1.00-v9.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n",
      "Loaded model weights from the checkpoint at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=02-valid_auc=1.00-v9.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_auc tensor(0.9565)\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.9617486338797814\n",
      "        test_auc            0.9565144777297974\n",
      "        test_loss           0.11047162115573883\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training dataset icbhi disease from model pretrained on operaCT with l2_strength 0.0001 lr 0.0001 head linear\n",
      "************************************************\n",
      "training dataset ICBHI disease from model pretrained on operaCT with l2_strength 0.0001 lr 0.0001 head linear\n",
      "loading weights from cks/model/combined/icbhi_icbhicycle_hf_lung_kauh_PulmonarySound_SPRSound/encoder-experiment_wo Coswara_kauh_Pul_20_SPR_20-epoch=129--valid_acc=0.77-valid_loss=0.8953.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/audio/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:652: Checkpoint directory /workspace/cks/finetune/icbhi exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n",
      "\n",
      "  | Name | Type             | Params\n",
      "------------------------------------------\n",
      "0 | net  | EncoderHTSAT     | 31.3 M\n",
      "1 | head | Sequential       | 1.5 K \n",
      "2 | loss | CrossEntropyLoss | 0     \n",
      "------------------------------------------\n",
      "30.2 M    Trainable params\n",
      "1.1 M     Non-trainable params\n",
      "31.3 M    Total params\n",
      "125.307   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768 2\n",
      "Counter({'COPD': 793, 'Pneumonia': 37, 'Healthy': 35, 'URTI': 23, 'Bronchiectasis': 16, 'Bronchiolitis': 13, 'LRTI': 2, 'Asthma': 1})\n",
      "(828, 256, 64) 828 828\n",
      "Counter({1: 355, 0: 14})\n",
      "Counter({1: 89, 0: 4})\n",
      "Counter({1: 349, 0: 17})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/audio/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "`Trainer.fit` stopped: `max_epochs=64` reached.\n",
      "/opt/conda/envs/audio/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:145: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "Restoring states from the checkpoint path at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=02-valid_auc=1.00-v10.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n",
      "Loaded model weights from the checkpoint at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=02-valid_auc=1.00-v10.ckpt\n",
      "/opt/conda/envs/audio/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:492: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_auc tensor(0.9960)\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc             0.986449864498645\n",
      "        test_auc            0.9959758520126343\n",
      "        test_loss           0.03364144638180733\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=02-valid_auc=1.00-v10.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n",
      "Loaded model weights from the checkpoint at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=02-valid_auc=1.00-v10.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_auc tensor(1.)\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.9784946236559141\n",
      "        test_auc                    1.0\n",
      "        test_loss           0.04362513870000839\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=02-valid_auc=1.00-v10.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n",
      "Loaded model weights from the checkpoint at /workspace/cks/finetune/icbhi/finetuning_linear_operaCT_64_0.0001_64_0.0001-epoch=02-valid_auc=1.00-v10.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_auc tensor(0.9675)\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.9590163934426229\n",
      "        test_auc            0.9674700498580933\n",
      "        test_loss           0.12098512053489685\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "finished training dataset icbhi disease from model pretrained on operaCT with l2_strength 0.0001 lr 0.0001 head linear\n",
      "================================================\n",
      "[0.9831451177597046, 0.957188606262207, 0.982639491558075, 0.9565144777297974, 0.9674700498580933]\n",
      "Five times mean task icbhidisease finetuning from operaCT results: auc mean 0.969 ± 0.012\n",
      "================================================\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "task = \"icbhidisease\"\n",
    "pretrain = \"operaCT\"\n",
    "gridsearch = False\n",
    "lr = 1e-4\n",
    "l2_strength = 1e-5\n",
    "head = 'linear'\n",
    "modality = 'cough'\n",
    "mapgoogle = False\n",
    "dim =1280\n",
    "n_run = 5\n",
    "label = 'smoker'\n",
    "LOOCV = False\n",
    "avgprob = False\n",
    "\n",
    "if not LOOCV:\n",
    "    auc_scores = []\n",
    "    for seed in range(n_run):\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        seed_everything(seed, workers=True)\n",
    "        if task == \"icbhidisease\":\n",
    "            auc = finetune_icbhidisease(pretrain= pretrain, epochs=64, l2_strength=1e-4, feat_dim = 768)\n",
    "        auc_scores.append(auc)\n",
    "    print(\"=\" * 48)\n",
    "    print(auc_scores)\n",
    "    print(\"Five times mean task {} finetuning from {} results: auc mean {:.3f} ± {:.3f}\".format(task, pretrain, np.mean(auc_scores), np.std(auc_scores)) )\n",
    "    print(\"=\" * 48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a91f4f-d574-4b7a-93b4-e1cccf5d24bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio",
   "language": "python",
   "name": "audio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
