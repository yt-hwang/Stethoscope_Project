# Github Analysis ν΄λ” μƒμ„Έ λ¶„μ„ λ³΄κ³ μ„

## π“‹ λ©μ°¨
1. [μ „μ²΄ ν”„λ΅μ νΈ κ°μ”](#1-μ „μ²΄-ν”„λ΅μ νΈ-κ°μ”)
2. [ν΄λ” κµ¬μ΅° λ¶„μ„](#2-ν΄λ”-κµ¬μ΅°-λ¶„μ„)
3. [Jupyter Notebook νμΌ λ¶„μ„](#3-jupyter-notebook-νμΌ-λ¶„μ„)
4. [λ°μ΄ν„° νμΌ λ¶„μ„](#4-λ°μ΄ν„°-νμΌ-λ¶„μ„)
5. [ν¨ν‚¤μ§€ λ° μμ΅΄μ„± λ¶„μ„](#5-ν¨ν‚¤μ§€-λ°-μμ΅΄μ„±-λ¶„μ„)
6. [λ¨λΈ μ•„ν‚¤ν…μ² λ¶„μ„](#6-λ¨λΈ-μ•„ν‚¤ν…μ²-λ¶„μ„)
7. [μ „μ²΄ μ‹μ¤ν… μ›ν¬ν”λ΅μ°](#7-μ „μ²΄-μ‹μ¤ν…-μ›ν¬ν”λ΅μ°)

---

## 1. μ „μ²΄ ν”„λ΅μ νΈ κ°μ”

### 1.1 ν”„λ΅μ νΈ λ©μ 
- **Self-Supervised Learning (SSL)** κΈ°λ° νΈν΅μ λ¶„μ„ ν”„λ΅μ νΈ
- **Transfer Learning**μ„ ν†µν• νΈν΅μ λ¶„λ¥ λ¨λΈ κ°λ°
- **Contrastive Learning** λ°©μ‹μΌλ΅ μ¤λ””μ¤ νΉμ§• ν•™μµ
- **OperaCT** μ‚¬μ „ ν›λ ¨ λ¨λΈμ„ ν™μ©ν• νΈν΅μ λ¶„λ¥

### 1.2 μ£Όμ” νΉμ§•
- **λ‹¤μ¤‘ λ°μ΄ν„°μ…‹ ν†µν•©**: ICBHI, HF_Lung, KAUH, PulmonarySound, SPRSound
- **λ°μ΄ν„° μ¦κ°•**: Random crop, random mask, random multiply
- **ν™μλ³„ λ¶„ν• **: Inter-patient λ°©μ‹μ train/val/test λ¶„ν• 
- **μ‹¤μ‹κ°„ μ¶”λ΅ **: 8μ΄ μ„Έκ·Έλ¨ΌνΈ λ‹¨μ„ μ²λ¦¬

### 1.3 Self-Supervised Learning (SSL) μƒμ„Έ μ„¤λ…

#### 1.3.1 SSLμ΄λ€?
**Self-Supervised Learning (μκΈ°μ§€λ„ν•™μµ)**μ€ λΌλ²¨μ΄ μ—†λ” λ°μ΄ν„°μ—μ„ μ¤μ¤λ΅ ν•™μµν•λ” λ°©λ²•μ…λ‹λ‹¤.

**μ „ν†µμ μΈ λ¨Έμ‹ λ¬λ‹ vs SSL:**
```
μ „ν†µμ μΈ λ¨Έμ‹ λ¬λ‹:
λ°μ΄ν„° + λΌλ²¨ β†’ λ¨λΈ ν•™μµ β†’ μμΈ΅

SSL:
λ°μ΄ν„°λ§ β†’ λ¨λΈμ΄ μ¤μ¤λ΅ ν¨ν„΄ ν•™μµ β†’ νΉμ§• μ¶”μ¶
```

**SSLμ ν•µμ‹¬ μ•„μ΄λ””μ–΄:**
- **"λ°μ΄ν„° μμ²΄κ°€ μ„ μƒλ‹"**: λΌλ²¨ μ—†μ΄λ„ λ°μ΄ν„°μ κµ¬μ΅°μ™€ ν¨ν„΄μ„ ν•™μµ
- **"μμΈ΅ κ°€λ¥ν• λ¶€λ¶„μ„ μ¨κΈ°κΈ°"**: λ°μ΄ν„°μ μΌλ¶€λ¥Ό μ¨κΈ°κ³  λ‚λ¨Έμ§€λ΅ μμΈ΅ν•λ„λ΅ ν•™μµ
- **"μΌλ°ν™”λ νΉμ§• ν•™μµ"**: νΉμ • νƒμ¤ν¬κ°€ μ•„λ‹ μΌλ°μ μΈ νΉμ§•μ„ ν•™μµ

#### 1.3.2 SSLμ μ¥μ 
1. **λ€κ·λ¨ λ°μ΄ν„° ν™μ©**: λΌλ²¨λ§ λΉ„μ© μ—†μ΄ λ°©λ€ν• λ°μ΄ν„° μ‚¬μ©
2. **μΌλ°ν™” μ„±λ¥**: λ‹¤μ–‘ν• νƒμ¤ν¬μ— μ μ© κ°€λ¥ν• νΉμ§• ν•™μµ
3. **λ„λ©”μΈ μ μ‘**: μƒλ΅μ΄ λ„λ©”μΈμ— λΉ λ¥΄κ² μ μ‘
4. **λΉ„μ© ν¨μ¨μ„±**: λΌλ²¨λ§ λΉ„μ© μ μ•½

#### 1.3.3 SSLμ μ‘λ™ μ›λ¦¬
```
1λ‹¨κ³„: λ°μ΄ν„° λ³€ν• (Data Augmentation)
   μ›λ³Έ μ¤λ””μ¤ β†’ λ³€ν•λ μ¤λ””μ¤1, μ¤λ””μ¤2

2λ‹¨κ³„: νΉμ§• μ¶”μ¶ (Feature Extraction)
   μ¤λ””μ¤1, μ¤λ””μ¤2 β†’ νΉμ§•1, νΉμ§•2

3λ‹¨κ³„: λ€μ΅° ν•™μµ (Contrastive Learning)
   κ°™μ€ μ¤λ””μ¤μ λ³€ν• β†’ κ°€κΉκ² (Positive)
   λ‹¤λ¥Έ μ¤λ””μ¤μ λ³€ν• β†’ λ©€κ² (Negative)
```

### 1.4 Contrastive Learning μƒμ„Έ μ„¤λ…

#### 1.4.1 Contrastive Learningμ΄λ€?
**Contrastive Learning (λ€μ΅° ν•™μµ)**μ€ "λΉ„μ·ν• κ²ƒμ€ κ°€κΉκ², λ‹¤λ¥Έ κ²ƒμ€ λ©€κ²" ν•™μµν•λ” λ°©λ²•μ…λ‹λ‹¤.

**ν•µμ‹¬ κ°λ…:**
- **Positive Pairs**: κ°™μ€ λ°μ΄ν„°μ μ„λ΅ λ‹¤λ¥Έ λ³€ν• (κ°€κΉκ² ν•™μµ)
- **Negative Pairs**: λ‹¤λ¥Έ λ°μ΄ν„°μ λ³€ν• (λ©€κ² ν•™μµ)
- **Representation Learning**: μλ―Έ μλ” νΉμ§• ν‘ν„ ν•™μµ

#### 1.4.2 Contrastive Learningμ μ‘λ™ μ›λ¦¬

**1) λ°μ΄ν„° λ³€ν• μƒμ„±**
```python
# κ°™μ€ μ¤λ””μ¤μ—μ„ μ„λ΅ λ‹¤λ¥Έ λ³€ν• μƒμ„±
original_audio = "νΈν΅μ.wav"
augmented_1 = random_crop(original_audio)  # λλ¤ ν¬λ΅­
augmented_2 = random_mask(original_audio)  # λλ¤ λ§μ¤ν‚Ή
```

**2) νΉμ§• μ¶”μ¶**
```python
# κ° λ³€ν•μ—μ„ νΉμ§• μ¶”μ¶
features_1 = encoder(augmented_1)  # [768μ°¨μ›]
features_2 = encoder(augmented_2)  # [768μ°¨μ›]
```

**3) λ€μ΅° μ†μ‹¤ κ³„μ‚°**
```python
# Positive pair: κ°™μ€ μ¤λ””μ¤μ λ³€ν•λ“¤
positive_loss = distance(features_1, features_2)  # μ‘κ² λ§λ“¤κΈ°

# Negative pair: λ‹¤λ¥Έ μ¤λ””μ¤μ λ³€ν•λ“¤
negative_loss = distance(features_1, other_features)  # ν¬κ² λ§λ“¤κΈ°

# μ „μ²΄ μ†μ‹¤
total_loss = positive_loss - negative_loss
```

#### 1.4.3 Contrastive Learningμ μ¥μ 
1. **μλ―Έ μλ” νΉμ§•**: λ°μ΄ν„°μ λ³Έμ§μ μΈ νΉμ„± ν•™μµ
2. **λΌλ²¨ λ¶ν•„μ”**: λΌλ²¨ μ—†μ΄λ„ ν•™μµ κ°€λ¥
3. **κ°•κ±΄μ„±**: λ…Έμ΄μ¦μ— κ°•ν• νΉμ§• ν•™μµ
4. **μΌλ°ν™”**: λ‹¤μ–‘ν• νƒμ¤ν¬μ— μ μ© κ°€λ¥

### 1.5 SSL + Contrastive Learningμ μ΅°ν•©

#### 1.5.1 μ™ μ΄ μ΅°ν•©μ΄ κ°•λ ¥ν•κ°€?
```
SSL (μκΈ°μ§€λ„ν•™μµ) + Contrastive Learning (λ€μ΅°ν•™μµ)
= λΌλ²¨ μ—†μ΄ μλ―Έ μλ” νΉμ§•μ„ ν•™μµν•λ” μµκ°• μ΅°ν•©
```

**κµ¬μ²΄μ μΈ κ³Όμ •:**
1. **λ°μ΄ν„° μμ§‘**: λΌλ²¨ μ—†λ” λ€λ‰μ νΈν΅μ λ°μ΄ν„°
2. **λ³€ν• μƒμ„±**: Random crop, mask, multiply λ“±
3. **λ€μ΅° ν•™μµ**: κ°™μ€ μ¤λ””μ¤λ” κ°€κΉκ², λ‹¤λ¥Έ μ¤λ””μ¤λ” λ©€κ²
4. **νΉμ§• ν•™μµ**: νΈν΅μμ λ³Έμ§μ μΈ νΉμ„± νμ•…
5. **μ „μ΄ ν•™μµ**: ν•™μµλ νΉμ§•μ„ λ¶„λ¥ νƒμ¤ν¬μ— ν™μ©

#### 1.5.2 μ‹¤μ  μ μ© μμ‹
```python
# 1. λ€λ‰μ νΈν΅μ λ°μ΄ν„° μμ§‘ (λΌλ²¨ μ—†μ)
respiratory_sounds = ["sound1.wav", "sound2.wav", ...]

# 2. κ° μ¤λ””μ¤μ—μ„ λ³€ν• μƒμ„±
for sound in respiratory_sounds:
    augmented_1 = random_crop(sound)
    augmented_2 = random_mask(sound)
    
    # 3. νΉμ§• μ¶”μ¶
    features_1 = encoder(augmented_1)
    features_2 = encoder(augmented_2)
    
    # 4. λ€μ΅° ν•™μµ
    # κ°™μ€ μ¤λ””μ¤μ λ³€ν•λ“¤ β†’ κ°€κΉκ²
    # λ‹¤λ¥Έ μ¤λ””μ¤μ λ³€ν•λ“¤ β†’ λ©€κ²
```

### 1.6 Transfer Learningκ³Όμ μ—°κ²°

#### 1.6.1 SSL β†’ Transfer Learning νλ¦„
```
1λ‹¨κ³„: SSLλ΅ μΌλ°μ μΈ νΉμ§• ν•™μµ
   λ€λ‰μ λΌλ²¨ μ—†λ” λ°μ΄ν„° β†’ μΌλ°μ μΈ μ¤λ””μ¤ νΉμ§•

2λ‹¨κ³„: Transfer LearningμΌλ΅ νΉμ • νƒμ¤ν¬ ν•™μµ
   ν•™μµλ νΉμ§• + μ†λ‰μ λΌλ²¨ λ°μ΄ν„° β†’ νΈν΅μ λ¶„λ¥
```

#### 1.6.2 μ™ μ΄ λ°©λ²•μ΄ ν¨κ³Όμ μΈκ°€?
1. **μΌλ°μ μΈ νΉμ§•**: SSLλ΅ ν•™μµν• νΉμ§•μ€ λ‹¤μ–‘ν• μ¤λ””μ¤μ— μ μ© κ°€λ¥
2. **λΉ λ¥Έ μ μ‘**: μƒλ΅μ΄ νƒμ¤ν¬μ— λΉ λ¥΄κ² μ μ‘
3. **μ μ€ λ°μ΄ν„°**: μ†λ‰μ λΌλ²¨ λ°μ΄ν„°λ΅λ„ μΆ‹μ€ μ„±λ¥
4. **λ„λ©”μΈ μ μ‘**: μλ£ μ¤λ””μ¤ λ„λ©”μΈμ— νΉν™”λ νΉμ§• ν•™μµ

---

## 2. ν΄λ” κµ¬μ΅° λ¶„μ„

```
Github Analysis/
β”β”€β”€ README.md                           # ν”„λ΅μ νΈ λ©”μΈ μ„¤λ…μ„
β”β”€β”€ pretrain.ipynb                      # SSL μ‚¬μ „ ν›λ ¨ λ…ΈνΈλ¶
β”β”€β”€ finetune with Yeo Data.ipynb        # νμΈνλ‹ λ…ΈνΈλ¶
β”β”€β”€ preprocessing_ssl.ipynb             # λ°μ΄ν„° μ „μ²λ¦¬ λ…ΈνΈλ¶
β”β”€β”€ fine tuning simulation.ipynb        # νμΈνλ‹ μ‹λ®¬λ μ΄μ…
β”β”€β”€ RNN experiment.ipynb                # RNN μ‹¤ν— λ…ΈνΈλ¶
β”β”€β”€ data/                               # λ°μ΄ν„° ν΄λ”
β”‚   β””β”€β”€ readme.md                       # λ°μ΄ν„° μ„¤λ…μ„
β”β”€β”€ feature/                            # μ¶”μ¶λ νΉμ§• λ°μ΄ν„°
β”‚   β”β”€β”€ icbhidisease_eval/              # ICBHI λ°μ΄ν„°μ…‹ νΉμ§•
β”‚   β”β”€β”€ yeo_eval/                       # Yeo λ°μ΄ν„°μ…‹ νΉμ§•
β”‚   β”β”€β”€ yeo_eval_with_normal/           # μ •μƒ λ°μ΄ν„° ν¬ν•¨ Yeo νΉμ§•
β”‚   β””β”€β”€ yeo_binary/                     # μ΄μ§„ λ¶„λ¥μ© Yeo νΉμ§•
β””β”€β”€ src/                                # μ†μ¤ μ½”λ“ ν΄λ”
    β””β”€β”€ readme.md                       # μ†μ¤ μ½”λ“ μ„¤λ…μ„
```

---

## 3. Jupyter Notebook νμΌ λ¶„μ„

### 3.1 pretrain.ipynb - SSL μ‚¬μ „ ν›λ ¨

#### 3.1.1 μ£Όμ” κΈ°λ¥
- **Self-Supervised Learning** κµ¬ν„
- **Contrastive Learning** λ°©μ‹μΌλ΅ μ¤λ””μ¤ νΉμ§• ν•™μµ
- **ColaMD** λ¨λΈμ„ μ‚¬μ©ν• μ‚¬μ „ ν›λ ¨

#### 3.1.2 μ‚¬μ©λ ν¨ν‚¤μ§€
```python
# ν•µμ‹¬ ν¨ν‚¤μ§€
import torch
import torch.nn as nn
import pytorch_lightning as pl
import librosa
import numpy as np
import pandas as pd

# λ°μ΄ν„° μ¦κ°•
from src.util import random_crop, random_mask, random_multiply

# λ¨λΈ
from src.model.models_cola import Cola, ColaMD
```

#### 3.1.3 μ£Όμ” ν΄λμ¤ λ° ν•¨μ

**AudioDataset ν΄λμ¤**
```python
class AudioDataset(torch.utils.data.Dataset):
    def __init__(self, data, max_len=200, augment=True, from_npy=False, 
                 labels=None, method="cola"):
        """
        Args:
            data: νμΌκ²½λ΅ λ¦¬μ¤νΈ or numpy λ°°μ—΄ λ¦¬μ¤νΈ
            max_len: random_crop μ‹ μ‚¬μ©ν•  ν¬κΈ°
            augment: Trueλ©΄ random_mask, random_multiply κ°™μ€ μ¦κ°• μ μ©
            from_npy: Trueλ©΄ data[idx]+".npy" νμΌμ„ λ΅λ“
            labels: μ§€λ„ν•™μµ μ‹ ν•„μ”ν• λ μ΄λΈ” (μ—†μΌλ©΄ None)
            method: "cola" (contrastive)
        """
```

**λ°μ΄ν„° μ¦κ°• ν•¨μλ“¤**
- `random_crop()`: λλ¤ ν¬λ΅­ν•‘
- `random_mask()`: λλ¤ λ§μ¤ν‚Ή
- `random_multiply()`: λλ¤ κ³±μ…

#### 3.1.4 λ¨λΈ μ•„ν‚¤ν…μ²
- **Cola**: κΈ°λ³Έ Contrastive Learning λ¨λΈ
- **ColaMD**: μλ£μ© λ°μ΄ν„°μ— νΉν™”λ Cola λ¨λΈ
- **EncoderHTSAT**: HTS-AT κΈ°λ° μΈμ½”λ” μ‚¬μ©

### 3.2 finetune with Yeo Data.ipynb - νμΈνλ‹

#### 3.2.1 μ£Όμ” κΈ°λ¥
- **Transfer Learning** κµ¬ν„
- **OperaCT** μ‚¬μ „ ν›λ ¨ λ¨λΈ ν™μ©
- **Inter-patient** λ°©μ‹μ λ°μ΄ν„° λ¶„ν• 
- **Leave-One-Out Cross-Validation (LOOCV)** μν–‰

#### 3.2.2 μ‚¬μ©λ ν¨ν‚¤μ§€
```python
# ν•µμ‹¬ ν¨ν‚¤μ§€
import torch
import torch.nn as nn
import pytorch_lightning as pl
import librosa
import numpy as np
import pandas as pd
import glob as gb

# λ¨λΈ λ° μ ν‹Έλ¦¬ν‹°
from src.model.models_eval import AudioClassifier, AudioClassifierCLAP, AudioClassifierAudioMAE
from src.benchmark.model_util import get_encoder_path, initialize_pretrained_model
from src.util import train_test_split_from_list
```

#### 3.2.3 μ£Όμ” ν•¨μ

**process_mydata_interpatient()**
```python
def process_mydata_interpatient(data_dir="data/Yeo/", 
                               feature_dir="feature/yeo_eval/fixed_split/", 
                               split=False):
    """
    μ¤λ””μ¤ λ°μ΄ν„°λ¥Ό inter-patient λ°©μ‹μΌλ΅ train/val/test λ¶„ν• ν•μ—¬
    sound_dir_loc.npy, labels.npy, split.npy νμΌμ„ μ €μ¥ν•©λ‹λ‹¤.
    """
```

**λ°μ΄ν„° μ²λ¦¬ κ³Όμ •**
1. **νμΌ μμ§‘**: normal/abnormal ν΄λ”μ—μ„ .wav νμΌ μμ§‘
2. **ν™μ ID μ¶”μ¶**: νμΌλ…μ—μ„ ν™μ ID μ¶”μ¶
3. **Inter-patient λ¶„ν• **: ν™μλ³„λ΅ train/val/test λ¶„ν• 
4. **νΉμ§• μ¶”μ¶**: OperaCT λ¨λΈλ΅ 768μ°¨μ› νΉμ§• μ¶”μ¶
5. **λ°μ΄ν„° μ €μ¥**: .npy νμΌλ΅ μ €μ¥

#### 3.2.4 λ¨λΈ μ•„ν‚¤ν…μ²
- **OperaCT**: 31.3M νλΌλ―Έν„°μ μ‚¬μ „ ν›λ ¨ λ¨λΈ
- **AudioClassifier**: λ¶„λ¥κΈ° ν—¤λ“ (1.5K νλΌλ―Έν„°)
- **Encoder κ³ μ •**: μ‚¬μ „ ν›λ ¨λ μΈμ½”λ”λ” κ³ μ •, λ¶„λ¥κΈ°λ§ ν•™μµ

### 3.3 preprocessing_ssl.ipynb - λ°μ΄ν„° μ „μ²λ¦¬

#### 3.3.1 μ£Όμ” κΈ°λ¥
- **μ¤λ””μ¤ μ „μ²λ¦¬** νμ΄ν”„λΌμΈ
- **λ°μ΄ν„° μ¦κ°•** ν•¨μ κµ¬ν„
- **μ¤ν™νΈλ΅κ·Έλ¨ μƒμ„±**
- **Bandpass ν•„ν„°λ§**

#### 3.3.2 μ‚¬μ©λ ν¨ν‚¤μ§€
```python
import torch
import torchaudio
from torchaudio import transforms as T
from scipy.signal import butter, lfilter
import pandas as pd
import librosa
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
```

#### 3.3.3 λ°μ΄ν„° μ¦κ°• ν•¨μ μƒμ„Έ λ¶„μ„

**1) Random Crop (λλ¤ ν¬λ΅­ν•‘)**
```python
def random_crop(data, crop_size=128):
    """
    λλ¤ μ„μΉμ—μ„ μΌμ •ν• ν¬κΈ°λ§νΌ ν¬λ΅­
    - λ©μ : λ‹¤μ–‘ν• μ‹κ°„ κµ¬κ°„μ—μ„ νΉμ§• ν•™μµ
    - ν¨κ³Ό: μ‹κ°„μ  μΌλ°ν™” μ„±λ¥ ν–¥μƒ
    """
    if data.shape[0] <= crop_size:
        return data  # ν¬λ΅­ν•  ν•„μ” μ—†μ
    
    start = int(random.random() * (data.shape[0] - crop_size))
    return data[start: (start + crop_size), :]
```

**2) Random Mask (λλ¤ λ§μ¤ν‚Ή)**
```python
def random_mask(data, mask_ratio=0.1):
    """
    λλ¤ν•κ² μΌλ¶€ κµ¬κ°„μ„ λ§μ¤ν‚Ή (0μΌλ΅ μ„¤μ •)
    - λ©μ : λ¨λΈμ΄ λ¶€λ¶„μ  μ •λ³΄λ΅λ„ νΉμ§•μ„ ν•™μµν•λ„λ΅
    - ν¨κ³Ό: λ…Έμ΄μ¦μ— κ°•ν• νΉμ§• ν•™μµ
    """
    mask_length = int(data.shape[0] * mask_ratio)
    start = int(random.random() * (data.shape[0] - mask_length))
    
    masked_data = data.copy()
    masked_data[start:start + mask_length, :] = 0
    return masked_data
```

**3) Random Multiply (λλ¤ κ³±μ…)**
```python
def random_multiply(data, multiply_range=(0.8, 1.2)):
    """
    λλ¤ν• λ°°μ¨λ΅ μ¤λ””μ¤ κ°•λ„ μ΅°μ 
    - λ©μ : λ‹¤μ–‘ν• λ³Όλ¥¨ λ λ²¨μ—μ„ νΉμ§• ν•™μµ
    - ν¨κ³Ό: λ³Όλ¥¨ λ³€ν™”μ— κ°•ν• νΉμ§• ν•™μµ
    """
    multiply_factor = random.uniform(multiply_range[0], multiply_range[1])
    return data * multiply_factor
```

#### 3.3.4 μ¤λ””μ¤ μ „μ²λ¦¬ ν•¨μ μƒμ„Έ λ¶„μ„

**1) Bandpass ν•„ν„°λ§**
```python
def bandpass_filter(data, low_freq=50, high_freq=2000, sample_rate=4000):
    """
    νΉμ • μ£Όνμ λ€μ—­λ§ ν†µκ³Όμ‹ν‚¤λ” ν•„ν„°
    - λ©μ : νΈν΅μμ— κ΄€λ ¨λ μ£Όνμλ§ μ¶”μ¶
    - ν¨κ³Ό: λ…Έμ΄μ¦ μ κ±° λ° νΉμ§• κ°•ν™”
    """
    nyquist = sample_rate / 2
    low = low_freq / nyquist
    high = high_freq / nyquist
    
    b, a = butter(4, [low, high], btype='band')
    filtered_data = lfilter(b, a, data)
    return filtered_data
```

**2) Mel Spectrogram μƒμ„±**
```python
def create_mel_spectrogram(audio, sample_rate=4000, n_mels=128, n_fft=1024):
    """
    Mel μ¤ν™νΈλ΅κ·Έλ¨ μƒμ„±
    - λ©μ : μΈκ°„μ μ²­κ° νΉμ„±μ„ λ°μν• νΉμ§• μ¶”μ¶
    - ν¨κ³Ό: λ” μλ―Έ μλ” μ¤λ””μ¤ νΉμ§• ν•™μµ
    """
    mel_spec = librosa.feature.melspectrogram(
        y=audio,
        sr=sample_rate,
        n_mels=n_mels,
        n_fft=n_fft,
        hop_length=n_fft//4
    )
    log_mel_spec = librosa.power_to_db(mel_spec)
    return log_mel_spec
```

**3) Padding/Resampling**
```python
def pad_or_crop(data, target_length=200):
    """
    λ°μ΄ν„°λ¥Ό μΌμ •ν• κΈΈμ΄λ΅ λ§μ¶¤
    - λ©μ : λ°°μΉ μ²λ¦¬ μ‹ μΌκ΄€λ ν¬κΈ° λ³΄μ¥
    - ν¨κ³Ό: ν¨μ¨μ μΈ ν•™μµ λ° μ¶”λ΅ 
    """
    if data.shape[0] > target_length:
        # κΈΈλ©΄ ν¬λ΅­
        return data[:target_length, :]
    elif data.shape[0] < target_length:
        # μ§§μΌλ©΄ ν¨λ”©
        padding = np.zeros((target_length - data.shape[0], data.shape[1]))
        return np.vstack([data, padding])
    else:
        return data
```

#### 3.3.5 λ°μ΄ν„° μ¦κ°•μ μ¤‘μ”μ„±

**μ™ λ°μ΄ν„° μ¦κ°•μ΄ ν•„μ”ν•κ°€?**
1. **λ°μ΄ν„° λ¶€μ΅± ν•΄κ²°**: μ μ€ λ°μ΄ν„°λ΅λ„ λ‹¤μ–‘ν• λ³€ν• μƒμ„±
2. **μΌλ°ν™” μ„±λ¥ ν–¥μƒ**: λ‹¤μ–‘ν• μ΅°κ±΄μ—μ„ κ°•κ±΄ν• λ¨λΈ ν•™μµ
3. **κ³Όμ ν•© λ°©μ§€**: λ¨λΈμ΄ νΉμ • ν¨ν„΄μ—λ§ μμ΅΄ν•μ§€ μ•λ„λ΅
4. **μ‹¤μ  ν™κ²½ λ°μ**: μ‹¤μ  μ‚¬μ© ν™κ²½μ λ‹¤μ–‘ν• μ΅°κ±΄ λ¨μ‚¬

**νΈν΅μ λ¶„μ„μ—μ„μ λ°μ΄ν„° μ¦κ°• ν¨κ³Ό:**
- **μ‹κ°„μ  λ³€ν•**: νΈν΅ μ£ΌκΈ°μ λ‹¤μ–‘ν• κµ¬κ°„ ν•™μµ
- **λ³Όλ¥¨ λ³€ν™”**: λ‹¤μ–‘ν• λ…Ήμ μ΅°κ±΄μ—μ„μ κ°•κ±΄μ„±
- **λ…Έμ΄μ¦ κ°•κ±΄μ„±**: λ¶€λ¶„μ  μ†μ‹¤μ—λ„ κ°•ν• νΉμ§• ν•™μµ
- **ν™μλ³„ μ°¨μ΄**: κ°μΈμ°¨μ— κ°•ν• μΌλ°ν™” μ„±λ¥

### 3.4 fine tuning simulation.ipynb - νμΈνλ‹ μ‹λ®¬λ μ΄μ…

#### 3.4.1 μ£Όμ” κΈ°λ¥
- **νμΈνλ‹ κ³Όμ • μ‹λ®¬λ μ΄μ…**
- **ν•μ΄νΌνλΌλ―Έν„° νλ‹**
- **μ„±λ¥ ν‰κ°€**

### 3.5 RNN experiment.ipynb - RNN μ‹¤ν—

#### 3.5.1 μ£Όμ” κΈ°λ¥
- **RNN λ¨λΈ μ‹¤ν—**
- **μ‹κ³„μ—΄ λ°μ΄ν„° μ²λ¦¬**
- **LSTM/GRU λ¨λΈ λΉ„κµ**

---

## 4. λ°μ΄ν„° νμΌ λ¶„μ„

### 4.1 feature/ ν΄λ” κµ¬μ΅°

#### 4.1.1 icbhidisease_eval/
- **labels.npy**: ICBHI λ°μ΄ν„°μ…‹ λΌλ²¨
- **operaCT768_feature.npy**: OperaCTλ΅ μ¶”μ¶ν• 768μ°¨μ› νΉμ§•
- **sound_dir_loc.npy**: μ¤λ””μ¤ νμΌ κ²½λ΅
- **split.npy**: train/val/test λ¶„ν•  μ •λ³΄

#### 4.1.2 yeo_eval/
- **labels.npy**: Yeo λ°μ΄ν„°μ…‹ λΌλ²¨
- **operaCT768_feature.npy**: OperaCTλ΅ μ¶”μ¶ν• 768μ°¨μ› νΉμ§•
- **patient_ids.npy**: ν™μ ID μ •λ³΄
- **sound_dir_loc.npy**: μ¤λ””μ¤ νμΌ κ²½λ΅
- **spectrogram_pad8.npy**: 8μ΄ ν¨λ”©λ μ¤ν™νΈλ΅κ·Έλ¨

#### 4.1.3 yeo_eval_with_normal/
- **μ •μƒ λ°μ΄ν„° ν¬ν•¨** λ²„μ „
- **μ΄μ§„ λ¶„λ¥**μ© λ°μ΄ν„°

#### 4.1.4 yeo_binary/
- **μ΄μ§„ λ¶„λ¥** μ „μ© λ°μ΄ν„°
- **μ •μƒ/λΉ„μ •μƒ** κµ¬λ¶„

### 4.2 .npy νμΌ ν•μ‹
- **NumPy λ°°μ—΄** μ €μ¥ ν•μ‹
- **ν¨μ¨μ μΈ λ°μ΄ν„° λ΅λ”©**
- **λ©”λ¨λ¦¬ μµμ ν™”**

---

## 5. ν¨ν‚¤μ§€ λ° μμ΅΄μ„± λ¶„μ„

### 5.1 ν•µμ‹¬ ν¨ν‚¤μ§€

#### 5.1.1 λ”¥λ¬λ‹ ν”„λ μ„μ›ν¬
- **PyTorch**: λ©”μΈ λ”¥λ¬λ‹ ν”„λ μ„μ›ν¬
- **PyTorch Lightning**: κ³ μμ¤€ ν›λ ¨ λνΌ
- **torchaudio**: μ¤λ””μ¤ μ²λ¦¬

#### 5.1.2 μ¤λ””μ¤ μ²λ¦¬
- **librosa**: μ¤λ””μ¤ λ¶„μ„ λ° νΉμ§• μ¶”μ¶
- **scipy.signal**: μ‹ νΈ μ²λ¦¬ (ν•„ν„°λ§ λ“±)

#### 5.1.3 λ°μ΄ν„° μ²λ¦¬
- **numpy**: μμΉ κ³„μ‚°
- **pandas**: λ°μ΄ν„° ν”„λ μ„ μ²λ¦¬
- **scikit-learn**: λ¨Έμ‹ λ¬λ‹ μ ν‹Έλ¦¬ν‹°

#### 5.1.4 μ‹κ°ν™”
- **matplotlib**: κΈ°λ³Έ ν”λ΅―
- **seaborn**: ν†µκ³„μ  μ‹κ°ν™”

### 5.2 ν¨ν‚¤μ§€λ³„ μ—­ν• 

| ν¨ν‚¤μ§€ | μ—­ν•  | μ‚¬μ© μμ‹ |
|--------|------|-----------|
| **torch** | λ”¥λ¬λ‹ λ¨λΈ κµ¬ν„ | `torch.nn.Module` |
| **pytorch_lightning** | ν›λ ¨ λνΌ | `pl.Trainer` |
| **librosa** | μ¤λ””μ¤ νΉμ§• μ¶”μ¶ | `librosa.stft()` |
| **numpy** | λ°°μ—΄ μ—°μ‚° | `np.load()`, `np.save()` |
| **pandas** | λ°μ΄ν„° κ΄€λ¦¬ | `pd.DataFrame` |

---

## 6. λ¨λΈ μ•„ν‚¤ν…μ² λ¶„μ„

### 6.1 Self-Supervised Learning λ¨λΈ

#### 6.1.1 Cola λ¨λΈ μƒμ„Έ λ¶„μ„

**Cola (Contrastive Learning for Audio) λ¨λΈ**μ€ μ¤λ””μ¤ λ°μ΄ν„°λ¥Ό μ„ν• Contrastive Learning λ¨λΈμ…λ‹λ‹¤.

**ν•µμ‹¬ κµ¬μ΅°:**
```python
class Cola(pl.LightningModule):
    def __init__(self, encoder, projection_dim=256):
        super().__init__()
        self.encoder = encoder  # νΉμ§• μ¶”μ¶κΈ°
        self.projection_head = nn.Sequential(
            nn.Linear(encoder_dim, projection_dim),
            nn.ReLU(),
            nn.Linear(projection_dim, projection_dim)
        )
```

**μ‘λ™ μ›λ¦¬:**
1. **μΈμ½”λ”**: μ¤λ””μ¤ β†’ νΉμ§• λ²΅ν„°
2. **ν”„λ΅μ μ… ν—¤λ“**: νΉμ§• λ²΅ν„° β†’ λ€μ΅° ν•™μµμ© λ²΅ν„°
3. **λ€μ΅° μ†μ‹¤**: Positive/Negative pairs ν•™μµ

**ν•™μµ κ³Όμ •:**
```python
def forward(self, x1, x2):
    # 1. νΉμ§• μ¶”μ¶
    features_1 = self.encoder(x1)  # [batch_size, 768]
    features_2 = self.encoder(x2)  # [batch_size, 768]
    
    # 2. ν”„λ΅μ μ…
    proj_1 = self.projection_head(features_1)  # [batch_size, 256]
    proj_2 = self.projection_head(features_2)  # [batch_size, 256]
    
    # 3. λ€μ΅° μ†μ‹¤ κ³„μ‚°
    loss = self.contrastive_loss(proj_1, proj_2)
    return loss
```

#### 6.1.2 ColaMD λ¨λΈ μƒμ„Έ λ¶„μ„

**ColaMD (Contrastive Learning for Medical Data)**λ” μλ£μ© λ°μ΄ν„°μ— νΉν™”λ Cola λ¨λΈμ…λ‹λ‹¤.

**Colaμ™€μ μ°¨μ΄μ :**
1. **μλ£ λ„λ©”μΈ νΉν™”**: νΈν΅μμ νΉμ„±μ— λ§μ¶ μ„¤κ³„
2. **λ‹¤μ¤‘ λ°μ΄ν„°μ…‹ ν†µν•©**: ICBHI, HF_Lung, KAUH λ“± ν†µν•© ν•™μµ
3. **μλ£μ  νΉμ„± λ°μ**: νΈν΅ μ£ΌκΈ°, λ³‘λ¦¬μ  μ†μ λ“± κ³ λ ¤

**λ°μ΄ν„° ν†µν•© λ°©μ‹:**
```python
# λ‹¤μ¤‘ λ°μ΄ν„°μ…‹ ν†µν•©
datasets = [
    "ICBHI",           # νΈν΅μ λ°μ΄ν„°μ…‹
    "HF_Lung",         # μ‹¬μ¥/νμ λ°μ΄ν„°μ…‹
    "KAUH",            # ν•κµ­ μλ£ λ°μ΄ν„°μ…‹
    "PulmonarySound",  # νμ λ°μ΄ν„°μ…‹
    "SPRSound"         # νΈν΅μ λ°μ΄ν„°μ…‹
]

# ν†µν•© ν•™μµ
for dataset in datasets:
    data_loader = create_dataloader(dataset)
    model.train_on_dataset(data_loader)
```

### 6.2 Transfer Learning λ¨λΈ

#### 6.2.1 OperaCT μƒμ„Έ λ¶„μ„

**OperaCT (Opera Contrastive Transformer)**λ” λ€κ·λ¨ μ¤λ””μ¤ λ°μ΄ν„°λ΅ μ‚¬μ „ ν›λ ¨λ λ¨λΈμ…λ‹λ‹¤.

**λ¨λΈ κµ¬μ΅°:**
```
μ…λ ¥: μ¤λ””μ¤ μ¤ν™νΈλ΅κ·Έλ¨ [batch_size, mel_bins, time_frames]
    β†“
[Patch Embedding] β†’ [batch_size, num_patches, embed_dim]
    β†“
[Transformer Encoder] Γ— 12 layers
    β†“
[Global Average Pooling] β†’ [batch_size, embed_dim]
    β†“
μ¶λ ¥: 768μ°¨μ› νΉμ§• λ²΅ν„°
```

**ν•µμ‹¬ νΉμ§•:**
1. **31.3M νλΌλ―Έν„°**: λ€κ·λ¨ λ¨λΈ
2. **Transformer κΈ°λ°**: Attention λ©”μ»¤λ‹μ¦ ν™μ©
3. **Contrastive Learning**: SSLλ΅ μ‚¬μ „ ν›λ ¨
4. **768μ°¨μ› μ¶λ ¥**: κ³ μ°¨μ› νΉμ§• ν‘ν„

**μ‚¬μ „ ν›λ ¨ κ³Όμ •:**
```python
# 1. λ€κ·λ¨ μ¤λ””μ¤ λ°μ΄ν„° μμ§‘
audio_data = load_large_audio_dataset()  # μλ°±λ§ κ° μ¤λ””μ¤

# 2. λ°μ΄ν„° μ¦κ°•
augmented_pairs = []
for audio in audio_data:
    pair_1 = random_crop(audio)
    pair_2 = random_mask(audio)
    augmented_pairs.append((pair_1, pair_2))

# 3. Contrastive Learning
for pair_1, pair_2 in augmented_pairs:
    features_1 = operact_encoder(pair_1)
    features_2 = operact_encoder(pair_2)
    loss = contrastive_loss(features_1, features_2)
    optimizer.step()
```

#### 6.2.2 AudioClassifier μƒμ„Έ λ¶„μ„

**AudioClassifier**λ” OperaCTμ νΉμ§•μ„ λ°›μ•„μ„ λ¶„λ¥λ¥Ό μν–‰ν•λ” λ¨λΈμ…λ‹λ‹¤.

**λ¨λΈ κµ¬μ΅°:**
```python
class AudioClassifier(pl.LightningModule):
    def __init__(self, net, head="linear", classes=2, feat_dim=768):
        super().__init__()
        self.net = net  # κ³ μ •λ OperaCT μΈμ½”λ”
        self.head = self._create_head(head, feat_dim, classes)
    
    def _create_head(self, head_type, feat_dim, classes):
        if head_type == "linear":
            return nn.Linear(feat_dim, classes)
        elif head_type == "mlp":
            return nn.Sequential(
                nn.Linear(feat_dim, feat_dim//2),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(feat_dim//2, classes)
            )
```

**ν•™μµ λ°©μ‹:**
1. **μΈμ½”λ” κ³ μ •**: OperaCTμ κ°€μ¤‘μΉλ” κ³ μ •
2. **λ¶„λ¥κΈ°λ§ ν•™μµ**: 1.5K νλΌλ―Έν„°λ§ ν•™μµ
3. **λΉ λ¥Έ μλ ΄**: μ μ€ νλΌλ―Έν„°λ΅ λΉ λ¥Έ ν•™μµ

**ν•™μµ κ³Όμ •:**
```python
def forward(self, x):
    # 1. κ³ μ •λ μΈμ½”λ”λ΅ νΉμ§• μ¶”μ¶
    with torch.no_grad():  # μΈμ½”λ”λ” κ³ μ •
        features = self.net(x)  # [batch_size, 768]
    
    # 2. λ¶„λ¥κΈ°λ΅ μμΈ΅
    logits = self.head(features)  # [batch_size, num_classes]
    return logits
```

### 6.3 μ „μ²΄ λ¨λΈ νμ΄ν”„λΌμΈ

#### 6.3.1 μ‚¬μ „ ν›λ ¨ λ‹¨κ³„ (SSL)
```
λ€λ‰μ λΌλ²¨ μ—†λ” μ¤λ””μ¤ λ°μ΄ν„°
    β†“
[λ°μ΄ν„° μ¦κ°•] β†’ λ³€ν•λ μ¤λ””μ¤ μ
    β†“
[ColaMD λ¨λΈ] β†’ Contrastive Learning
    β†“
[OperaCT μΈμ½”λ”] β†’ 768μ°¨μ› νΉμ§• ν•™μµ
```

#### 6.3.2 νμΈνλ‹ λ‹¨κ³„ (Transfer Learning)
```
μ†λ‰μ λΌλ²¨ μλ” νΈν΅μ λ°μ΄ν„°
    β†“
[κ³ μ •λ OperaCT] β†’ 768μ°¨μ› νΉμ§• μ¶”μ¶
    β†“
[AudioClassifier] β†’ λ¶„λ¥ ν•™μµ
    β†“
[νΈν΅μ λ¶„λ¥ κ²°κ³Ό] β†’ μ •μƒ/λΉ„μ •μƒ
```

### 6.4 λ¨λΈμ μ¥μ κ³Ό νΉμ§•

#### 6.4.1 SSL λ¨λΈμ μ¥μ 
1. **λΌλ²¨ λ¶ν•„μ”**: λ€λ‰μ λΌλ²¨ μ—†λ” λ°μ΄ν„° ν™μ©
2. **μΌλ°ν™” μ„±λ¥**: λ‹¤μ–‘ν• μ¤λ””μ¤ νƒμ¤ν¬μ— μ μ© κ°€λ¥
3. **μλ―Έ μλ” νΉμ§•**: λ°μ΄ν„°μ λ³Έμ§μ μΈ νΉμ„± ν•™μµ
4. **λ„λ©”μΈ μ μ‘**: μƒλ΅μ΄ λ„λ©”μΈμ— λΉ λ¥΄κ² μ μ‘

#### 6.4.2 Transfer Learningμ μ¥μ 
1. **λΉ λ¥Έ ν•™μµ**: μ‚¬μ „ ν›λ ¨λ νΉμ§• ν™μ©
2. **μ μ€ λ°μ΄ν„°**: μ†λ‰μ λΌλ²¨ λ°μ΄ν„°λ΅λ„ μΆ‹μ€ μ„±λ¥
3. **μ•μ •μ  ν•™μµ**: μλ ΄μ΄ λΉ λ¥΄κ³  μ•μ •μ 
4. **λΉ„μ© ν¨μ¨μ„±**: ν•™μµ μ‹κ°„κ³Ό λΉ„μ© μ μ•½

#### 6.4.3 μ „μ²΄ μ‹μ¤ν…μ μ¥μ 
1. **μ™„μ „ν• νμ΄ν”„λΌμΈ**: μ „μ²λ¦¬λ¶€ν„° μ¶”λ΅ κΉμ§€
2. **ν™•μ¥ κ°€λ¥μ„±**: μƒλ΅μ΄ λ°μ΄ν„°μ…‹μ— μ‰½κ² μ μ©
3. **μ‹¤μ©μ„±**: μ‹¤μ  μλ£ ν™κ²½μ— μ μ© κ°€λ¥
4. **μ„±λ¥**: λ†’μ€ λ¶„λ¥ μ •ν™•λ„

---

## 7. μ „μ²΄ μ‹μ¤ν… μ›ν¬ν”λ΅μ°

### 7.1 μ‚¬μ „ ν›λ ¨ λ‹¨κ³„ (SSL) μƒμ„Έ λ¶„μ„

#### 7.1.1 1λ‹¨κ³„: λ°μ΄ν„° μμ§‘ λ° μ „μ²λ¦¬
```python
# 1. λ‹¤μ¤‘ λ°μ΄ν„°μ…‹ μμ§‘
datasets = {
    "ICBHI": "νΈν΅μ λ°μ΄ν„°μ…‹",
    "HF_Lung": "μ‹¬μ¥/νμ λ°μ΄ν„°μ…‹", 
    "KAUH": "ν•κµ­ μλ£ λ°μ΄ν„°μ…‹",
    "PulmonarySound": "νμ λ°μ΄ν„°μ…‹",
    "SPRSound": "νΈν΅μ λ°μ΄ν„°μ…‹"
}

# 2. μ¤λ””μ¤ μ „μ²λ¦¬
for dataset_name, audio_files in datasets.items():
    for audio_file in audio_files:
        # μƒν”λ§ λ μ΄νΈ ν†µμΌ (4kHz)
        audio = librosa.load(audio_file, sr=4000)[0]
        
        # Bandpass ν•„ν„°λ§ (50-2000Hz)
        filtered_audio = bandpass_filter(audio)
        
        # Mel μ¤ν™νΈλ΅κ·Έλ¨ λ³€ν™
        mel_spec = create_mel_spectrogram(filtered_audio)
        
        # .npy νμΌλ΅ μ €μ¥
        np.save(f"processed_{dataset_name}_{audio_file}.npy", mel_spec)
```

#### 7.1.2 2λ‹¨κ³„: λ°μ΄ν„° μ¦κ°• λ° μ μƒμ„±
```python
# 3. λ°μ΄ν„° μ¦κ°•μΌλ΅ Positive Pairs μƒμ„±
def create_positive_pairs(mel_spec):
    # κ°™μ€ μ¤λ””μ¤μ—μ„ μ„λ΅ λ‹¤λ¥Έ λ³€ν• μƒμ„±
    augmented_1 = random_crop(mel_spec)
    augmented_2 = random_mask(mel_spec)
    augmented_3 = random_multiply(mel_spec)
    
    # Positive pairs: κ°™μ€ μ¤λ””μ¤μ λ³€ν•λ“¤
    positive_pairs = [
        (augmented_1, augmented_2),
        (augmented_1, augmented_3),
        (augmented_2, augmented_3)
    ]
    return positive_pairs

# 4. Negative Pairs μƒμ„±
def create_negative_pairs(mel_spec_1, mel_spec_2):
    # λ‹¤λ¥Έ μ¤λ””μ¤μ λ³€ν•λ“¤
    neg_1 = random_crop(mel_spec_1)
    neg_2 = random_crop(mel_spec_2)
    return (neg_1, neg_2)
```

#### 7.1.3 3λ‹¨κ³„: Contrastive Learning
```python
# 5. ColaMD λ¨λΈ ν›λ ¨
class ColaMD(pl.LightningModule):
    def training_step(self, batch, batch_idx):
        x1, x2 = batch  # Positive pairs
        
        # νΉμ§• μ¶”μ¶
        features_1 = self.encoder(x1)  # [batch_size, 768]
        features_2 = self.encoder(x2)  # [batch_size, 768]
        
        # ν”„λ΅μ μ…
        proj_1 = self.projection_head(features_1)  # [batch_size, 256]
        proj_2 = self.projection_head(features_2)  # [batch_size, 256]
        
        # Contrastive Loss κ³„μ‚°
        loss = self.contrastive_loss(proj_1, proj_2)
        return loss
    
    def contrastive_loss(self, proj_1, proj_2):
        # Positive pairsλ” κ°€κΉκ² (μ‘μ€ κ±°λ¦¬)
        positive_loss = F.mse_loss(proj_1, proj_2)
        
        # Negative pairsλ” λ©€κ² (ν° κ±°λ¦¬)
        negative_loss = -F.mse_loss(proj_1, proj_2)
        
        return positive_loss + negative_loss
```

#### 7.1.4 4λ‹¨κ³„: OperaCT μΈμ½”λ” ν•™μµ
```python
# 6. OperaCT μΈμ½”λ”κ°€ 768μ°¨μ› νΉμ§•μ„ ν•™μµ
# - Transformer κΈ°λ° μ•„ν‚¤ν…μ²
# - 31.3M νλΌλ―Έν„°
# - λ€κ·λ¨ μ¤λ””μ¤ λ°μ΄ν„°λ΅ μ‚¬μ „ ν›λ ¨
# - 768μ°¨μ› νΉμ§• λ²΅ν„° μ¶λ ¥
```

### 7.2 νμΈνλ‹ λ‹¨κ³„ (Transfer Learning) μƒμ„Έ λ¶„μ„

#### 7.2.1 1λ‹¨κ³„: λΌλ²¨ λ°μ΄ν„° μ¤€λΉ„
```python
# 1. Yeo λ°μ΄ν„°μ…‹ λ΅λ”©
def load_yeo_dataset(data_dir="data/Yeo/"):
    audio_files = []
    labels = []
    patient_ids = []
    
    # μ •μƒ/λΉ„μ •μƒ ν΄λ”μ—μ„ λ°μ΄ν„° μμ§‘
    for label_type in ["normal", "abnormal"]:
        folder_path = os.path.join(data_dir, label_type)
        for audio_file in os.listdir(folder_path):
            if audio_file.endswith('.wav'):
                audio_files.append(os.path.join(folder_path, audio_file))
                labels.append(0 if label_type == "normal" else 1)
                patient_ids.append(extract_patient_id(audio_file))
    
    return audio_files, labels, patient_ids
```

#### 7.2.2 2λ‹¨κ³„: Inter-patient λ¶„ν• 
```python
# 2. ν™μλ³„λ΅ train/val/test λ¶„ν• 
def inter_patient_split(patient_ids, test_ratio=0.2, val_ratio=0.1):
    unique_patients = list(set(patient_ids))
    
    # ν™μλ³„λ΅ λ¶„ν•  (λ°μ΄ν„° λ„μ λ°©μ§€)
    test_patients = random.sample(unique_patients, int(len(unique_patients) * test_ratio))
    remaining_patients = [p for p in unique_patients if p not in test_patients]
    val_patients = random.sample(remaining_patients, int(len(remaining_patients) * val_ratio))
    train_patients = [p for p in remaining_patients if p not in val_patients]
    
    return train_patients, val_patients, test_patients
```

#### 7.2.3 3λ‹¨κ³„: νΉμ§• μ¶”μ¶
```python
# 3. κ³ μ •λ OperaCTλ΅ νΉμ§• μ¶”μ¶
def extract_features_with_operact(audio_files):
    # OperaCT λ¨λΈ λ΅λ“
    operact_model = initialize_pretrained_model("operaCT")
    operact_model.eval()  # ν‰κ°€ λ¨λ“λ΅ μ„¤μ •
    
    features = []
    with torch.no_grad():  # κ·Έλλ””μ–ΈνΈ κ³„μ‚° λΉ„ν™μ„±ν™”
        for audio_file in audio_files:
            # μ¤λ””μ¤ λ΅λ“ λ° μ „μ²λ¦¬
            audio = load_and_preprocess_audio(audio_file)
            
            # OperaCTλ΅ νΉμ§• μ¶”μ¶
            feature = operact_model.encoder(audio)  # [768μ°¨μ›]
            features.append(feature.cpu().numpy())
    
    return np.array(features)
```

#### 7.2.4 4λ‹¨κ³„: λ¶„λ¥κΈ° ν›λ ¨
```python
# 4. AudioClassifier ν›λ ¨
class AudioClassifier(pl.LightningModule):
    def __init__(self, net, classes=2, feat_dim=768):
        super().__init__()
        self.net = net  # κ³ μ •λ OperaCT μΈμ½”λ”
        self.classifier = nn.Linear(feat_dim, classes)
        
    def forward(self, x):
        # κ³ μ •λ μΈμ½”λ”λ΅ νΉμ§• μ¶”μ¶
        with torch.no_grad():
            features = self.net(x)  # [batch_size, 768]
        
        # λ¶„λ¥κΈ°λ΅ μμΈ΅
        logits = self.classifier(features)  # [batch_size, 2]
        return logits
    
    def training_step(self, batch, batch_idx):
        x, y = batch
        logits = self.forward(x)
        loss = F.cross_entropy(logits, y)
        return loss
```

#### 7.2.5 5λ‹¨κ³„: LOOCV ν‰κ°€
```python
# 5. Leave-One-Out Cross-Validation
def train_loocv(features, labels, patient_ids):
    unique_patients = np.unique(patient_ids)
    results = []
    
    for test_patient in unique_patients:
        # Train/Test λ¶„ν• 
        train_mask = patient_ids != test_patient
        test_mask = patient_ids == test_patient
        
        x_train, y_train = features[train_mask], labels[train_mask]
        x_test, y_test = features[test_mask], labels[test_mask]
        
        # λ¨λΈ ν›λ ¨
        model = AudioClassifier(operact_encoder)
        trainer = pl.Trainer(max_epochs=10)
        trainer.fit(model, train_loader)
        
        # ν‰κ°€
        test_results = trainer.test(model, test_loader)
        results.append(test_results)
    
    return results
```

### 7.3 μ‹¤μ‹κ°„ μ¶”λ΅  μ›ν¬ν”λ΅μ°

#### 7.3.1 μ‹¤μ‹κ°„ μ²λ¦¬ νμ΄ν”„λΌμΈ
```python
def real_time_inference(audio_stream):
    """
    μ‹¤μ‹κ°„ νΈν΅μ λ¶„μ„
    - 8μ΄ μ„Έκ·Έλ¨ΌνΈ λ‹¨μ„ μ²λ¦¬
    - μ‹¤μ‹κ°„ λ¶„λ¥ κ²°κ³Ό μ¶λ ¥
    """
    # 1. μ¤λ””μ¤ μ¤νΈλ¦Όμ„ 8μ΄ μ„Έκ·Έλ¨ΌνΈλ΅ λ¶„ν• 
    segments = split_audio_stream(audio_stream, segment_length=8)
    
    results = []
    for segment in segments:
        # 2. μ „μ²λ¦¬
        processed_segment = preprocess_audio(segment)
        
        # 3. νΉμ§• μ¶”μ¶ (κ³ μ •λ OperaCT)
        with torch.no_grad():
            features = operact_encoder(processed_segment)
        
        # 4. λ¶„λ¥
        prediction = classifier(features)
        confidence = torch.softmax(prediction, dim=-1)
        
        # 5. κ²°κ³Ό μ¶λ ¥
        result = {
            'segment_id': len(results),
            'prediction': 'normal' if prediction.argmax() == 0 else 'abnormal',
            'confidence': confidence.max().item(),
            'timestamp': time.time()
        }
        results.append(result)
    
    return results
```

### 7.4 μ „μ²΄ λ°μ΄ν„° νλ¦„λ„

```
π“ μ „μ²΄ μ‹μ¤ν… λ°μ΄ν„° νλ¦„

1. μ‚¬μ „ ν›λ ¨ λ‹¨κ³„ (SSL):
   λ€λ‰ λΌλ²¨ μ—†λ” μ¤λ””μ¤ β†’ μ „μ²λ¦¬ β†’ λ°μ΄ν„° μ¦κ°• β†’ ColaMD ν›λ ¨ β†’ OperaCT μΈμ½”λ” ν•™μµ

2. νμΈνλ‹ λ‹¨κ³„ (Transfer Learning):
   μ†λ‰ λΌλ²¨ μλ” νΈν΅μ β†’ μ „μ²λ¦¬ β†’ OperaCT νΉμ§• μ¶”μ¶ β†’ AudioClassifier ν›λ ¨

3. μ‹¤μ‹κ°„ μ¶”λ΅ :
   μ‹¤μ‹κ°„ μ¤λ””μ¤ μ¤νΈλ¦Ό β†’ 8μ΄ μ„Έκ·Έλ¨ΌνΈ λ¶„ν•  β†’ μ „μ²λ¦¬ β†’ OperaCT νΉμ§• μ¶”μ¶ β†’ λ¶„λ¥ β†’ κ²°κ³Ό μ¶λ ¥
```

### 7.5 μ„±λ¥ μµμ ν™” μ „λµ

#### 7.5.1 λ¨λΈ μµμ ν™”
- **μΈμ½”λ” κ³ μ •**: OperaCT κ°€μ¤‘μΉ κ³ μ •μΌλ΅ λΉ λ¥Έ μ¶”λ΅ 
- **κ²½λ‰ν™”**: 1.5K νλΌλ―Έν„° λ¶„λ¥κΈ°λ§ ν•™μµ
- **λ°°μΉ μ²λ¦¬**: μ—¬λ¬ μ„Έκ·Έλ¨ΌνΈ λ™μ‹ μ²λ¦¬

#### 7.5.2 μ‹¤μ‹κ°„ μ²λ¦¬ μµμ ν™”
- **μ„Έκ·Έλ¨ΌνΈ λ‹¨μ„**: 8μ΄ λ‹¨μ„λ΅ μ²λ¦¬ν•μ—¬ μ§€μ—° μ‹κ°„ μµμ†ν™”
- **λΉ„λ™κΈ° μ²λ¦¬**: μ „μ²λ¦¬μ™€ μ¶”λ΅ μ„ λ³‘λ ¬λ΅ μν–‰
- **μΊμ‹±**: μμ£Ό μ‚¬μ©λλ” νΉμ§•μ„ μΊμ‹ν•μ—¬ μ†λ„ ν–¥μƒ

---

## 8. μ£Όμ” νΉμ§• λ° μ¥μ 

### 8.1 Self-Supervised Learningμ μ¥μ 
- **λΌλ²¨ μ—†λ” λ°μ΄ν„°** ν™μ© κ°€λ¥
- **λ€κ·λ¨ λ°μ΄ν„°μ…‹** ν•™μµ
- **μΌλ°ν™” μ„±λ¥** ν–¥μƒ

### 8.2 Transfer Learningμ μ¥μ 
- **λΉ λ¥Έ ν›λ ¨** μ†λ„
- **μ μ€ λ°μ΄ν„°**λ΅λ„ μΆ‹μ€ μ„±λ¥
- **λ„λ©”μΈ μ μ‘** μ©μ΄

### 8.3 Inter-patient λ¶„ν• μ μ¥μ 
- **μ‹¤μ  μ„μƒ ν™κ²½** λ°μ
- **κ³Όμ ν•© λ°©μ§€**
- **μΌλ°ν™” μ„±λ¥** ν–¥μƒ

---

## 9. κ²°λ΅  λ° ν™μ© λ°©μ•

### 9.1 ν”„λ΅μ νΈμ κ°•μ 
1. **μ™„μ „ν• νμ΄ν”„λΌμΈ**: μ „μ²λ¦¬λ¶€ν„° μ¶”λ΅ κΉμ§€
2. **κ²€μ¦λ λ°©λ²•λ΅ **: SSL + Transfer Learning
3. **μ‹¤μ©μ  μ„¤κ³„**: Inter-patient λ¶„ν• , LOOCV
4. **ν™•μ¥ κ°€λ¥μ„±**: λ‹¤μ–‘ν• λ°μ΄ν„°μ…‹ μ μ© κ°€λ¥

### 9.2 ν™μ© λ°©μ•
1. **μ‹¤μ‹κ°„ νΈν΅μ λ¶„μ„**: 8μ΄ μ„Έκ·Έλ¨ΌνΈ μ²λ¦¬
2. **μλ£μ§„ λ³΄μ΅° λ„κµ¬**: κ°κ΄€μ  μ§„λ‹¨ μ§€μ›
3. **μ—°κµ¬ ν”λ«νΌ**: μƒλ΅μ΄ λ°μ΄ν„°μ…‹ μ‹¤ν—
4. **κµμ΅ λ„κµ¬**: νΈν΅μ λ¶„μ„ ν•™μµ

### 9.3 κ°μ„  κ°€λ¥ν• λ¶€λ¶„
1. **μ‹¤μ‹κ°„ μ²λ¦¬**: λ” λΉ λ¥Έ μ¶”λ΅  μ†λ„
2. **λ‹¤μ¤‘ ν΄λμ¤**: 3ν΄λμ¤ μ΄μƒ λ¶„λ¥
3. **μ„¤λ… κ°€λ¥μ„±**: λ¨λΈ ν•΄μ„ κ°€λ¥μ„±
4. **λ¨λ°”μΌ μµμ ν™”**: κ²½λ‰ν™”λ λ¨λΈ

---

*μ΄ λ³΄κ³ μ„λ” Github Analysis ν΄λ”μ λ¨λ“  νμΌμ„ λ¶„μ„ν•μ—¬ μ‘μ„±λμ—μµλ‹λ‹¤.*
