#!/usr/bin/env python3
"""
ì¢…í•© í…ŒìŠ¤íŠ¸: ëª¨ë“  ë…¸íŠ¸ë¶ì˜ í•µì‹¬ ê¸°ëŠ¥ í†µí•© í…ŒìŠ¤íŠ¸
"""

print("ğŸš€ ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹œì‘!")
print("=" * 60)

# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ import
print("\nğŸ“¦ 1ë‹¨ê³„: ë¼ì´ë¸ŒëŸ¬ë¦¬ import")
print("-" * 40)

import numpy as np
import os
import random
import math
import time
import pandas as pd
import librosa
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import pytorch_lightning as pl
from sklearn.model_selection import train_test_split
from tqdm import tqdm

# ìš°ë¦¬ê°€ ë§Œë“  ëª¨ë“ˆë“¤
from src.util import random_crop, random_mask, random_multiply, crop_first
from src.model.models_cola import Cola, ColaMD, SimpleEncoder
from src.model.models_eval import AudioClassifier
from src.benchmark.model_util import initialize_pretrained_model

print("âœ… ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì™„ë£Œ!")

# 2. ë°ì´í„° ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
print("\nğŸ”§ 2ë‹¨ê³„: ë°ì´í„° ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸")
print("-" * 40)

def create_mel_spectrogram(audio, sample_rate=16000, n_mels=128):
    """Mel Spectrogram ìƒì„±"""
    S = librosa.feature.melspectrogram(
        y=audio, sr=sample_rate, n_mels=n_mels, 
        n_fft=1024, hop_length=512
    )
    S = librosa.power_to_db(S, ref=np.max)
    if S.max() != S.min():
        mel_db = (S - S.min()) / (S.max() - S.min())
    else:
        mel_db = S
    return mel_db.T

# ë”ë¯¸ ì˜¤ë””ì˜¤ ìƒì„±
dummy_audio = np.random.randn(16000)  # 1ì´ˆ ê¸¸ì´
mel_spec = create_mel_spectrogram(dummy_audio)
print(f"Mel Spectrogram shape: {mel_spec.shape}")

# ë°ì´í„° ì¦ê°• í…ŒìŠ¤íŠ¸
cropped = random_crop(mel_spec, crop_size=128)
masked = random_mask(mel_spec)
multiplied = random_multiply(mel_spec)

print(f"Random crop shape: {cropped.shape}")
print(f"Random mask shape: {masked.shape}")
print(f"Random multiply shape: {multiplied.shape}")

print("âœ… ë°ì´í„° ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

# 3. Self-Supervised Learning í…ŒìŠ¤íŠ¸
print("\nğŸ”§ 3ë‹¨ê³„: Self-Supervised Learning í…ŒìŠ¤íŠ¸")
print("-" * 40)

class AudioDataset(torch.utils.data.Dataset):
    """Contrastive Learningìš© Dataset"""
    def __init__(self, data, max_len=200, augment=True, method="cola"):
        self.data = data
        self.max_len = max_len
        self.augment = augment
        self.method = method

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        x = self.data[idx]
        
        if self.method == "cola":
            if self.augment:
                x = random_mask(x)
            x1 = random_crop(x, crop_size=self.max_len)
            x2 = random_crop(x, crop_size=self.max_len)
            return x1, x2
        else:
            return x

# ë”ë¯¸ ë°ì´í„° ìƒì„±
dummy_data = [np.random.randn(200, 128) for _ in range(20)]
dataset = AudioDataset(dummy_data, max_len=128, augment=True, method="cola")
dataloader = DataLoader(dataset, batch_size=4, shuffle=True)

# Cola ëª¨ë¸ í…ŒìŠ¤íŠ¸
encoder = SimpleEncoder(input_dim=128, hidden_dim=512, output_dim=768)
cola_model = Cola(encoder, projection_dim=256, learning_rate=1e-4)

# ë°°ì¹˜ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
batch_x1, batch_x2 = next(iter(dataloader))
batch_x1 = batch_x1.float()
batch_x2 = batch_x2.float()

cola_model.eval()
with torch.no_grad():
    proj_1, proj_2 = cola_model(batch_x1, batch_x2)
    loss = cola_model.contrastive_loss(proj_1, proj_2)
    print(f"Contrastive Loss: {loss.item():.4f}")

print("âœ… Self-Supervised Learning í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

# 4. Transfer Learning í…ŒìŠ¤íŠ¸
print("\nğŸ”§ 4ë‹¨ê³„: Transfer Learning í…ŒìŠ¤íŠ¸")
print("-" * 40)

# ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ
pretrained_model = initialize_pretrained_model("operaCT")

# AudioClassifier ìƒì„±
classifier = AudioClassifier(
    net=pretrained_model,
    head="linear",
    classes=2,
    lr=1e-4,
    l2_strength=1e-4,
    feat_dim=768,
    freeze_encoder="none"
)

# ë”ë¯¸ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
X = torch.FloatTensor(np.random.randn(10, 200, 128))
y = torch.LongTensor([0, 1] * 5)

classifier.eval()
with torch.no_grad():
    logits = classifier(X)
    preds = torch.argmax(logits, dim=1)
    accuracy = (preds == y).float().mean()
    print(f"Transfer Learning ì •í™•ë„: {accuracy.item():.3f}")

print("âœ… Transfer Learning í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

# 5. ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
print("\nğŸ”§ 5ë‹¨ê³„: ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸")
print("-" * 40)

def full_pipeline_test():
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
    print("  ğŸ“Š 1. ì˜¤ë””ì˜¤ ë°ì´í„° ìƒì„±")
    audio = np.random.randn(16000)  # 1ì´ˆ ê¸¸ì´
    
    print("  ğŸ”§ 2. ì „ì²˜ë¦¬ ë° íŠ¹ì§• ì¶”ì¶œ")
    mel_spec = create_mel_spectrogram(audio)
    print(f"    Mel Spectrogram shape: {mel_spec.shape}")
    
    print("  ğŸ¯ 3. ë°ì´í„° ì¦ê°•")
    x1 = random_crop(mel_spec, crop_size=128)
    x2 = random_crop(mel_spec, crop_size=128)
    print(f"    Augmented data shapes: {x1.shape}, {x2.shape}")
    
    print("  ğŸ§  4. Self-Supervised Learning")
    encoder = SimpleEncoder(input_dim=128, hidden_dim=512, output_dim=768)
    cola_model = Cola(encoder, projection_dim=256)
    
    x1_tensor = torch.FloatTensor(x1).unsqueeze(0)  # [1, 128, 128]
    x2_tensor = torch.FloatTensor(x2).unsqueeze(0)  # [1, 128, 128]
    
    cola_model.eval()
    with torch.no_grad():
        proj_1, proj_2 = cola_model(x1_tensor, x2_tensor)
        ssl_loss = cola_model.contrastive_loss(proj_1, proj_2)
        print(f"    SSL Loss: {ssl_loss.item():.4f}")
    
    print("  ğŸ”„ 5. Transfer Learning")
    pretrained_model = initialize_pretrained_model("operaCT")
    classifier = AudioClassifier(
        net=pretrained_model,
        head="linear",
        classes=2,
        feat_dim=768
    )
    
    # íŠ¹ì§• ì¶”ì¶œ
    with torch.no_grad():
        features = pretrained_model(x1_tensor)
        print(f"    Extracted features shape: {features.shape}")
    
    # ë¶„ë¥˜
    with torch.no_grad():
        logits = classifier(x1_tensor)
        prediction = torch.argmax(logits, dim=1)
        print(f"    Prediction: {prediction.item()}")
    
    print("  âœ… ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")

full_pipeline_test()

print("\nğŸ¯ í•µì‹¬ ê°œë… ì¢…í•© ì •ë¦¬")
print("-" * 40)
print("1. ğŸ“Š ë°ì´í„° ì „ì²˜ë¦¬:")
print("   - Mel Spectrogram: ì¸ê°„ ì²­ê° íŠ¹ì„± ë°˜ì˜")
print("   - Bandpass í•„í„°ë§: í˜¸í¡ìŒ ê´€ë ¨ ì£¼íŒŒìˆ˜ ì¶”ì¶œ")
print("   - ë°ì´í„° ì¦ê°•: ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ")

print("\n2. ğŸ§  Self-Supervised Learning:")
print("   - Contrastive Learning: Positive/Negative pairs í•™ìŠµ")
print("   - Cola/ColaMD: ì˜¤ë””ì˜¤/ì˜ë£Œ ë°ì´í„° íŠ¹í™” ëª¨ë¸")
print("   - ë¼ë²¨ ì—†ëŠ” ë°ì´í„°ë¡œ ì˜ë¯¸ ìˆëŠ” íŠ¹ì§• í•™ìŠµ")

print("\n3. ğŸ”„ Transfer Learning:")
print("   - ì‚¬ì „ í›ˆë ¨ëœ ì¸ì½”ë” í™œìš©")
print("   - ì†ŒëŸ‰ì˜ ë¼ë²¨ ë°ì´í„°ë¡œ ë¶„ë¥˜ê¸° í•™ìŠµ")
print("   - ë¹ ë¥¸ ìˆ˜ë ´ê³¼ ì¢‹ì€ ì„±ëŠ¥")

print("\n4. ğŸ¥ ì˜ë£Œ ì‘ìš©:")
print("   - Inter-patient ë¶„í• : ì‹¤ì œ ì„ìƒ í™˜ê²½ ë°˜ì˜")
print("   - í˜¸í¡ìŒ ë¶„ë¥˜: ì •ìƒ/ë¹„ì •ìƒ êµ¬ë¶„")
print("   - ì‹¤ì‹œê°„ ì¶”ë¡ : 8ì´ˆ ì„¸ê·¸ë¨¼íŠ¸ ë‹¨ìœ„ ì²˜ë¦¬")

print("\nâœ… ì¢…í•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
print("=" * 60)
