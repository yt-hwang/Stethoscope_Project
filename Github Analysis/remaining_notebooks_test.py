#!/usr/bin/env python3
"""
λ‚λ¨Έμ§€ λ…ΈνΈλ¶λ“¤ μƒμ„Έ λ¶„μ„ λ° μ„¤λ…
- fine tuning simulation.ipynb
- RNN experiment.ipynb
"""

print("π”¬ λ‚λ¨Έμ§€ λ…ΈνΈλ¶λ“¤ μƒμ„Έ λ¶„μ„")
print("=" * 60)

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
import pytorch_lightning as pl
from sklearn.model_selection import train_test_split

# μ°λ¦¬κ°€ λ§λ“  λ¨λ“λ“¤ import
from src.util import random_crop, random_mask, random_multiply, crop_first
from src.model.models_eval import AudioClassifier
from src.benchmark.model_util import initialize_pretrained_model

print("\nπ“ 1λ‹¨κ³„: fine tuning simulation.ipynb λ¶„μ„")
print("-" * 50)

print("""
π― fine tuning simulation.ipynbμ λ©μ :

μ΄ λ…ΈνΈλ¶μ€ Transfer Learningμ νμΈνλ‹ κ³Όμ •μ„ μ‹λ®¬λ μ΄μ…ν•©λ‹λ‹¤.

μ£Όμ” κΈ°λ¥:
1. ν•μ΄νΌνλΌλ―Έν„° νλ‹
2. ν•™μµ κ³Όμ • λ¨λ‹ν„°λ§
3. μ„±λ¥ ν‰κ°€ λ° λΉ„κµ
4. λ‹¤μ–‘ν• μ„¤μ •μΌλ΅ μ‹¤ν—

ν•µμ‹¬ κ°λ…:
- Learning Rate μ¤μΌ€μ¤„λ§
- Early Stopping
- Model Checkpointing
- μ„±λ¥ λ©”νΈλ¦­ μ¶”μ 
""")

print("\nπ”§ 2λ‹¨κ³„: νμΈνλ‹ μ‹λ®¬λ μ΄μ… κµ¬ν„")
print("-" * 50)

class FineTuningSimulator:
    """νμΈνλ‹ μ‹λ®¬λ μ΄μ… ν΄λμ¤"""
    
    def __init__(self, model, train_loader, val_loader, test_loader):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.test_loader = test_loader
        self.history = {
            'train_loss': [],
            'val_loss': [],
            'train_acc': [],
            'val_acc': []
        }
    
    def train_epoch(self):
        """ν• μ—ν¬ν¬ ν•™μµ"""
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        for batch_x, batch_y in self.train_loader:
            # Forward pass
            logits = self.model(batch_x)
            loss = F.cross_entropy(logits, batch_y)
            
            # Backward pass
            loss.backward()
            
            # Update weights
            for param in self.model.parameters():
                if param.grad is not None:
                    param.data -= 0.001 * param.grad.data  # κ°„λ‹¨ν• SGD
                    param.grad.zero_()
            
            total_loss += loss.item()
            pred = torch.argmax(logits, dim=1)
            correct += (pred == batch_y).sum().item()
            total += batch_y.size(0)
        
        avg_loss = total_loss / len(self.train_loader)
        accuracy = correct / total
        
        return avg_loss, accuracy
    
    def validate_epoch(self):
        """ν• μ—ν¬ν¬ κ²€μ¦"""
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for batch_x, batch_y in self.val_loader:
                logits = self.model(batch_x)
                loss = F.cross_entropy(logits, batch_y)
                
                total_loss += loss.item()
                pred = torch.argmax(logits, dim=1)
                correct += (pred == batch_y).sum().item()
                total += batch_y.size(0)
        
        avg_loss = total_loss / len(self.val_loader)
        accuracy = correct / total
        
        return avg_loss, accuracy
    
    def simulate_training(self, epochs=10):
        """ν•™μµ μ‹λ®¬λ μ΄μ…"""
        print(f"νμΈνλ‹ μ‹λ®¬λ μ΄μ… μ‹μ‘ ({epochs} μ—ν¬ν¬)")
        print("-" * 40)
        
        for epoch in range(epochs):
            # ν•™μµ
            train_loss, train_acc = self.train_epoch()
            
            # κ²€μ¦
            val_loss, val_acc = self.validate_epoch()
            
            # νμ¤ν† λ¦¬ μ €μ¥
            self.history['train_loss'].append(train_loss)
            self.history['val_loss'].append(val_loss)
            self.history['train_acc'].append(train_acc)
            self.history['val_acc'].append(val_acc)
            
            print(f"Epoch {epoch+1:2d}: "
                  f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.3f} | "
                  f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.3f}")
        
        print("-" * 40)
        print("νμΈνλ‹ μ‹λ®¬λ μ΄μ… μ™„λ£!")
        
        return self.history

print("β… FineTuningSimulator ν΄λμ¤ μ •μ μ™„λ£!")

print("\nπ§ 3λ‹¨κ³„: νμΈνλ‹ μ‹λ®¬λ μ΄μ… ν…μ¤νΈ")
print("-" * 50)

# λ”λ―Έ λ°μ΄ν„° μƒμ„±
X = torch.FloatTensor(np.random.randn(100, 200, 128))
y = torch.LongTensor(np.random.randint(0, 2, 100))

# λ°μ΄ν„° λ¶„ν• 
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# DataLoader μƒμ„±
train_loader = DataLoader(torch.utils.data.TensorDataset(X_train, y_train), batch_size=8, shuffle=True)
val_loader = DataLoader(torch.utils.data.TensorDataset(X_val, y_val), batch_size=8, shuffle=False)
test_loader = DataLoader(torch.utils.data.TensorDataset(X_test, y_test), batch_size=8, shuffle=False)

print(f"λ°μ΄ν„° λ¶„ν•  μ™„λ£:")
print(f"  Train: {len(X_train)} samples")
print(f"  Val: {len(X_val)} samples")
print(f"  Test: {len(X_test)} samples")

# λ¨λΈ μƒμ„±
pretrained_model = initialize_pretrained_model("operaCT")
classifier = AudioClassifier(
    net=pretrained_model,
    head="linear",
    classes=2,
    feat_dim=768
)

# νμΈνλ‹ μ‹λ®¬λ μ΄μ… μ‹¤ν–‰
simulator = FineTuningSimulator(classifier, train_loader, val_loader, test_loader)
history = simulator.simulate_training(epochs=5)

print("\nπ“ 4λ‹¨κ³„: RNN experiment.ipynb λ¶„μ„")
print("-" * 50)

print("""
π― RNN experiment.ipynbμ λ©μ :

μ΄ λ…ΈνΈλ¶μ€ RNN λ¨λΈμ„ μ‹¤ν—ν•λ” λ…ΈνΈλ¶μ…λ‹λ‹¤.

μ£Όμ” κΈ°λ¥:
1. LSTM/GRU λ¨λΈ κµ¬ν„
2. μ‹κ³„μ—΄ λ°μ΄ν„° μ²λ¦¬
3. RNN vs CNN μ„±λ¥ λΉ„κµ
4. λ‹¤μ–‘ν• RNN μ•„ν‚¤ν…μ² μ‹¤ν—

ν•µμ‹¬ κ°λ…:
- μ‹κ³„μ—΄ λ°μ΄ν„°μ μμ°¨μ  μ²λ¦¬
- LSTMμ μ¥κΈ° μμ΅΄μ„± ν•™μµ
- GRUμ κ°„μ†ν™”λ κµ¬μ΅°
- Bidirectional RNN
""")

print("\nπ”§ 5λ‹¨κ³„: RNN λ¨λΈ κµ¬ν„")
print("-" * 50)

class LSTMRespiratoryClassifier(nn.Module):
    """LSTM κΈ°λ° νΈν΅μ λ¶„λ¥κΈ°"""
    
    def __init__(self, input_size=128, hidden_size=256, num_layers=2, num_classes=2, dropout=0.1):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # LSTM λ μ΄μ–΄
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0,
            bidirectional=True
        )
        
        # λ¶„λ¥κΈ°
        self.classifier = nn.Sequential(
            nn.Linear(hidden_size * 2, hidden_size),  # bidirectionalμ΄λ―€λ΅ *2
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size, num_classes)
        )
    
    def forward(self, x):
        # x: [batch_size, time_steps, features]
        batch_size = x.size(0)
        
        # LSTM forward
        lstm_out, (hidden, cell) = self.lstm(x)
        
        # λ§μ§€λ§‰ νƒ€μ„μ¤ν…μ μ¶λ ¥ μ‚¬μ©
        last_output = lstm_out[:, -1, :]  # [batch_size, hidden_size * 2]
        
        # λ¶„λ¥
        logits = self.classifier(last_output)
        
        return logits

class GRURespiratoryClassifier(nn.Module):
    """GRU κΈ°λ° νΈν΅μ λ¶„λ¥κΈ°"""
    
    def __init__(self, input_size=128, hidden_size=256, num_layers=2, num_classes=2, dropout=0.1):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # GRU λ μ΄μ–΄
        self.gru = nn.GRU(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0,
            bidirectional=True
        )
        
        # λ¶„λ¥κΈ°
        self.classifier = nn.Sequential(
            nn.Linear(hidden_size * 2, hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size, num_classes)
        )
    
    def forward(self, x):
        # x: [batch_size, time_steps, features]
        gru_out, hidden = self.gru(x)
        
        # λ§μ§€λ§‰ νƒ€μ„μ¤ν…μ μ¶λ ¥ μ‚¬μ©
        last_output = gru_out[:, -1, :]  # [batch_size, hidden_size * 2]
        
        # λ¶„λ¥
        logits = self.classifier(last_output)
        
        return logits

print("β… LSTM/GRU λ¨λΈ μ •μ μ™„λ£!")

print("\nπ§ 6λ‹¨κ³„: RNN λ¨λΈ ν…μ¤νΈ")
print("-" * 50)

# λ”λ―Έ μ‹κ³„μ—΄ λ°μ΄ν„° μƒμ„±
time_series_data = torch.FloatTensor(np.random.randn(20, 200, 128))  # [batch, time, features]
labels = torch.LongTensor(np.random.randint(0, 2, 20))

print(f"μ‹κ³„μ—΄ λ°μ΄ν„° shape: {time_series_data.shape}")
print(f"λΌλ²¨ shape: {labels.shape}")

# LSTM λ¨λΈ ν…μ¤νΈ
print("\nLSTM λ¨λΈ ν…μ¤νΈ:")
lstm_model = LSTMRespiratoryClassifier(input_size=128, hidden_size=256, num_layers=2)
print(f"LSTM νλΌλ―Έν„° μ: {sum(p.numel() for p in lstm_model.parameters()):,}")

lstm_model.eval()
with torch.no_grad():
    lstm_output = lstm_model(time_series_data)
    print(f"LSTM μ¶λ ¥ shape: {lstm_output.shape}")

# GRU λ¨λΈ ν…μ¤νΈ
print("\nGRU λ¨λΈ ν…μ¤νΈ:")
gru_model = GRURespiratoryClassifier(input_size=128, hidden_size=256, num_layers=2)
print(f"GRU νλΌλ―Έν„° μ: {sum(p.numel() for p in gru_model.parameters()):,}")

gru_model.eval()
with torch.no_grad():
    gru_output = gru_model(time_series_data)
    print(f"GRU μ¶λ ¥ shape: {gru_output.shape}")

print("\nπ”¬ 7λ‹¨κ³„: RNN vs CNN μ„±λ¥ λΉ„κµ")
print("-" * 50)

# κ°„λ‹¨ν• CNN λ¨λΈκ³Ό λΉ„κµ
class SimpleCNN(nn.Module):
    """κ°„λ‹¨ν• CNN λ¨λΈ"""
    
    def __init__(self, input_channels=128, num_classes=2):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.pool = nn.AdaptiveAvgPool2d((1, 1))
        self.classifier = nn.Linear(64, num_classes)
    
    def forward(self, x):
        # x: [batch, time, features] -> [batch, 1, time, features]
        x = x.unsqueeze(1)
        
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        
        return x

# CNN λ¨λΈ ν…μ¤νΈ
print("CNN λ¨λΈ ν…μ¤νΈ:")
cnn_model = SimpleCNN(input_channels=128)
print(f"CNN νλΌλ―Έν„° μ: {sum(p.numel() for p in cnn_model.parameters()):,}")

cnn_model.eval()
with torch.no_grad():
    cnn_output = cnn_model(time_series_data)
    print(f"CNN μ¶λ ¥ shape: {cnn_output.shape}")

print("\nπ“ λ¨λΈλ³„ νλΌλ―Έν„° μ λΉ„κµ:")
print(f"  LSTM: {sum(p.numel() for p in lstm_model.parameters()):,}")
print(f"  GRU:  {sum(p.numel() for p in gru_model.parameters()):,}")
print(f"  CNN:  {sum(p.numel() for p in cnn_model.parameters()):,}")

print("\nπ“ 8λ‹¨κ³„: ν•µμ‹¬ κ°λ… μ •λ¦¬")
print("-" * 50)

print("""
π― λ‚λ¨Έμ§€ λ…ΈνΈλ¶λ“¤μ ν•µμ‹¬ κ°λ…:

1. fine tuning simulation.ipynb:
   - νμΈνλ‹ κ³Όμ •μ μ‹λ®¬λ μ΄μ…
   - ν•μ΄νΌνλΌλ―Έν„° νλ‹
   - ν•™μµ κ³Όμ • λ¨λ‹ν„°λ§
   - μ„±λ¥ ν‰κ°€ λ° λΉ„κµ

2. RNN experiment.ipynb:
   - μ‹κ³„μ—΄ λ°μ΄ν„°μ μμ°¨μ  μ²λ¦¬
   - LSTM: μ¥κΈ° μμ΅΄μ„± ν•™μµ
   - GRU: κ°„μ†ν™”λ κµ¬μ΅°
   - RNN vs CNN μ„±λ¥ λΉ„κµ

3. κ° λ¨λΈμ νΉμ§•:
   - LSTM: λ³µμ΅ν•μ§€λ§ κ°•λ ¥ν• μ¥κΈ° μμ΅΄μ„± ν•™μµ
   - GRU: LSTMλ³΄λ‹¤ κ°„λ‹¨ν•μ§€λ§ λΉ„μ·ν• μ„±λ¥
   - CNN: κ³µκ°„μ  νΉμ§• ν•™μµμ— κ°•ν•¨
   - RNN: μ‹κ°„μ  νΉμ§• ν•™μµμ— κ°•ν•¨

4. νΈν΅μ λ¶„μ„μ—μ„μ μ μ©:
   - μ‹κ³„μ—΄ λ°μ΄ν„°: νΈν΅ μ£ΌκΈ°μ μ‹κ°„μ  λ³€ν™”
   - μμ°¨μ  μ²λ¦¬: νΈν΅μμ μ—°μ†μ„± κ³ λ ¤
   - μ¥κΈ° μμ΅΄μ„±: νΈν΅ μ£ΌκΈ° μ „μ²΄μ ν¨ν„΄ ν•™μµ
""")

print("\nβ… λ‚λ¨Έμ§€ λ…ΈνΈλ¶λ“¤ μ™„μ „ μ΄ν•΄ μ™„λ£!")
print("=" * 60)
